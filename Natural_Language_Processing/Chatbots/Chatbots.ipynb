{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AXkYII8ZL6HX",
        "outputId": "125266f9-cbe8-43d4-e006-a154ba4ae72a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rasa_nlu\n",
            "  Downloading rasa_nlu-0.15.1-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 24.6 MB/s \n",
            "\u001b[?25hCollecting scikit-learn~=0.20.2\n",
            "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4 MB 58.5 MB/s \n",
            "\u001b[?25hCollecting typing~=3.6\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting boto3~=1.9\n",
            "  Downloading boto3-1.25.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 66.2 MB/s \n",
            "\u001b[?25hCollecting klein~=17.10\n",
            "  Downloading klein-17.10.0-py2.py3-none-any.whl (30 kB)\n",
            "Collecting gevent~=1.3\n",
            "  Downloading gevent-1.5.0-cp37-cp37m-manylinux2010_x86_64.whl (5.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.1 MB 63.7 MB/s \n",
            "\u001b[?25hCollecting future~=0.17.1\n",
            "  Downloading future-0.17.1.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 82.7 MB/s \n",
            "\u001b[?25hCollecting jsonschema~=2.6\n",
            "  Downloading jsonschema-2.6.0-py2.py3-none-any.whl (39 kB)\n",
            "Collecting matplotlib~=2.2\n",
            "  Downloading matplotlib-2.2.5-cp37-cp37m-manylinux1_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 50.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from rasa_nlu) (1.21.6)\n",
            "Collecting coloredlogs~=10.0\n",
            "  Downloading coloredlogs-10.0-py2.py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml~=0.15.7\n",
            "  Downloading ruamel.yaml-0.15.100-cp37-cp37m-manylinux1_x86_64.whl (654 kB)\n",
            "\u001b[K     |████████████████████████████████| 654 kB 58.4 MB/s \n",
            "\u001b[?25hCollecting packaging~=18.0\n",
            "  Downloading packaging-18.0-py2.py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: tqdm~=4.19 in /usr/local/lib/python3.7/dist-packages (from rasa_nlu) (4.64.1)\n",
            "Collecting cloudpickle~=0.6.1\n",
            "  Downloading cloudpickle-0.6.1-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests~=2.20 in /usr/local/lib/python3.7/dist-packages (from rasa_nlu) (2.23.0)\n",
            "Collecting simplejson~=3.13\n",
            "  Downloading simplejson-3.17.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 9.9 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.4 MB/s \n",
            "\u001b[?25hCollecting botocore<1.29.0,>=1.28.0\n",
            "  Downloading botocore-1.28.0-py3-none-any.whl (9.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.3 MB 60.7 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.29.0,>=1.28.0->boto3~=1.9->rasa_nlu) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 67.4 MB/s \n",
            "\u001b[?25hCollecting humanfriendly>=4.7\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: greenlet>=0.4.14 in /usr/local/lib/python3.7/dist-packages (from gevent~=1.3->rasa_nlu) (1.1.3.post0)\n",
            "Requirement already satisfied: werkzeug in /usr/local/lib/python3.7/dist-packages (from klein~=17.10->rasa_nlu) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from klein~=17.10->rasa_nlu) (1.15.0)\n",
            "Collecting incremental\n",
            "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting Twisted>=15.5\n",
            "  Downloading Twisted-22.8.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 64.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=2.2->rasa_nlu) (0.11.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from matplotlib~=2.2->rasa_nlu) (2022.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=2.2->rasa_nlu) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=2.2->rasa_nlu) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib~=2.2->rasa_nlu) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests~=2.20->rasa_nlu) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests~=2.20->rasa_nlu) (3.0.4)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 66.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests~=2.20->rasa_nlu) (2022.9.24)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn~=0.20.2->rasa_nlu) (1.7.3)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=15.5->klein~=17.10->rasa_nlu) (22.1.0)\n",
            "Collecting zope.interface>=4.4.2\n",
            "  Downloading zope.interface-5.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (254 kB)\n",
            "\u001b[K     |████████████████████████████████| 254 kB 69.5 MB/s \n",
            "\u001b[?25hCollecting constantly>=15.1\n",
            "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting Automat>=0.8.0\n",
            "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting hyperlink>=17.1.1\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface>=4.4.2->Twisted>=15.5->klein~=17.10->rasa_nlu) (57.4.0)\n",
            "Building wheels for collected packages: future, typing\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.17.1-py3-none-any.whl size=488740 sha256=6f338d8d7def5c6b544102f925960597a18a5c4ded858e3061db44672ea3bc49\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/4c/84/8a6161d44282ede60ed233d090156c6109a7ab865e49c1c9f6\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26325 sha256=1465c095d32042d071a82a4b2b11471e7fcdad878744af27b019591408a41f53\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/f3/15/01aa6571f0a72ee6ae7b827c1491c37a1f72d686fd22b43b0e\n",
            "Successfully built future typing\n",
            "Installing collected packages: urllib3, jmespath, zope.interface, incremental, hyperlink, constantly, botocore, Automat, Twisted, s3transfer, humanfriendly, typing, simplejson, scikit-learn, ruamel.yaml, packaging, matplotlib, klein, jsonschema, gevent, future, coloredlogs, cloudpickle, boto3, rasa-nlu\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 21.3\n",
            "    Uninstalling packaging-21.3:\n",
            "      Successfully uninstalled packaging-21.3\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.3.3\n",
            "    Uninstalling jsonschema-4.3.3:\n",
            "      Successfully uninstalled jsonschema-4.3.3\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.5.0\n",
            "    Uninstalling cloudpickle-1.5.0:\n",
            "      Successfully uninstalled cloudpickle-1.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.20.4 which is incompatible.\n",
            "tensorflow-probability 0.16.0 requires cloudpickle>=1.3, but you have cloudpickle 0.6.1 which is incompatible.\n",
            "spacy 3.4.2 requires packaging>=20.0, but you have packaging 18.0 which is incompatible.\n",
            "pooch 1.6.0 requires packaging>=20.0, but you have packaging 18.0 which is incompatible.\n",
            "plotnine 0.8.0 requires matplotlib>=3.1.1, but you have matplotlib 2.2.5 which is incompatible.\n",
            "mizani 0.7.3 requires matplotlib>=3.1.1, but you have matplotlib 2.2.5 which is incompatible.\n",
            "librosa 0.8.1 requires packaging>=20.0, but you have packaging 18.0 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.20.4 which is incompatible.\n",
            "gym 0.25.2 requires cloudpickle>=1.2.0, but you have cloudpickle 0.6.1 which is incompatible.\n",
            "distributed 2022.2.0 requires cloudpickle>=1.5.0, but you have cloudpickle 0.6.1 which is incompatible.\n",
            "distributed 2022.2.0 requires packaging>=20.0, but you have packaging 18.0 which is incompatible.\n",
            "datascience 0.17.5 requires matplotlib>=3.0.0, but you have matplotlib 2.2.5 which is incompatible.\n",
            "dask 2022.2.0 requires cloudpickle>=1.1.1, but you have cloudpickle 0.6.1 which is incompatible.\n",
            "dask 2022.2.0 requires packaging>=20.0, but you have packaging 18.0 which is incompatible.\n",
            "arviz 0.12.1 requires matplotlib>=3.0, but you have matplotlib 2.2.5 which is incompatible.\n",
            "altair 4.2.0 requires jsonschema>=3.0, but you have jsonschema 2.6.0 which is incompatible.\u001b[0m\n",
            "Successfully installed Automat-20.2.0 Twisted-22.8.0 boto3-1.25.0 botocore-1.28.0 cloudpickle-0.6.1 coloredlogs-10.0 constantly-15.1.0 future-0.17.1 gevent-1.5.0 humanfriendly-10.0 hyperlink-21.0.0 incremental-22.10.0 jmespath-1.0.1 jsonschema-2.6.0 klein-17.10.0 matplotlib-2.2.5 packaging-18.0 rasa-nlu-0.15.1 ruamel.yaml-0.15.100 s3transfer-0.6.0 scikit-learn-0.20.4 simplejson-3.17.6 typing-3.7.4.3 urllib3-1.25.11 zope.interface-5.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install rasa_nlu\n",
        "#!pip install matplotlib --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt, numpy as np, seaborn as sns, pandas as pd, re, nltk, itertools, spacy, textblob, time, json, sqlite3, string\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "yf3a_osrMC3f",
        "outputId": "14223798-d60b-41aa-c0cc-43d10dd2789f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dtype=np.int):\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, positive=False):\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  precompute=False, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, random_state=None,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***EchoBot I***\n",
        "\n",
        "Hello, World!\n",
        "\n",
        "You'll begin learning how to build chatbots in Python by writing two functions to build the simplest bot possible: EchoBot. EchoBot just responds by replying with the same message it receives.\n",
        "\n",
        "In this exercise, you'll define a function that responds to a user's message. In the next exercise, you'll complete EchoBot by writing a function to send a message to the bot.\n",
        "\n",
        "* Write a function called `respond()` with a single parameter `message` which returns the bot's response. To do this, concatenate the strings `\"I can hear you! You said: \"` and `message`.\n",
        "\n",
        "* Store the concatenated strings in `bot_message`, and return this result."
      ],
      "metadata": {
        "id": "w43MrDffnlX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bot_template = \"BOT : {0}\"\n",
        "user_template = \"USER : {0}\"\n",
        "\n",
        "# Define a function that responds to a user's message: respond\n",
        "def respond(message):\n",
        "    # Concatenate the user's message to the end of a standard bot respone\n",
        "    bot_message = \"I can hear you! You said: \" + message\n",
        "    # Return the result\n",
        "    return bot_message\n",
        "\n",
        "# Test function\n",
        "print(respond(\"hello!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA7mSQp3mpLl",
        "outputId": "460b5bc5-0efa-4dec-85d9-cb21040889a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can hear you! You said: hello!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***EchoBot II***\n",
        "\n",
        "Having written your `respond()` function, you'll now define a function called `send_message()` with a single parameter `message` which logs the `message` and the bot's response.\n",
        "\n",
        "\n",
        "* Use the `user_template` string's **`.format()`** method to include the user's `message` into the user template, and print the result.\n",
        "\n",
        "* Call the `respond()` function with the message passed in and save the result as `response`.\n",
        "\n",
        "* Log the bot's response using the `bot_template` string's **`.format()`** method.\n",
        "\n",
        "* Send the message `\"hello\"` to the bot."
      ],
      "metadata": {
        "id": "3NqK7329pR97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create templates\n",
        "bot_template = \"BOT : {0}\"\n",
        "user_template = \"USER : {0}\"\n",
        "\n",
        "# Define a function that sends a message to the bot: send_message\n",
        "def send_message(message):\n",
        "    # Print user_template including the user_message\n",
        "    print(user_template.format(message))\n",
        "    # Get the bot's response to the message\n",
        "    response = respond(message)\n",
        "    # Print the bot template including the bot's response.\n",
        "    print(bot_template.format(response))\n",
        "\n",
        "# Send a message to the bot\n",
        "send_message(\"hello\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iz84-qCn9RG",
        "outputId": "11853c43-0f54-4bfb-fa99-4f47b12feef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USER : hello\n",
            "BOT : I can hear you! You said: hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Chitchat***\n",
        "\n",
        "Now you're going to leave the simple EchoBot behind and create a bot which can answer simple questions such as \"What's your name?\" and \"What's today's weather?\"\n",
        "\n",
        "You'll use a dictionary with these questions as keys and the correct responses as values.\n",
        "\n",
        "This means the bot will only respond correctly if the message matches `exactly`, which is a big limitation. In later exercises you will create much more robust solutions.\n",
        "\n",
        "The `send_message()` function has already been defined for you, as well as the `bot_template` and `user_template` variables.\n",
        "\n",
        "\n",
        "* Define a `respond()` function which takes in a `message` argument, checks if the `message` has a pre-defined response, and returns the response in the `responses` dictionary if there is a match, or the `\"default\"` message otherwise."
      ],
      "metadata": {
        "id": "z_OTLHeQwjK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define variables\n",
        "name = \"Greg\"\n",
        "weather = \"cloudy\"\n",
        "\n",
        "# Define a dictionary with the predefined responses\n",
        "responses = {\n",
        "  \"what's your name?\": \"my name is {0}\".format(name),\n",
        "  \"what's today's weather?\": \"the weather is {0}\".format(weather),\n",
        "  \"default\": \"default message\"\n",
        "}\n",
        "\n",
        "# Return the matching response if there is one, default otherwise\n",
        "def respond(message):\n",
        "    # Check if the message is in the responses\n",
        "    if message in responses:\n",
        "        # Return the matching message\n",
        "        bot_message = responses[message]\n",
        "    else:\n",
        "        # Return the \"default\" message\n",
        "        bot_message = responses[\"default\"]\n",
        "    return bot_message\n",
        "\n",
        "print(send_message(\"what's today's weather?\"))\n",
        "print(send_message(\"what's your name?\"))\n",
        "print(send_message(\"what's your favorite color?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpQlihZRuhNu",
        "outputId": "b85f7597-c61c-44e4-f3d8-d6463173d073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USER : what's today's weather?\n",
            "BOT : the weather is cloudy\n",
            "None\n",
            "USER : what's your name?\n",
            "BOT : my name is Greg\n",
            "None\n",
            "USER : what's your favorite color?\n",
            "BOT : default message\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Adding variety***\n",
        "\n",
        "It can get a little boring hearing the same old answers over and over. In this exercise, you'll add some variation. If you ask your bot how it's feeling, the likelihood that it responds with \"oh I'm great!\" or \"I'm very sad today\" should be equal.\n",
        "\n",
        "Here, you'll use the **`random`** module - specifically **`random.choice(ls)`** - which randomly selects an element from a list `ls`.\n",
        "\n",
        "A dictionary called `responses`, which maps each message to a list of possible responses, has been defined for you.\n",
        "\n",
        "\n",
        "* Import the **`random`** module.\n",
        "* If the message is in responses, use **`random.choice()`** in the `respond()` function to choose a random matching response.\n",
        "* If the `message` is not in responses, choose a random default response."
      ],
      "metadata": {
        "id": "akHCOgyayUMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "name = \"Greg\"\n",
        "weather = \"cloudy\"\n",
        "\n",
        "# Define a dictionary containing a list of responses for each message\n",
        "responses = {\"what's your name?\": [\"my name is {0}\".format(name),\n",
        "                                   \"they call me {0}\".format(name),\n",
        "                                   \"I go by {0}\".format(name)],  \n",
        "             \"what's today's weather?\": [\"the weather is {0}\".format(weather),\n",
        "                                         \"it's {0} today\".format(weather)],\n",
        "             \"default\": [\"default message\"]}\n",
        "\n",
        "# Use random.choice() to choose a matching response\n",
        "def respond(message):\n",
        "    # Check if the message is in the responses\n",
        "    if message in responses:\n",
        "        # Return a random matching response\n",
        "        bot_message = random.choice(responses[message])\n",
        "    else:\n",
        "        # Return a random \"default\" response\n",
        "        bot_message = random.choice(responses[\"default\"])\n",
        "    return bot_message\n",
        "\n",
        "for i in range(3):\n",
        "    print(send_message(\"what's your name?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFQ4D-fBxqZl",
        "outputId": "64206466-b7f9-425b-c7d1-3e7e05f69be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USER : what's your name?\n",
            "BOT : they call me Greg\n",
            "None\n",
            "USER : what's your name?\n",
            "BOT : I go by Greg\n",
            "None\n",
            "USER : what's your name?\n",
            "BOT : my name is Greg\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***ELIZA I: asking questions***\n",
        "\n",
        "Asking questions is a great way to create an engaging conversation. Here, you'll create the very first hint of ELIZA's famous personality, by responding to statements with a question and responding to questions with answers.\n",
        "\n",
        "A dictionary of responses with `\"question\"` and `\"statement\"` as keys and lists of appropriate responses as values has already been defined for you. Explore this in the Shell with `responses.keys()` and `responses[\"question\"]`.\n",
        "\n",
        "* Define a `respond()` function which takes in `message` as an argument, and uses the string's **`.endswith()`** method to check if a message ends with a question mark.\n",
        "\n",
        "* If the message does end with a question mark, choose a random `\"question\"` from the `responses` dictionary. Else, choose a random `\"statement\"` from the `responses`.\n",
        "\n",
        "* Send the bot multiple messages with and without a question mark - these have been provided for you. If you want to experiment further in the Shell, be sure to first hit 'Run Code'."
      ],
      "metadata": {
        "id": "wmLKYk-OzxSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "responses = {'question': [\"I don't know :(\", \n",
        "                          'you tell me!'], \n",
        "             'statement': ['tell me more!',\n",
        "                           'why do you think that?',\n",
        "                           'how long have you felt this way?',\n",
        "                           'I find that extremely interesting',\n",
        "                           'can you back that up?',\n",
        "                           'oh wow!',\n",
        "                           ':)']}\n",
        "\n",
        "def respond(message):\n",
        "    # Check for a question mark\n",
        "    if message.endswith('?'):\n",
        "        # Return a random question\n",
        "        return random.choice(responses[\"question\"])\n",
        "    # Return a random statement\n",
        "    return random.choice(responses[\"statement\"])\n",
        "\n",
        "\n",
        "# Send messages ending in a question mark\n",
        "send_message(\"what's today's weather?\")\n",
        "send_message(\"what's today's weather?\")\n",
        "\n",
        "# Send messages which don't end with a question mark\n",
        "send_message(\"I love building chatbots\")\n",
        "send_message(\"I love building chatbots\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ay6YyFPzbzM",
        "outputId": "d9f2b2fb-09f6-4fa6-b870-0606f50aaeb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USER : what's today's weather?\n",
            "BOT : you tell me!\n",
            "USER : what's today's weather?\n",
            "BOT : you tell me!\n",
            "USER : I love building chatbots\n",
            "BOT : oh wow!\n",
            "USER : I love building chatbots\n",
            "BOT : why do you think that?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text processing with regular expressions**\n",
        "\n",
        "## ***ELIZA II: Extracting key phrases***\n",
        "\n",
        "The really clever thing about ELIZA is the way the program appears to understand what you told it by occasionally including phrases uttered by the user in its responses.\n",
        "\n",
        "In this exercise, you will match messages against some common patterns and extract phrases using **`re.search()`**. A dictionary called `rules` has already been defined, which matches the following patterns:\n",
        "\n",
        "  - `\"do you think (.*)\"`\n",
        "  - `\"do you remember (.*)\"`\n",
        "  - `\"I want (.*)\"`\n",
        "  -  `\"if (.*)\"`\n",
        "\n",
        "* Iterate over the `rules` dictionary using its **`.items()`** method, with `pattern` and `responses` as your iterator variables.\n",
        "\n",
        "* Use **`re.search()`** with the `pattern` and `message` to create a `match` object.\n",
        "\n",
        "* If there is a match, use **`random.choice()`** to pick a response.\n",
        "\n",
        "* If `'{0}'` is in that `response`, use the `match` object's **`.group()`** method with index `1` to retrieve a phrase."
      ],
      "metadata": {
        "id": "X3TLEOcG1hrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "rules = {'I want (.*)':           ['What would it mean if you got {0}',\n",
        "                                   'Why do you want {0}',\n",
        "                                   \"What's stopping you from getting {0}\"],\n",
        "          'do you remember (.*)': ['Did you think I would forget {0}',\n",
        "                                   \"Why haven't you been able to forget {0}\",\n",
        "                                   'What about {0}',  \n",
        "                                   'Yes .. and?'],\n",
        "          'do you think (.*)':    ['if {0}? Absolutely.', \n",
        "                                   'No chance'],\n",
        "          'if (.*)':              [\"Do you really think it's likely that {0}\",\n",
        "                                   'Do you wish that {0}',\n",
        "                                   'What do you think about {0}',\n",
        "                                   'Really--if {0}']}\n",
        "\n",
        "# Define match_rule()\n",
        "def match_rule(rules, message):\n",
        "    response, phrase = \"default\", None\n",
        "    \n",
        "    # Iterate over the rules dictionary\n",
        "    for pattern, responses in rules.items():\n",
        "        # Create a match object\n",
        "        match = re.search(pattern, message)\n",
        "        if match is not None:\n",
        "            # Choose a random response\n",
        "            response = random.choice(responses)\n",
        "            if '{0}' in response:\n",
        "                phrase = match.group(1)\n",
        "    # Return the response and phrase\n",
        "    return response.format(phrase)\n",
        "\n",
        "# Test match_rule\n",
        "print(match_rule(rules, \"if you fuck\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKhcDs031GKX",
        "outputId": "606d0268-7c5a-4abe-b3ca-cf31ba1a5037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do you really think it's likely that you fuck\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***ELIZA III: Pronouns***\n",
        "\n",
        "To make responses grammatically coherent, you'll want to transform the extracted phrases from first to second person and vice versa. In English, conjugating verbs is easy, and simply swapping \"me\" and 'you', \"my\" and \"your\" works in `most` cases.\n",
        "\n",
        "In this exercise, you'll define a function called `replace_pronouns()` which uses **`re.sub()`** to map \"me\" and \"my\" to \"you\" and \"your\" (and vice versa) in a string.\n",
        "\n",
        "\n",
        "* If `'me'` is in message, use **`re.sub()`** to replace it with `'you'`.\n",
        "* If `'my'` is in message, replace it with `'your'`.\n",
        "* If `'your'` is in message, replace it with `'my'`.\n",
        "* If `'you'` is in message, replace it with `'me'`."
      ],
      "metadata": {
        "id": "gH68Cya_eQwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define replace_pronouns()\n",
        "def replace_pronouns(message):\n",
        "\n",
        "    message = message.lower()\n",
        "    if 'me' in message:\n",
        "        # Replace 'me' with 'you'\n",
        "        return re.sub('me', 'you', message)\n",
        "    if 'my' in message:\n",
        "        # Replace 'my' with 'your'\n",
        "        return re.sub('my', 'your', message)\n",
        "    if 'your' in message:\n",
        "        # Replace 'your' with 'my'\n",
        "        return re.sub('your', 'my', message)\n",
        "    if 'you' in message:\n",
        "        # Replace 'you' with 'me'\n",
        "        return re.sub('you', 'me', message)\n",
        "\n",
        "    return message\n",
        "\n",
        "print(replace_pronouns(\"my last birthday\"))\n",
        "print(replace_pronouns(\"when you went to Florida\"))\n",
        "print(replace_pronouns(\"I had my own castle\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPcam3MPapIC",
        "outputId": "16caf883-042d-4f7e-abe6-aadac85f4f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your last birthday\n",
            "when me went to florida\n",
            "i had your own castle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, as you can see, one of the pitfalls of manually conjugating verbs is that they may not always be grammatically correct.\n",
        "\n",
        "## ***ELIZA IV: Putting it all together***\n",
        "\n",
        "Now you're going to put everything from the previous exercises together and experience the magic! The `match_rule()`,` send_message()`, and `replace_pronouns`() functions have already been defined, and the `rules` dictionary is available in your workspace.\n",
        "\n",
        "Your job here is to write a function called `respond()` with a single argument `message` which creates an appropriate response to be handled by `send_message()`.\n",
        "\n",
        "* Get a `response` and `phrase` by calling `match_rule()` with the `rules` dictionary and message.\n",
        "\n",
        "* Check if the `response` is a template by seeing if it includes the string `'{0}'`. If it does:\n",
        "  * Use the `replace_pronouns()` function on phrase.\n",
        "  * Include the phrase by using `.format()` on `response` and overriding the value of `response`.\n"
      ],
      "metadata": {
        "id": "LoIUWqBdiYvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bot_template = \"BOT : {0}\"\n",
        "user_template = \"USER : {0}\"\n",
        "\n",
        "def match_rule(rules, message):\n",
        "    for pattern, responses in rules.items():\n",
        "        match = re.search(pattern, message)\n",
        "        if match is not None:\n",
        "            response = random.choice(responses)\n",
        "            var = match.group(1) if '{0}' in response else None\n",
        "            return response, var\n",
        "    return \"default\", None\n",
        "\n",
        "def send_message(message):\n",
        "    print(user_template.format(message))\n",
        "    response = respond(message)\n",
        "    print(bot_template.format(response))\n",
        "\n",
        "def replace_pronouns(message):\n",
        "\n",
        "    message = message.lower()\n",
        "    if 'me' in message:\n",
        "        return re.sub('me', 'you', message)\n",
        "    elif 'my' in message:\n",
        "        return re.sub('my', 'your', message)\n",
        "    elif 'your' in message:\n",
        "        return re.sub('your', 'my', message)\n",
        "    elif 'you' in message:\n",
        "        return re.sub('you', 'me', message)\n",
        "\n",
        "    return message\n",
        "\n",
        "rules = {'I want (.*)':           ['What would it mean if you got {0}',\n",
        "                                   'Why do you want {0}',\n",
        "                                   \"What's stopping you from getting {0}\"],\n",
        "          'do you remember (.*)': ['Did you think I would forget {0}',\n",
        "                                   \"Why haven't you been able to forget {0}\",\n",
        "                                   'What about {0}',  \n",
        "                                   'Yes .. and?'],\n",
        "          'do you think (.*)':    ['if {0}? Absolutely.', \n",
        "                                   'No chance'],\n",
        "          'if (.*)':              [\"Do you really think it's likely that {0}\",\n",
        "                                   'Do you wish that {0}',\n",
        "                                   'What do you think about {0}',\n",
        "                                   'Really--if {0}']}\n",
        "\n",
        "# Define respond()\n",
        "def respond(message):\n",
        "    # Call match_rule\n",
        "    response, phrase = match_rule(rules, message)\n",
        "    if '{0}' in response:\n",
        "        # Replace the pronouns in the phrase\n",
        "        phrase = replace_pronouns(phrase)\n",
        "        # Include the phrase in the response\n",
        "        response = response.format(phrase)\n",
        "    return response\n",
        "\n",
        "# Send the messages\n",
        "send_message(\"do you remember your last birthday\")\n",
        "send_message(\"do you think humans should be worried about AI\")\n",
        "send_message(\"I want a robot friend\")\n",
        "send_message(\"what if you could be anything you wanted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A-i8FA6jqgH",
        "outputId": "da944d8d-6a90-4684-c89d-0b7990b423b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USER : do you remember your last birthday\n",
            "BOT : Did you think I would forget my last birthday\n",
            "USER : do you think humans should be worried about AI\n",
            "BOT : if humans should be worried about ai? Absolutely.\n",
            "USER : I want a robot friend\n",
            "BOT : What would it mean if you got a robot friend\n",
            "USER : what if you could be anything you wanted\n",
            "BOT : Really--if me could be anything me wanted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Intent classification with regex I***\n",
        "\n",
        "You'll begin by implementing a very simple technique to recognize intents - looking for the presence of keywords.\n",
        "\n",
        "A dictionary, `keywords`, has already been defined. It has the intents `\"greet\"`, `\"goodbye\"`, and `\"thankyou\"` as keys, and lists of keywords as the corresponding values. For example, `keywords[\"greet\"]` is set to `\"[\"hello\",\"hi\",\"hey\"]`.\n",
        "\n",
        "Also defined is a second dictionary, `responses`, indicating how the bot should respond to each of these intents. It also has a default response with the key `\"default\"`.\n",
        "\n",
        "The function `send_message()`, along with the bot and user templates, have also already been defined. Your job in this exercise is to create a dictionary with the intents as keys and regex objects as values.\n",
        "\n",
        "* Iterate over the `keywords` dictionary, using `intent` and `keys` as your iterator variables.\n",
        "\n",
        "* Use `'|'.join(keys)` to create regular expressions to match at least one of the keywords and pass it to **`re.compile()`** to compile the regular expressions into pattern objects. Store the result as the value of the `patterns` dictionary."
      ],
      "metadata": {
        "id": "qGJG713Bym2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = {'greet': ['hello', 'hi', 'hey'], 'goodbye': ['bye', 'farewell'], 'thankyou': ['thank', 'thx']}\n",
        "\n",
        "# Define a dictionary of patterns\n",
        "patterns = {}\n",
        "\n",
        "# Iterate over the keywords dictionary\n",
        "for intent, keys in keywords.items():\n",
        "    # Create regular expressions and compile them into pattern objects\n",
        "    patterns[intent] = re.compile('|'.join(keys))\n",
        "    \n",
        "# Print the patterns\n",
        "print(patterns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEO4b9n5x1xe",
        "outputId": "a4c638bb-d1d8-4a12-badd-6b73bfc8e197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'greet': re.compile('hello|hi|hey'), 'goodbye': re.compile('bye|farewell'), 'thankyou': re.compile('thank|thx')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Intent classification with regex II***\n",
        "\n",
        "With your patterns dictionary created, it's now time to define a function to find the intent of a message.\n",
        "\n",
        "\n",
        "* Iterate over the `intent`s and `pattern`s in the `patterns` dictionary using its **`.items()`** method.\n",
        "\n",
        "* Use the **`.search()`** method of `pattern` to look for keywords in the `message`.\n",
        "\n",
        "* If there is a match, return the corresponding `intent`.\n",
        "\n",
        "* Call your `match_intent()` function inside `respond()` with message as the argument."
      ],
      "metadata": {
        "id": "QnC1ougEz4Lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "responses = {'default': 'default message',\n",
        "             'goodbye': 'goodbye for now',\n",
        "             'greet': 'Hello you! :)',\n",
        "             'thankyou': 'you are very welcome'}\n",
        "\n",
        "# Define a function to find the intent of a message\n",
        "def match_intent(message):\n",
        "    matched_intent = None\n",
        "    for intent, pattern in patterns.items():\n",
        "        # Check if the pattern occurs in the message \n",
        "        if re.search(pattern, message):\n",
        "            matched_intent = intent\n",
        "    return matched_intent\n",
        "\n",
        "# Define a respond function\n",
        "def respond(message):\n",
        "    # Call the match_intent function\n",
        "    intent = match_intent(message)\n",
        "    # Fall back to the default response\n",
        "    key = \"default\"\n",
        "    if intent in responses:\n",
        "        key = intent\n",
        "    return responses[key]\n",
        "\n",
        "# Send messages\n",
        "send_message(\"hello!\")\n",
        "send_message(\"bye byeee\")\n",
        "send_message(\"thanks very much!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR1HH7oR0yHa",
        "outputId": "b13a8e17-f391-4bcf-cbda-16bc79b61d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USER : hello!\n",
            "BOT : Hello you! :)\n",
            "USER : bye byeee\n",
            "BOT : goodbye for now\n",
            "USER : thanks very much!\n",
            "BOT : you are very welcome\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Entity extraction with regex***\n",
        "\n",
        "Now you'll use another simple method, this time for finding a person's name in a sentence, such as \"hello, my name is David Copperfield\".\n",
        "\n",
        "You'll look for the keywords `\"name\"` or `\"call(ed)\"`, and find capitalized words using regex and assume those are names. Your job in this exercise is to define a `find_name()` function to do this.\n",
        "\n",
        "* Use **`re.compile()`** to create a pattern for checking if `\"name\"` or `\"call\"` keywords occur.\n",
        "\n",
        "* Create a pattern for finding capitalized words.\n",
        "\n",
        "* Use the **`.findall()`** method on `name_pattern` to retrieve all matching words in message.\n",
        "\n",
        "* Call your `find_name()` function inside `respond()`."
      ],
      "metadata": {
        "id": "fY7l0iM52Xf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define find_name()\n",
        "def find_name(message):\n",
        "    name = None\n",
        "    # Create a pattern for checking if the keywords occur\n",
        "    name_keyword = re.compile('name|call')\n",
        "    # Create a pattern for finding capitalized words\n",
        "    name_pattern = re.compile('[A-Z]{1}[a-z]*')\n",
        "    if name_keyword.search(message):\n",
        "        # Get the matching words in the string\n",
        "        name_words = re.findall(name_pattern, message)\n",
        "        if len(name_words) > 0:\n",
        "            # Return the name if the keywords are present\n",
        "            name = ' '.join(name_words)\n",
        "    return name\n",
        "\n",
        "# Define respond()\n",
        "def respond(message):\n",
        "    # Find the name\n",
        "    name = find_name(message)\n",
        "    if name is None:\n",
        "        return \"Hi there!\"\n",
        "    else:\n",
        "        return \"Hello, {0}!\".format(name)\n",
        "\n",
        "# Send messages\n",
        "send_message(\"my name is David Copperfield\")\n",
        "send_message(\"call me Ishmael\")\n",
        "send_message(\"People call me Cassandra\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG1-0-9O19Hw",
        "outputId": "2531c875-162c-45f9-dea1-5465a310294f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USER : my name is David Copperfield\n",
            "BOT : Hello, David Copperfield!\n",
            "USER : call me Ishmael\n",
            "BOT : Hello, Ishmael!\n",
            "USER : People call me Cassandra\n",
            "BOT : Hello, People Cassandra!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(re.findall(re.compile('[A-Z]{1}[a-z]*'), 'People call me Cassandra'))\n",
        "print(re.findall(re.compile('[A-Z]{1}[a-z]*'), 'people call me Cassandra'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHpCMO4B45TE",
        "outputId": "94905d91-1715-4290-e656-296135252a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['People', 'Cassandra']\n",
            "['Cassandra']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see with the final output of send_message(), the mix of using regex while making assumptions does have its limitations.\n",
        "\n",
        "# **Word vectors**\n",
        "\n"
      ],
      "metadata": {
        "id": "uZHpZPbD6Kr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.vocab.vectors_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZuufsIF53Be",
        "outputId": "b8188203-e34f-433a-8888-eaa56d6eafe1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ไม่แน่ใจว่าทำไม `nlp.vocab.vectors_length` = 0 ไม่ใช่ 300 คือ เวอร์ชั่นมันห่างกันมาก"
      ],
      "metadata": {
        "id": "abie0dnbbhDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp('hello can you help me?')\n",
        "for token in doc:\n",
        "    print(\"{} : {}\".format(token, token.vector[:3]))"
      ],
      "metadata": {
        "id": "2dtzEyhqWFkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91769717-2799-4268-ccc0-8e4f31ce3424"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello : [ 0.12180586 -0.4320478   0.06123605]\n",
            "can : [ 1.3111997  -1.5120994  -0.92238146]\n",
            "you : [-0.6397991   0.08952954 -1.0624535 ]\n",
            "help : [-0.3975733   1.1522754  -0.75847507]\n",
            "me : [-0.4355018 -0.5858723 -0.9023175]\n",
            "? : [-0.91452414  0.3720032  -0.18198386]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`.similarity()`**\n",
        "\n",
        "* \"can\" and \"cat\" are spelled similarly but have low similarity\n",
        "* but \"cat\" and \"dog\" have high similarity"
      ],
      "metadata": {
        "id": "oNJQYH08X0XY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(\"cat\")\n",
        "doc.similarity(nlp(\"can\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YX-ub2eoXCbA",
        "outputId": "35dd092d-28ce-468a-8903-23278c9343a7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09407368320209995"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc.similarity(nlp(\"dog\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jpw0GKYHYCXT",
        "outputId": "3da22b70-c007-43ae-e9e3-eabb94df8dff"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6407916510007929"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***word vectors with spaCy***\n",
        "\n",
        "In this exercise you'll get your first experience with word vectors! You're going to use the ATIS dataset, which contains thousands of sentences from real people interacting with a flight booking system.\n",
        "\n",
        "The user utterances are available in the list `sentences`, and the corresponding intents in `labels`.\n",
        "\n",
        "Your job is to create a 2D array X with as many rows as there are sentences in the dataset, where each row is a vector describing that sentence.\n",
        "\n",
        "* Load the `spaCy` English model by calling **`spacy.load()`** with argument `'en'`.\n",
        "\n",
        "* Calculate the length of sentences using `len()` and the dimensionality of the word vectors using **`nlp.vocab.vectors_length`**.\n",
        "\n",
        "* For each sentence, call the `nlp` object with the `sentence` as the sole argument. Store the result as `doc`.\n",
        "\n",
        "* Use the **`.vector`** attribute of `doc` to get the vector representation of each sentence, and store this vector in the appropriate row of `X`."
      ],
      "metadata": {
        "id": "ZXKeXBhnYtGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [' i want to fly from boston at 838 am and arrive in denver at 1110 in the morning',\n",
        "             ' what flights are available from pittsburgh to baltimore on thursday morning',\n",
        "             ' what is the arrival time in san francisco for the 755 am flight leaving washington',\n",
        "             ' cheapest airfare from tacoma to orlando',\n",
        "             ' round trip fares from pittsburgh to philadelphia under 1000 dollars',\n",
        "             ' i need a flight tomorrow from columbus to minneapolis',\n",
        "             ' what kind of aircraft is used on a flight from cleveland to dallas',\n",
        "             ' show me the flights from pittsburgh to los angeles on thursday',\n",
        "             ' all flights from boston to washington',\n",
        "             ' what kind of ground transportation is available in denver',\n",
        "             ' show me the flights from dallas to san francisco',\n",
        "             ' show me the flights from san diego to newark by way of houston',\n",
        "             ' what is the cheapest flight from boston to bwi',\n",
        "             ' all flights to baltimore after 6 pm',\n",
        "             ' show me the first class fares from boston to denver',\n",
        "             ' show me the ground transportation in denver',\n",
        "             ' all flights from denver to pittsburgh leaving after 6 pm and before 7 pm',\n",
        "             ' i need information on flights for tuesday leaving baltimore for dallas dallas to boston and boston to baltimore',\n",
        "             ' please give me the flights from boston to pittsburgh on thursday of next week',\n",
        "             ' i would like to fly from denver to pittsburgh on united airlines',\n",
        "             ' show me the flights from san diego to newark',\n",
        "             ' please list all first class flights on united from denver to baltimore',\n",
        "             ' what kinds of planes are used by american airlines',\n",
        "             \" i'd like to have some information on a ticket from denver to pittsburgh and atlanta\",\n",
        "             \" i'd like to book a flight from atlanta to denver\",\n",
        "             ' which airline serves denver pittsburgh and atlanta',\n",
        "             \" show me all flights from boston to pittsburgh on wednesday of next week which leave boston after 2 o'clock pm\",\n",
        "             ' atlanta ground transportation',\n",
        "             ' i also need service from dallas to boston arriving by noon',\n",
        "             ' show me the cheapest round trip fare from baltimore to dallas']\n",
        "\n",
        "# Load the spacy model: nlp\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Calculate the length of sentences\n",
        "n_sentences = len(sentences)\n",
        "\n",
        "# Calculate the dimensionality of nlp\n",
        "embedding_dim = 96 # nlp.vocab.vectors_length\n",
        "\n",
        "# Initialize the array with zeros: X\n",
        "X = np.zeros((n_sentences, embedding_dim))\n",
        "\n",
        "# Iterate over the sentences\n",
        "for idx, sentence in enumerate(sentences):\n",
        "    # Pass each each sentence to the nlp object to create a document\n",
        "    doc = nlp(sentence)\n",
        "    # Save the document's .vector attribute to the corresponding row in X\n",
        "    X[idx, :] = doc.vector\n",
        "\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oujdEQnhZSOn",
        "outputId": "72c20686-ec7f-4e9b-f780-34086761c6a7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.02233886,  0.24285591,  1.14085233, ...,  0.41054299,\n",
              "        -0.23884244, -0.29071414],\n",
              "       [ 0.42830324,  0.38204679,  0.67446786, ...,  0.34716579,\n",
              "         0.08950311, -0.26096511],\n",
              "       [ 0.4628748 , -0.35164732, -0.00185303, ...,  0.53669786,\n",
              "        -0.03321095,  0.22019197],\n",
              "       ...,\n",
              "       [ 0.89593959, -0.10473762,  0.03952669, ...,  1.45879316,\n",
              "         0.47024876,  1.24406469],\n",
              "       [ 0.00615226,  0.20572662,  0.45837235, ...,  0.51501089,\n",
              "        -0.24547839, -0.24608922],\n",
              "       [-0.07881036,  0.02160925,  0.12819993, ...,  0.2332174 ,\n",
              "        -0.2767455 , -0.13803241]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now find vector representations of words and sentences with spaCy.\n",
        "\n",
        "# **ATIS dataset**"
      ],
      "metadata": {
        "id": "3x_sXb4Xc3QO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1, df2 = pd.read_csv('atis_intents_train.csv', header=None), pd.read_csv('atis_intents_test.csv', header=None)\n",
        "sentences_train, labels_train, sentences_test, labels_test = df1[1].values, df1[0].values, df2[1].values, df2[0].values\n",
        "\n",
        "X_train_shape = (len(sentences_train), 96) # nlp.vocab.vectors_length)\n",
        "X_train = np.zeros(X_train_shape)\n",
        "\n",
        "for i, sentence in enumerate(sentences_train):\n",
        "    X_train[i,:] = nlp(sentence).vector"
      ],
      "metadata": {
        "id": "9v00rnEDcUYg"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Nearest neighbor classification in scikit-learn**\n",
        "\n"
      ],
      "metadata": {
        "id": "TuCRzn76gUAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "test_message = \"\"\"i would like to find a flight from charlotte to las vegas that makes a stop in st. louis\"\"\"\n",
        "\n",
        "test_x = nlp(test_message).vector\n",
        "scores = [cosine_similarity(X_train[i,:], test_x) for i in range(len(sentences_train))]\n",
        "labels_train[np.argmax(scores)]"
      ],
      "metadata": {
        "id": "_wPOjIPNhVsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "'atis_flight'\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "KwT-jcyMh9ZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Support vector machines***"
      ],
      "metadata": {
        "id": "pEe2HSQkibtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "clf = SVC()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "R-QYvCdmiBbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Intent classification with sklearn***\n",
        "\n",
        "An array `X` containing vectors describing each of the sentences in the ATIS dataset has been created for you, along with a 1D array `y` containing the labels. The labels are integers corresponding to the intents in the dataset. For example, label `0` corresponds to the intent `atis_flight`.\n",
        "\n",
        "Now, you'll use the scikit-learn library to train a classifier on this same dataset. Specifically, you will fit and evaluate a support vector classifier.\n",
        "\n",
        "* Import the **`SVC`** class from **`sklearn.svm`**.\n",
        "* Instantiate a classifier `clf` by calling **`SVC`** with a single keyword argument **`C`** with value `1`.\n",
        "* Fit the classifier to the training data `X_train` and `y_train`.\n",
        "* Predict the labels of the test set, `X_test`."
      ],
      "metadata": {
        "id": "9r7t79kIincD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create a support vector classifier\n",
        "clf = SVC(C=1)\n",
        "\n",
        "# Fit the classifier using the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Count the number of correct predictions\n",
        "n_correct = 0\n",
        "for i in range(len(y_test)):\n",
        "    if y_pred[i] == y_test[i]:\n",
        "        n_correct += 1\n",
        "\n",
        "print(\"Predicted {0} correctly out of {1} test examples\".format(n_correct, len(y_test)))"
      ],
      "metadata": {
        "id": "jVSzbTgWjd_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Predicted 162 correctly out of 201 test examples\n",
        "```\n",
        "You just trained a support vector machine for recognizing intents!\n",
        "\n",
        "# **SpaCy Named Entity Recognition**\n"
      ],
      "metadata": {
        "id": "TqCZfAM1j2do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(\"my friend Mary has worked at Google since 2009\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV6kRDJ5j6J_",
        "outputId": "c794581c-fdc3-49a7-ce89-60b01bc7210f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mary PERSON\n",
            "Google ORG\n",
            "2009 DATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp('a flight to Shanghai from Singapore')\n",
        "shanghai, singapore = doc[3], doc[5]\n",
        "list(shanghai.ancestors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE0ILNS2xM7u",
        "outputId": "b205d168-b6be-46dc-8032-fbbee1da3539"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[to, flight]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(singapore.ancestors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbgX-9kUx8lQ",
        "outputId": "b9ac709e-519b-443f-8c90-cde0ad78f881"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[from, flight]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Using spaCy's entity recognizer***\n",
        "\n",
        "In this exercise, you'll use `spaCy`'s built-in entity recognizer to extract names, dates, and organizations from search queries. The `spaCy` library has been imported for you, and its English model has been loaded as `nlp`.\n",
        "\n",
        "Your job is to define a function called `extract_entities()`, which takes in a single argument `message` and returns a dictionary with the included entity types as keys, and the extracted entities as values. The included entity types are contained in a list called `include_entities`.\n",
        "\n",
        "* Create a dictionary called `ents` to hold the entities by calling **`dict.fromkeys()`** with `include_entities` as the sole argument.\n",
        "\n",
        "* Create a `spacy` document called `doc` by passing the `message` to the `nlp` object.\n",
        "\n",
        "* Iterate over the entities in the document (`doc.ents`).\n",
        "\n",
        "* Check whether the entity's `.label_` is one we are interested in. If so, assign the entity's `.text` attribute to the corresponding key in the `ents` dictionary."
      ],
      "metadata": {
        "id": "VCSO5CTXzNIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define included_entities\n",
        "include_entities = ['DATE', 'ORG', 'PERSON']\n",
        "\n",
        "# Define extract_entities()\n",
        "def extract_entities(message):\n",
        "    # Create a dict to hold the entities\n",
        "    ents = dict.fromkeys(include_entities)\n",
        "    # Create a spacy document\n",
        "    doc = nlp(message)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in include_entities:\n",
        "            # Save interesting entities\n",
        "            ents[ent.label_] = ent.text\n",
        "    return ents\n",
        "\n",
        "print(extract_entities('friends called Mary who have worked at Google since 2010'))\n",
        "print(extract_entities('people who graduated from MIT in 1999'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg241VJ0yhG3",
        "outputId": "2de10ad1-a4de-4343-e52b-5f1d0984eb7e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'DATE': '2010', 'ORG': 'Google', 'PERSON': 'Mary'}\n",
            "{'DATE': '1999', 'ORG': 'MIT', 'PERSON': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your bot can now make use of spaCy's built-in entity recognizer.\n",
        "\n",
        "## ***Assigning roles using spaCy's parser***\n",
        "\n",
        "In this exercise you'll use `spaCy`'s powerful syntax parser to assign roles to the entities in your users' messages. To do this, you'll define two functions, `find_parent_item()` and `assign_colors()`. In doing so, you'll use a parse tree to assign roles.\n",
        "\n",
        "Recall that you can access the ancestors of a word using its **`.ancestors`** attribute.\n",
        "\n",
        "* Create a spacy document called `doc` by passing the message `\"let's see that jacket in red and some blue jeans\"` to the `nlp` object.\n",
        "\n",
        "* In the `find_parent_item(word)` function, iterate over the ancestors of each `word` until an `entity_type()` of `\"item\"` is found.\n",
        "\n",
        "* In the `assign_colors(doc)` function, iterate over the `doc` until an `entity_type` of `\"color\"` is found. Then, find the parent item of this `word`.\n",
        "\n",
        "* Pass in the `spacy` document to the `assign_colors()` function."
      ],
      "metadata": {
        "id": "1pxv32C91caw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colors, items = ['black', 'red', 'blue'], ['shoes', 'handback', 'jacket', 'jeans']\n",
        "\n",
        "def entity_type(word):  \n",
        "    _type = None\n",
        "    if word.text in colors:\n",
        "        _type = \"color\"\n",
        "    elif word.text in items:\n",
        "        _type = \"item\"\n",
        "    return _type\n",
        "\n",
        "# Create the document\n",
        "doc = nlp(\"let's see that jacket in red and some blue jeans\")\n",
        "\n",
        "# Iterate over parents in parse tree until an item entity is found\n",
        "def find_parent_item(word):\n",
        "    # Iterate over the word's ancestors\n",
        "    for parent in word.ancestors:\n",
        "        # Check for an \"item\" entity\n",
        "        if entity_type(parent) == \"item\":\n",
        "            return parent.text\n",
        "    return None\n",
        "\n",
        "# For all color entities, find their parent item\n",
        "def assign_colors(doc):\n",
        "    # Iterate over the document\n",
        "    for word in doc:\n",
        "        # Check for \"color\" entities\n",
        "        if entity_type(word) == \"color\":\n",
        "            # Find the parent\n",
        "            item =  find_parent_item(word)\n",
        "            print(\"item: {0} has color : {1}\".format(item, word))\n",
        "\n",
        "# Assign the colors\n",
        "assign_colors(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-_Mqrk41XE2",
        "outputId": "9a6a82be-8f3d-43d7-ea6f-a3dfa891c3d1"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "item: jacket has color : red\n",
            "item: jeans has color : blue\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your bot can now figure out simple relationships between entities.\n",
        "\n",
        "* คุณสมบัติ **`.ancestors`** อาจจะไปเอาคำที่อยู่ก่อนหน้า หรือตามหลังมาก็ได้ ในตัวอย่างนี้เอาคำก่อนหน้ามาเลยถูก\n",
        "\n",
        "# **Robust Language Understanding**\n",
        "# **Rasa data format**"
      ],
      "metadata": {
        "id": "um1ZbIdl6DAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rasa_nlu import load_data\n",
        "import json\n",
        "\n",
        "training_data = load_data(\"./training_data.json\")\n",
        "print(json.dumps(data.training_examples[22], indent=2))"
      ],
      "metadata": {
        "id": "uiv62Z3QtxrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "{ \"text\"    : \"i'm looking for a place in the north of town\",\n",
        "  \"intent\"  : \"restaurant_search\",\n",
        "  \"entities\": [{\"start\" : 31,\n",
        "                \"end\"   : 36,\n",
        "                \"value\" : \"north\",\n",
        "                \"entity\": \"location\" }]}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "zkxl9aqu8zTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Interpreters**\n",
        "\n"
      ],
      "metadata": {
        "id": "R4sDHGsw9qH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = \"I want to book a flight to London\"\n",
        "interpreter.parse(message))"
      ],
      "metadata": {
        "id": "z4b7iiMV5LPq"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "{ \"intent\"  : {\"name\"       : \"flight_search\",\n",
        "               \"confidence\" : 0.9},\n",
        "  \"entities\": [{\"entity\": \"location\", \n",
        "                \"value\" : \"London\",\n",
        "                \"start\" : 27,\n",
        "                \"end\"   : 33}]\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "n8q5KnRA9yOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **How to create an interpreter**"
      ],
      "metadata": {
        "id": "b1Hx9ArQssxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a model\n",
        "from rasa_nlu.config import RasaNLUConfig\n",
        "from rasa_nlu.model import Trainer\n",
        "\n",
        "config = RasaNLUConfig(cmdline_args={\"pipeline\": \"spacy_sklearn\"})\n",
        "trainer = Trainer(config)\n",
        "interpreter = trainer.train(training_data)"
      ],
      "metadata": {
        "id": "KqZNBXAsuCEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rasa pipelines**\n",
        "\n",
        "* A Rasa pipeline is a list of components that will be used to process text\n",
        "  * `nlp_spacy` initializes the spacy English model\n",
        "  * `ner_crf` uses a *c*onditional *r*andom *f*ield entity recognizer\n",
        "  * `ner_sysnonyms` หาคำเหมือนเช่น NY กับ New York\n",
        "  * `intent_featurizer_spacy` สร้างเวคเตอร์ของประโยค\n",
        "  * `intent_classifier_sklearn` ใช้ Support Vector Classifer มาแยกแยะ intent\n",
        "  * `intent_featurizer_ngrams` ใช้แก้ไขคำที่พิมพ์ผิด"
      ],
      "metadata": {
        "id": "f0Rt7temufHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_sklearn_pipeline = [\"nlp_spacy\",\n",
        "                          \"ner_crf\",\n",
        "                          \"ner_synonyms\",\n",
        "                          \"intent_featurizer_spacy\",\n",
        "                          \"intent_classifier_sklearn\",\n",
        "                          \"intent_featurizer_ngrams\"]\n",
        "\n",
        "# These two statements are identical:\n",
        "RasaNLUConfig(cmdline_args={\"pipeline\": spacy_sklearn_pipeline})\n",
        "RasaNLUConfig(cmdline_args={\"pipeline\": \"spacy_sklearn\"})"
      ],
      "metadata": {
        "id": "vpPDQduKug5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conditional random fields (CRF)**\n",
        "\n",
        "* Machine Learning model, popular for named entity\n",
        "recognition, can perform well even with small training data\n",
        "\n",
        "## ***Rasa NLU***\n",
        "\n",
        "In this exercise, you'll use Rasa NLU to create an `interpreter`, which parses incoming user messages and returns a set of entities. Your job is to train an `interpreter` using the MITIE entity recognition model in Rasa NLU.\n",
        "\n",
        "\n",
        "* Create a dictionary called `args` with a single key `\"pipeline\"` with value `\"spacy_sklearn\"`.\n",
        "\n",
        "* Create a `config` by calling `RasaNLUConfig()` with the single argument `cmdline_args` with value `args`.\n",
        "\n",
        "* Create a `trainer` by calling `Trainer()` using the configuration as the argument.\n",
        "\n",
        "* Create a `interpreter` by calling `trainer.train()` with the `training_data`."
      ],
      "metadata": {
        "id": "YQ5JbeYHy4M4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from rasa_nlu.converters import load_data\n",
        "from rasa_nlu.config import RasaNLUConfig\n",
        "from rasa_nlu.model import Trainer\n",
        "\n",
        "# Create args dictionary\n",
        "args = {\"pipeline\":\"spacy_sklearn\"}\n",
        "\n",
        "# Create a configuration and trainer\n",
        "config = RasaNLUConfig(cmdline_args=args)\n",
        "trainer = Trainer(config)\n",
        "\n",
        "# Load the training data\n",
        "training_data = load_data(\"./training_data.json\")\n",
        "\n",
        "# Create an interpreter by training the model\n",
        "interpreter = trainer.train(training_data)\n",
        "\n",
        "# Test the interpreter\n",
        "print(interpreter.parse(\"I'm looking for a Mexican restaurant in the North of town\"))"
      ],
      "metadata": {
        "id": "tH0Be_4_22Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
        "    \n",
        "{'intent': {'name': 'restaurant_search', 'confidence': 0.6627604390878398},\n",
        " 'entities': [{'start': 18, 'end': 25, 'value': 'mexican', 'entity': 'cuisine'\n",
        "               'extractor': 'ner_crf'}, \n",
        "              {'start': 44, 'end': 49, 'value': 'north', 'entity': 'location',\n",
        "               'extractor': 'ner_crf'}], \n",
        " 'intent_ranking': [{'name': 'restaurant_search', 'confidence': 0.6627604390878398}, \n",
        "                    {'name': 'goodbye', 'confidence': 0.14633725788681204},\n",
        "                    {'name': 'affirm', 'confidence': 0.09756426473688806},\n",
        "                    {'name': 'greet', 'confidence': 0.09333803828846025}], \n",
        " 'text': \"I'm looking for a Mexican restaurant in the North of town\"}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "WelSbYGG34FO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You just trained an intent and entity recognizer without having to create any arrays.\n",
        "\n",
        "## ***Data-efficient entity recognition***\n",
        "\n",
        "Most systems for extracting entities from text are built to extract 'Universal' things like names, dates, and places. But you probably don't have enough training data for your bot to make these systems perform well!\n",
        "\n",
        "In this exercise, you'll activate the MITIE entity recognizer inside Rasa to extract restaurants-related entities using a very small amount of training data. A dictionary `args` has already been defined for you, along with a `training_data` object.\n",
        "\n",
        "* Create a config by calling `RasaNLUConfig()` with a single argument `cmdline_args` with value `{\"pipeline\": pipeline}`.\n",
        "* Create a `trainer` and use it to create an `interpreter`, just as you did in the previous exercise."
      ],
      "metadata": {
        "id": "4KhnFaxe5LYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from rasa_nlu.config import RasaNLUConfig\n",
        "from rasa_nlu.model import Trainer\n",
        "\n",
        "pipeline = [\"nlp_spacy\", \"tokenizer_spacy\", \"ner_crf\"]\n",
        "\n",
        "# Create a config that uses this pipeline\n",
        "config = RasaNLUConfig(cmdline_args={\"pipeline\": pipeline})\n",
        "# Create a trainer that uses this config\n",
        "trainer = Trainer(config)\n",
        "\n",
        "# Create an interpreter by training the model\n",
        "interpreter = trainer.train(training_data)\n",
        "\n",
        "# Parse some messages\n",
        "print(interpreter.parse(\"show me Chinese food in the centre of town\"))\n",
        "print(interpreter.parse(\"I want an Indian restaurant in the west\"))\n",
        "print(interpreter.parse(\"are there any good pizza places in the center?\"))"
      ],
      "metadata": {
        "id": "jtQZeBAQ4Fx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " You just built a custom entity recogniser with Rasa NLU.\n",
        "\n",
        " # **SQLite with Python**\n",
        "\n"
      ],
      "metadata": {
        "id": "R51MSqGf6BSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect('hotels.db')\n",
        "c = conn.cursor()\n",
        "c.execute(\"SELECT * FROM hotels WHERE area='south' and price='hi'\")\n",
        "c.fetchall()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MabMgeodQ0bM",
        "outputId": "3d76be61-927c-422d-b92a-bda323f512fe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Grand Hotel', 'hi', 'south', 5)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c.execute(\"SELECT name from hotels where price = 'mid' AND area = 'north'\").fetchall()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy-SpNElS2p7",
        "outputId": "28480c8b-29c5-4d07-97a3-9efc1afd29f8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Hotel California',)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***SQL statements in Python***\n",
        "\n",
        "It's time to begin writing SQL queries! In this exercise, your job is to run a query against the hotels database to find all the expensive hotels in the south. The connection to the database has been created for you, along with a cursor `c`.\n",
        "\n",
        "As Alan described in the video, you should be careful about SQL injection. Here, you'll pass parameters the safe way: As an extra tuple argument to the **`.execute()`** method. This ensures malicious code can't be injected into your query.\n",
        "\n",
        "\n",
        "* Define a tuple `t` of strings `\"south\"` and `\"hi\"` for the `area` and `price`.\n",
        "* Execute the query using the cursor's **`.execute()`** method. You're looking for all of the fields for all `hotels` where the area is `\"south\"` and the price is `\"hi\"`.\n",
        "* Print the results using the cursor's `.fetchall()` method."
      ],
      "metadata": {
        "id": "_sVDwzOaURW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import sqlite3\n",
        "import sqlite3\n",
        "\n",
        "# Open connection to DB\n",
        "conn = sqlite3.connect('hotels.db')\n",
        "\n",
        "# Create a cursor\n",
        "c = conn.cursor()\n",
        "\n",
        "# Define area and price\n",
        "area, price = \"south\", \"hi\"\n",
        "t = (area, price)\n",
        "\n",
        "# Execute the query\n",
        "c.execute('SELECT * FROM hotels WHERE area=? AND price=?', t)\n",
        "\n",
        "# Print the results\n",
        "print(c.fetchall())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyLlDctIUDD6",
        "outputId": "3d802644-6872-4a4a-b0ca-8ee6319b73f2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Grand Hotel', 'hi', 'south', 5)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parameters from text**"
      ],
      "metadata": {
        "id": "zPekgCONba0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = \"a cheap hotel in the north\"\n",
        "data = interpreter.parse(message)\n",
        "data"
      ],
      "metadata": {
        "id": "KNcfMEIdVzy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "{\n",
        "  'entities' : [{'end': '7', 'entity': 'price', 'start': 2, 'value': 'lo'}, \n",
        "                {'end': 26, 'entity': 'location', 'start': 21, 'value': 'north'}],\n",
        "  'intent'   :  {'confidence': 0.9, 'name': 'hotel_search'}\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "TQqg6LWsbj_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {}\n",
        "for ent in data[\"entities\"]:\n",
        "    params[ent[\"entity\"]] = ent[\"value\"]\n",
        "params"
      ],
      "metadata": {
        "id": "09tR3Q1-btUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "{'location': 'north', 'price': 'lo'}\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xKaiQd2rcSSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"SELECT name FROM hotels\"\n",
        "filters = [\"{}=?\".format(k) for k in params.keys()]\n",
        "filters"
      ],
      "metadata": {
        "id": "cpsj0gK3cgR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "['price=?', 'location=?']\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iP6RAQ3Ccn3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conditions = \" and \".join(filters)\n",
        "conditions"
      ],
      "metadata": {
        "id": "L0LRgxVxctOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "'price=? and location=?'\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oPERP9btcuDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_q = \" WHERE \".join([query, conditions])\n",
        "final_q"
      ],
      "metadata": {
        "id": "dwyJ5Natczky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "'SELECT name FROM hotels WHERE price=? and location=?'\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "lS5Bb6Hkc3kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Responses**"
      ],
      "metadata": {
        "id": "oRlmiwCJdUhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "responses = [\"I'm sorry :( I couldn't find anything like that\",\n",
        "             \"what about {}?\",\n",
        "             \"{} is one option, but I know others too :)\"]"
      ],
      "metadata": {
        "id": "9g32C0rvc6Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Creating queries from parameters***\n",
        "\n",
        "Now you're going to implement a more powerful function for querying the hotels database. The goal is for that function to take arguments that can later be specified by other parts of your code.\n",
        "\n",
        "More specifically, your job is to define a `find_hotels()` function which takes a single argument - a dictionary of column names and values - and returns a list of matching hotels from the database.\n",
        "\n",
        "* A `filters` list has been created for you. Join this list together with the strings `\" WHERE \"` and `\" and \"`.\n",
        "\n",
        "* Create a tuple of the values of the `params` dictionary.\n",
        "\n",
        "* Create a connection and cursor to `\"hotels.db\"` and then execute the `query`, just as in the previous exercise.\n",
        "\n",
        "* Return the results of the query."
      ],
      "metadata": {
        "id": "-_kc14ktdq7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define find_hotels()\n",
        "def find_hotels(params):\n",
        "    # Create the base query\n",
        "    query = 'SELECT * FROM hotels'\n",
        "    # Add filter clauses for each of the parameters\n",
        "    if len(params) > 0:\n",
        "        filters = [\"{}=?\".format(k) for k in params]\n",
        "        query += \" WHERE \" + \" AND \".join(filters)\n",
        "    # Create the tuple of values\n",
        "    t = tuple(params.values())\n",
        "    \n",
        "    # Open connection to DB\n",
        "    conn = sqlite3.connect(\"hotels.db\")\n",
        "    # Create a cursor\n",
        "    c = conn.cursor()\n",
        "    # Execute the query\n",
        "    c.execute(query, t)\n",
        "    # Return the results\n",
        "    return c.fetchall()"
      ],
      "metadata": {
        "id": "ymuw1S6AejFE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You've now got a function that can find matching hotels for any area and price range combination. \n",
        "\n",
        "## ***Using your custom function to find hotels***\n",
        "\n",
        "Here, you'll see your `find_hotels()` function in action! Recall that it accepts a single argument, `params`, which is a dictionary of column names and values.\n",
        "\n",
        "* Create the `params` dictionary with the column names (keys) `\"area\"` and `\"price\"`, with corresponding values `\"south\"` and `\"lo\"`.\n",
        "\n",
        "* Use the `find_hotels()` function along with your `params` dictionary to find all inexpensive hotels in the South."
      ],
      "metadata": {
        "id": "QMJK0HKjgjDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dictionary of column names and values\n",
        "params = {'area':'south', 'price':'lo'}\n",
        "\n",
        "# Find the hotels that match the parameters\n",
        "print(find_hotels(params))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sk_iWaw5iEQV",
        "outputId": "ee4f9651-fe25-4e4f-ec91-7c3d804005c8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Cozy Cottage', 'lo', 'south', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Creating SQL from natural language***\n",
        "\n",
        "Now you'll write a `respond()` function that can handle messages like `\"I want an expensive hotel in the south of town\"` and respond appropriately according to the number of matching results in a database. This is an important functionality for any database-backed chatbot.\n",
        "\n",
        "Your `find_hotels()` function from the previous exercises has already been defined for you, along with a Rasa NLU `interpreter` object, which can handle hotel queries, and a list of `responses`, which you can explore in the Shell.\n",
        "\n",
        "\n",
        "* Use the **`.parse()`** method of `interpreter` to extract the `\"entities\"` in the `message`.\n",
        "\n",
        "* Find matching hotels using the `params` dictionary and `find_hotels()` function.\n",
        "\n",
        "* Use the **`min()`** function to choose the right index for the response to send. In this case, `n` is the number of results.\n",
        "\n",
        "* Select the appropriate response from the `responses` list and insert the `names` of hotels using the `.format()` method."
      ],
      "metadata": {
        "id": "GgFS_QQOic_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "responses = [\"I'm sorry :( I couldn't find anything like that\",\n",
        "             '{} is a great hotel!',\n",
        "             '{} or {} would work!',\n",
        "             '{} is one option, but I know others too :)']\n",
        "\n",
        "# Define respond()\n",
        "def respond(message):\n",
        "    # Extract the entities\n",
        "    entities = interpreter.parse(message)[\"entities\"]\n",
        "    # Initialize an empty params dictionary\n",
        "    params = {}\n",
        "    # Fill the dictionary with entities\n",
        "    for ent in entities:\n",
        "        params[ent[\"entity\"]] = str(ent[\"value\"])\n",
        "\n",
        "    # Find hotels that match the dictionary\n",
        "    results = find_hotels(params)\n",
        "    # Get the names of the hotels and index of the response\n",
        "    names = [r[0] for r in results]\n",
        "    n = min(len(results),3)\n",
        "    # Select the nth element of the responses array\n",
        "    return responses[n].format(*names)\n",
        "\n",
        "# Test the respond() function\n",
        "print(respond(\"I want an expensive hotel in the south of town\"))"
      ],
      "metadata": {
        "id": "gi57lxdLlPtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Grand Hotel is a great hotel!\n",
        "```\n",
        "\n",
        "## ***Refining your search***\n",
        "\n",
        "Now you'll write a bot that allows users to add filters incrementally, just in case they don't specify all of their preferences in one message.\n",
        "\n",
        "To do this, initialize an empty dictionary `params` outside of your `respond()` function (as opposed to inside the function, like in the previous exercise). Your `respond()` function will take in this dictionary as an argument.\n",
        "\n",
        "\n",
        "* Define a `respond()` function that accepts two arguments - a `message` and a dictionary of `params` - and returns two results - the `message` to send to the user and the updated `params` dictionary.\n",
        "\n",
        "* Extract `\"entities\"` from the message using the **`.parse()`** method of the `interpreter`, exactly like you did in the previous exercise.\n",
        "\n",
        "* Find the hotels that match `params` using your `find_hotels()` function.\n",
        "\n",
        "* Initialize the params dictionary outside the respond() function and pass the messages to the bot.\n"
      ],
      "metadata": {
        "id": "QJqD4tkBlW9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_hotels(params):\n",
        "    query = 'SELECT * FROM hotels'\n",
        "    if len(params) > 0:\n",
        "        filters = [\"{}=?\".format(k) for k in params]\n",
        "        query += \" WHERE \" + \" and \".join(filters)\n",
        "    t = tuple(params.values())\n",
        "    \n",
        "    # open connection to DB\n",
        "    conn = sqlite3.connect('hotels.db')\n",
        "    # create a cursor\n",
        "    c = conn.cursor()\n",
        "    c.execute(query, t)\n",
        "    return c.fetchall()\n",
        "\n",
        "# Define a respond function, taking the message and existing params as input\n",
        "def respond(message, params):\n",
        "    # Extract the entities\n",
        "    entities = interpreter.parse(message)[\"entities\"]\n",
        "    # Fill the dictionary with entities\n",
        "    for ent in entities:\n",
        "        params[ent[\"entity\"]] = str(ent[\"value\"])\n",
        "\n",
        "    # Find the hotels\n",
        "    results = find_hotels(params)\n",
        "    names = [r[0] for r in results]\n",
        "    n = min(len(results), 3)\n",
        "    # Return the appropriate response\n",
        "    return responses[n].format(*names), params\n",
        "\n",
        "# Initialize params dictionary\n",
        "params = {}\n",
        "\n",
        "# Pass the messages to the bot\n",
        "for message in [\"I want an expensive hotel\", \"in the north of town\"]:\n",
        "    print(\"USER: {}\".format(message))\n",
        "    response, params = respond(message, params)\n",
        "    print(\"BOT: {}\".format(response))"
      ],
      "metadata": {
        "id": "8Q41g9pMlYor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "USER: I want an expensive hotel\n",
        "BOT: Grand Hotel is one option, but I know others too :)\n",
        "USER: in the north of town\n",
        "BOT: Ben's BnB is a great hotel!\n",
        "```\n",
        "Your chatbot can now help users even when they split their preferences over a few messages.\n",
        "\n",
        "## ***Basic negation***\n",
        "\n",
        "Quite often, you'll find your users telling you what they don't want - and that's important to understand! In general, negation is a difficult problem in NLP. Here, we'll take a very simple approach that works for many cases.\n",
        "\n",
        "A list of tests called `tests` has been defined for you. Each test is a tuple consisting of:\n",
        "\n",
        "- A string containing a message with entities.\n",
        "- A dictionary containing the entities as keys and a Boolean saying whether they are negated as the key.\n",
        "\n",
        "Your job is to define a function called `negated_ents()` which looks for negated entities in a message.\n",
        "\n",
        "\n",
        "- Using list comprehension, check if the words `\"south\"` or `\"north\"` appear in the message and extract those entities.\n",
        "\n",
        "- Split the sentence into chunks ending with each entity. To do this:\n",
        "\n",
        "  * Use the **`.index()`** method of `phrase` to find the starting index of each entity `e` and add the entity's length to it to find the index of the end of the entity.\n",
        "\n",
        "  * Starting with `start=0`, take slices of the string from `start` to `end` for each `end` in `ends`. Append each slice of the sentence to the list, chunks. Ensure you update your starting position with each iteration.\n",
        "For each entity, if `\"not\"` or `\"n't\"` appears in the chunk, consider this entity negated.\n"
      ],
      "metadata": {
        "id": "PWPecqQkPlAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tests = [(\"no I don't want to be in the south\", {'south': False}),\n",
        "         ('no it should be in the south', {'south': True}),\n",
        "         ('no in the south not the north', {'north': False, 'south': True}),\n",
        "         ('not north', {'north': False})]\n",
        "\n",
        "# Define negated_ents()\n",
        "def negated_ents(phrase):\n",
        "    # Extract the entities using keyword matching\n",
        "    ents = [e for e in [\"south\", \"north\"] if e in phrase]\n",
        "    # Find the index of the final character of each entity\n",
        "    ends = sorted([phrase.index(e) + len(e) for e in ents])\n",
        "    # Initialise a list to store sentence chunks\n",
        "    chunks = []\n",
        "    # Take slices of the sentence up to and including each entitiy\n",
        "    start = 0\n",
        "    for end in ends:\n",
        "        chunks.append(phrase[start:end])\n",
        "        start = end\n",
        "    result = {}\n",
        "    # Iterate over the chunks and look for entities\n",
        "    for chunk in chunks:\n",
        "        for ent in ents:\n",
        "            if ent in chunk:\n",
        "                # If the entity contains a negation, assign the key to be False\n",
        "                if \"not\" in chunk or \"n't\" in chunk:\n",
        "                    result[ent] = False\n",
        "                else:\n",
        "                    result[ent] = True\n",
        "    return result  \n",
        "\n",
        "# Check that the entities are correctly assigned as True or False\n",
        "for test in tests:\n",
        "    print(negated_ents(test[0]) == test[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41slhw_GPu8l",
        "outputId": "6df623ab-b30b-46d4-9015-7101b05c3935"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phrase = \"no I don't want to be in the south\"\n",
        "\n",
        "ents = [e for e in [\"south\", \"north\"] if e in phrase]\n",
        "\n",
        "print(ents, \"\\n\")\n",
        "\n",
        "ends = sorted([phrase.index(e) + len(e) for e in ents])\n",
        "\n",
        "print(ends)\n",
        "\n",
        "chunks = []\n",
        "start = 0\n",
        "\n",
        "for end in ends:\n",
        "    chunks.append(phrase[start:end])\n",
        "    start = end\n",
        "\n",
        "print(chunks)\n",
        "\n",
        "result = {}\n",
        "\n",
        "for chunk in chunks:\n",
        "    for ent in ents:\n",
        "        if ent in chunk:\n",
        "            if \"not\" in chunk or \"n't\" in chunk:\n",
        "                result[ent] = False\n",
        "            else:\n",
        "                result[ent] = True\n",
        "\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Kf6zNAfVZBl",
        "outputId": "c18b2f06-e380-4010-a537-305acf753ff0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['south'] \n",
            "\n",
            "[34]\n",
            "[\"no I don't want to be in the south\"]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'south': False}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Filtering with excluded slots***\n",
        "\n",
        "Now you're going to put together some of the ideas from previous exercises in order to allow users to tell your bot about what they do and do not want, split across multiple messages.\n",
        "\n",
        "The `negated_ents()` function has already been defined for you. Additionally, a slightly tweaked version of the `find_hotels()` function, which accepts a `neg_params` dictionary in addition to a `params` dictionary, has been defined.\n",
        "\n",
        "\n",
        "* Define a `respond()` function which accepts a `message`, `params`, and `neg_params` as arguments.\n",
        "\n",
        "* Use the `negated_ents()` function with `message` and `ent_vals` as arguments. Store the result in `negated`.\n",
        "\n",
        "* Use the tweaked `find_hotels()` function with the `params` and `neg_params` dictionaries as arguments to find matching hotels. Store the result in `results`.\n",
        "\n",
        "* Initialize the `params` and `neg_params` dictionaries outside the `respond()` function."
      ],
      "metadata": {
        "id": "hx0gMXk3XMK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def negated_ents(phrase, ent_vals):\n",
        "    ents = [e for e in ent_vals if e in phrase]\n",
        "    ends = sorted([phrase.index(e)+len(e) for e in ents])\n",
        "    start = 0\n",
        "    chunks = []\n",
        "    for end in ends:\n",
        "        chunks.append(phrase[start:end])\n",
        "        start = end\n",
        "    result = {}\n",
        "    for chunk in chunks:\n",
        "        for ent in ents:\n",
        "            if ent in chunk:\n",
        "                if \"not\" in chunk or \"n't\" in chunk:\n",
        "                    result[ent] = False\n",
        "                else:\n",
        "                    result[ent] = True\n",
        "    return result  \n",
        "\n",
        "def find_hotels(params, neg_params):\n",
        "    query = 'SELECT * FROM hotels'\n",
        "    if len(params) > 0:\n",
        "        filters = [\"{}=?\".format(k) for k in params] + [\"{}!=?\".format(k) for k in neg_params] \n",
        "        query += \" WHERE \" + \" and \".join(filters)\n",
        "    t = tuple(params.values())\n",
        "    \n",
        "    # open connection to DB\n",
        "    conn = sqlite3.connect('hotels.db')\n",
        "    # create a cursor\n",
        "    c = conn.cursor()\n",
        "    c.execute(query, t)\n",
        "    return c.fetchall()\n",
        "\n",
        "################################################################################\n",
        "\n",
        "# Define the respond function\n",
        "def respond(message, params, neg_params):\n",
        "    # Extract the entities\n",
        "    entities = interpreter.parse(message)[\"entities\"]\n",
        "    ent_vals = [e[\"value\"] for e in entities]\n",
        "    # Look for negated entities\n",
        "    negated = negated_ents(message, ent_vals)\n",
        "    for ent in entities:\n",
        "        if ent[\"value\"] in negated and negated[ent[\"value\"]]:\n",
        "            neg_params[ent[\"entity\"]] = str(ent[\"value\"])\n",
        "        else:\n",
        "            params[ent[\"entity\"]] = str(ent[\"value\"])\n",
        "    # Find the hotels\n",
        "    results = find_hotels(params, neg_params)\n",
        "    names = [r[0] for r in results]\n",
        "    n = min(len(results),3)\n",
        "    # Return the correct response\n",
        "    return responses[n].format(*names), params, neg_params\n",
        "\n",
        "# Initialize params and neg_params\n",
        "params = {}\n",
        "neg_params = {}\n",
        "\n",
        "# Pass the messages to the bot\n",
        "for message in [\"I want a cheap hotel\", \"but not in the north of town\"]:\n",
        "    print(\"USER: {}\".format(message))\n",
        "    response, params, neg_params = respond(message, params, neg_params)\n",
        "    print(\"BOT: {}\".format(response))"
      ],
      "metadata": {
        "id": "WDtB6foyWsHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "USER: I want a cheap hotel\n",
        "BOT: Cozy Cottage is a great hotel!\n",
        "USER: but not in the north of town\n",
        "BOT: I'm sorry :( I couldn't find anything like that\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "yrDYdtylrbpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bot's State**\n",
        "\n",
        "## ***Form filling***\n",
        "\n",
        "You'll often want your bot to guide users through a series of steps, such as when they're placing an order.\n",
        "\n",
        "In this exercise, you'll begin building a bot that lets users order coffee. They can choose between two types: Colombian and Kenyan. If the user provides unexpected input, your bot will handle this differently depending on where they are in the flow.\n",
        "\n",
        "Your job here is to identify the appropriate state and next state based on the intents and response messages provided. For example, if the intent is `\"order\"`, then the state changes from `INIT` to `CHOOSE_COFFEE`.\n",
        "\n",
        "A function `send_message(policy, state, message)` has already been defined for you. It takes the `policy`, the current `state`, and `message` as arguments, and returns the new `state` as a result. Additionally, an `interpret(message)` function has been pre-defined.\n",
        "\n",
        "* Define three states: `INIT` with value `0`, `CHOOSE_COFFEE` with value `1`, and `ORDERED` with value `2`.\n",
        "\n",
        "* Create a dictionary called `policy` with tuples as keys and values. \n",
        "\n",
        "  - Each key is a tuple containing a state and an intent, and \n",
        "  - each value is a tuple containing the next state and the response message.\n",
        "   \n",
        "- The messages have been filled in for you. Your job is to fill in the states.\n",
        "\n",
        "- Instantiate a variable `state` with the value `INIT`.\n",
        "\n",
        "- For each of the messages, call the `send_message()` function, passing in the `policy`, `state`, and `message`."
      ],
      "metadata": {
        "id": "rnYRs8wgUDfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def send_message(policy, state, message):\n",
        "    print(\"USER : {}\".format(message))\n",
        "    new_state, response = respond(policy, state, message)\n",
        "    print(\"BOT : {}\".format(response))\n",
        "    return new_state\n",
        "\n",
        "def respond(policy, state, message):\n",
        "    (new_state, response) = policy[(state, interpret(message))]\n",
        "    return new_state, response\n",
        "\n",
        "def interpret(message):\n",
        "    msg = message.lower()\n",
        "    if 'order' in msg:\n",
        "        return 'order'\n",
        "    if 'kenyan' in msg or 'colombian' in msg:\n",
        "        return 'specify_coffee'\n",
        "    return 'none'\n",
        "\n",
        "# Define the INIT state\n",
        "INIT = 0\n",
        "\n",
        "# Define the CHOOSE_COFFEE state\n",
        "CHOOSE_COFFEE = 1\n",
        "\n",
        "# Define the ORDERED state\n",
        "ORDERED = 2\n",
        "\n",
        "# Define the policy rules\n",
        "policy = {\n",
        "    (INIT, \"order\"): (CHOOSE_COFFEE, \"ok, Colombian or Kenyan?\"),\n",
        "    (INIT, \"none\"): (INIT, \"I'm sorry - I'm not sure how to help you\"),\n",
        "    (CHOOSE_COFFEE, \"specify_coffee\"): (ORDERED, \"perfect, the beans are on their way!\"),\n",
        "    (CHOOSE_COFFEE, \"none\"): (CHOOSE_COFFEE, \"I'm sorry - would you like Colombian or Kenyan?\"),\n",
        "}\n",
        "\n",
        "# Create the list of messages\n",
        "messages = [\n",
        "    \"I'd like to become a professional dancer\",\n",
        "    \"well then I'd like to order some coffee\",\n",
        "    \"my favourite animal is a zebra\",\n",
        "    \"kenyan\"\n",
        "]\n",
        "\n",
        "# Call send_message() for each message\n",
        "state = INIT\n",
        "for message in messages:    \n",
        "    state = send_message(policy, state, message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRRmZ7w3rg9i",
        "outputId": "d389033e-75b7-4269-ff7a-f5fbb024a72f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USER : I'd like to become a professional dancer\n",
            "BOT : I'm sorry - I'm not sure how to help you\n",
            "USER : well then I'd like to order some coffee\n",
            "BOT : ok, Colombian or Kenyan?\n",
            "USER : my favourite animal is a zebra\n",
            "BOT : I'm sorry - would you like Colombian or Kenyan?\n",
            "USER : kenyan\n",
            "BOT : perfect, the beans are on their way!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Asking contextual questions***\n",
        "\n",
        "Sometimes your users need some help! They will have questions and expect the bot to help them.\n",
        "\n",
        "In this exercise, you'll allow users to ask the coffee bot to explain the steps to them. As in the previous exercise, the answer they get will depend on where they are in the flow.\n",
        "\n",
        "- Add two rules to your `policy_rules` to handle the intent `\"ask_explanation\"` when in the states `INIT` or `CHOOSE_COFFEE`.\n",
        "\n",
        "- Inside the `send_messages()` function, call the `send_message()` function with `state` and `msg` as arguments to define the new state. "
      ],
      "metadata": {
        "id": "c0QGQnuQXcJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def send_message(state, message):\n",
        "    print(\"USER : {}\".format(message))\n",
        "    new_state, response = respond(state, message)\n",
        "    print(\"BOT : {}\".format(response))\n",
        "    return new_state\n",
        "\n",
        "def respond(state, message):\n",
        "    (new_state, response) = policy_rules[(state, interpret(message))]\n",
        "    return new_state, response\n",
        "\n",
        "def interpret(message):\n",
        "    msg = message.lower()\n",
        "    if 'order' in msg:\n",
        "        return 'order'\n",
        "    if 'kenyan' in msg or 'colombian' in msg:\n",
        "        return 'specify_coffee'\n",
        "    if 'what' in msg:\n",
        "        return 'ask_explanation'\n",
        "    return 'none'\n",
        "\n",
        "#####################################################################\n",
        "\n",
        "# Define the states\n",
        "INIT=0 \n",
        "CHOOSE_COFFEE=1\n",
        "ORDERED=2\n",
        "\n",
        "# Define the policy rules dictionary\n",
        "policy_rules = {\n",
        "    (INIT, \"ask_explanation\"): (INIT, \"I'm a bot to help you order coffee beans\"),\n",
        "    (INIT, \"order\"): (CHOOSE_COFFEE, \"ok, Colombian or Kenyan?\"),\n",
        "    (CHOOSE_COFFEE, \"specify_coffee\"): (ORDERED, \"perfect, the beans are on their way!\"),\n",
        "    (CHOOSE_COFFEE, \"ask_explanation\"): (CHOOSE_COFFEE, \"We have two kinds of coffee beans - the Kenyan ones make a slightly sweeter coffee, and cost $6. The Brazilian beans make a nutty coffee and cost $5.\")    \n",
        "}\n",
        "\n",
        "# Define send_messages()\n",
        "def send_messages(messages):\n",
        "    state = INIT\n",
        "    for msg in messages:\n",
        "        state = send_message(state, msg)\n",
        "\n",
        "# Send the messages\n",
        "send_messages([\"what can you do for me?\",\n",
        "               \"well then I'd like to order some coffee\",\n",
        "               \"what do you mean by that?\",\n",
        "               \"kenyan\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlO-H4dtXMVE",
        "outputId": "da611c63-d8c4-44c1-f947-47e1077f89ad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USER : what can you do for me?\n",
            "BOT : I'm a bot to help you order coffee beans\n",
            "USER : well then I'd like to order some coffee\n",
            "BOT : ok, Colombian or Kenyan?\n",
            "USER : what do you mean by that?\n",
            "BOT : We have two kinds of coffee beans - the Kenyan ones make a slightly sweeter coffee, and cost $6. The Brazilian beans make a nutty coffee and cost $5.\n",
            "USER : kenyan\n",
            "BOT : perfect, the beans are on their way!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Dealing with rejection***\n",
        "\n",
        "What happens if you make a suggestion to your user and they don't like it? Your bot will look really silly if it makes the same suggestion again right away.\n",
        "\n",
        "Here, you're going to modify your `respond()` function so that it accepts and returns 4 arguments:\n",
        "\n",
        "- The user message as an argument, and the bot response as the first return value.\n",
        "- A dictionary `params` including the entities the user has specified.\n",
        "- A `prev_suggestions` list. When passed to `respond()`, this should contain the suggestions made in the previous bot message. When returned by `respond()`, it should contain the current suggestions.\n",
        "- An excluded list, which contains all of the results your user has already explicitly rejected.\n",
        "\n",
        "Your function should add the previous suggestions to the excluded list whenever it receives a `\"deny\"` intent. It should also filter out excluded suggestions from the response.\n",
        "\n",
        "\n",
        "- Define a `respond()` function with 4 arguments: `message`, `params`, `prev_suggestions`, and `excluded`.\n",
        "\n",
        "- Interpret the message and store the result in `parse_data`.\n",
        "\n",
        "- The value of the `\"intent\"` key of `parse_data` is itself a dictionary of key-value pairs. Assign `parse_data[\"intent\"][\"name\"]` to `intent`, and `parse_data[\"entities\"]` to `entities`.\n",
        "\n",
        "- If the `intent` is `\"deny\"`, use the **`.extend()`** method of the excluded list to add `prev_suggestions` to it.\n",
        "\n",
        "* Initialize the empty `params` dictionary and empty suggestions and excluded lists. "
      ],
      "metadata": {
        "id": "uEpWYsSAZMNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_hotels(params, excluded):\n",
        "    query = 'SELECT * FROM hotels'\n",
        "    if len(params) > 0:\n",
        "        filters = [\"{}=?\".format(k) for k in params] + [\"name!='?'\".format(k) for k in excluded] \n",
        "        query += \" WHERE \" + \" and \".join(filters)\n",
        "    t = tuple(params.values())\n",
        "    \n",
        "    # open connection to DB\n",
        "    conn = sqlite3.connect('hotels.db')\n",
        "    # create a cursor\n",
        "    c = conn.cursor()\n",
        "    c.execute(query, t)\n",
        "    return c.fetchall()\n",
        "\n",
        "def interpret(message):\n",
        "    data = interpreter.parse(message)\n",
        "    if 'no' in message:\n",
        "        data[\"intent\"][\"name\"] = \"deny\"\n",
        "    return data\n",
        "\n",
        "responses = [\"I'm sorry :( I couldn't find anything like that\",\n",
        "             '{} is a great hotel!',\n",
        "             '{} or {} would work!',\n",
        "             '{} is one option, but I know others too :)']\n",
        "\n",
        "#######################################################################\n",
        "\n",
        "# Define respond()\n",
        "def respond(message, params, prev_suggestions, excluded):\n",
        "    # Interpret the message\n",
        "    parse_data = interpret(message)\n",
        "    # Extract the intent\n",
        "    intent = parse_data[\"intent\"][\"name\"]\n",
        "    # Extract the entities\n",
        "    entities = parse_data[\"entities\"]\n",
        "    # Add the suggestion to the excluded list if intent is \"deny\"\n",
        "    if intent == \"deny\":\n",
        "        excluded.extend(prev_suggestions)\n",
        "    # Fill the dictionary with entities\t\n",
        "    for ent in entities:\n",
        "        params[ent[\"entity\"]] = str(ent[\"value\"])\n",
        "    # Find matching hotels\n",
        "    results = [\n",
        "        r \n",
        "        for r in find_hotels(params, excluded) \n",
        "        if r[0] not in excluded\n",
        "    ]\n",
        "    # Extract the suggestions\n",
        "    names = [r[0] for r in results]\n",
        "    n = min(len(results), 3)\n",
        "    suggestions = names[:2]\n",
        "    return responses[n].format(*names), params, suggestions, excluded\n",
        "\n",
        "# Initialize the empty dictionary and lists\n",
        "params, suggestions, excluded = {}, [], []\n",
        "\n",
        "# Send the messages\n",
        "for message in [\"I want a mid range hotel\", \"no that doesn't work for me\"]:\n",
        "    print(\"USER: {}\".format(message))\n",
        "    response, params, suggestions, excluded = respond(message, params, suggestions, excluded)\n",
        "    print(\"BOT: {}\".format(response))\n"
      ],
      "metadata": {
        "id": "yxFeOF0Xcb9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "USER: I want a mid range hotel\n",
        "BOT: Hotel for Dogs is one option, but I know others too :)\n",
        "USER: no that doesn't work for me\n",
        "BOT: Grand Hotel is one option, but I know others too :)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "JyBTrawYcnVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Pending actions I***\n",
        "\n",
        "You can really improve the user experience of your bot by asking the user simple yes or no follow-up questions. One easy way to handle these follow-ups is to define pending actions which get executed as soon as the user says \"yes\", and wiped if the user says \"no\".\n",
        "\n",
        "In this exercise, you're going to define a `policy()` function which takes the `intent` as its sole argument and returns two values: The next action to take and a pending action. The policy function should return this pending action when a `\"yes\"` or `\"affirm\"` intent is returned and should wipe the pending actions if a `\"no\"` or `\"deny\"` intent is returned.\n",
        "\n",
        "Here, the `interpret(message)` function has been defined for you such that if `\"yes\"` is in the message, `\"affirm\"` is returned, and if `\"no\"` is in the message, then `\"deny\"` is returned.\n",
        "\n",
        "* Define a function called `policy()` which takes `intent` as its argument.\n",
        "* If the `intent` is `\"affirm\"`, return a `\"do_pending\"` action and `None`.\n",
        "* If the `intent` is `\"deny\"`, return a `\"Ok\"` action and `None`."
      ],
      "metadata": {
        "id": "JkXDQePjhjIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpret(message):\n",
        "    msg = message.lower()\n",
        "    if 'order' in msg:\n",
        "        return 'order'\n",
        "    elif 'yes' in msg:\n",
        "        return 'affirm'\n",
        "    elif 'no' in msg:\n",
        "        return 'deny'\n",
        "    return 'none'\n",
        "\n",
        "# Define policy()\n",
        "def policy(intent):\n",
        "    # Return \"do_pending\" if the intent is \"affirm\"\n",
        "    if intent == \"affirm\":\n",
        "        return \"do_pending\", None\n",
        "    # Return \"Ok\" if the intent is \"deny\"\n",
        "    if intent == \"deny\":\n",
        "        return \"Ok\", None\n",
        "    if intent == \"order\":\n",
        "        return \"Unfortunately, the Kenyan coffee is currently out of stock, would you like to order the Brazilian beans?\", \"Alright, I've ordered that for you!\""
      ],
      "metadata": {
        "id": "SdC_JPu5cq0N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a `policy()` function defined, you can now incorporate it into a `send_message()` function.\n",
        "\n",
        "## ***Pending actions II***\n",
        "\n",
        "Having defined your `policy()` function, it's now time to write a `send_message()` function which takes both a `pending` action and a `message` as its arguments and leverages the `policy()` function to determine the bot's response.\n",
        "\n",
        "Your `policy(intent)` function from the previous exercise has been pre-loaded.\n",
        "\n",
        "* Define a function called `send_message()` which takes in two arguments: `pending` and `message`.\n",
        "\n",
        "* Pass in the interpretation of `message` as an argument to `policy()` and unpack the result into the variables `action` and `pending_action`.\n",
        "\n",
        "* If the action is `\"do_pending\"` and `pending` is not `None`, print the `pending` response. Else, print the `action`.\n",
        "\n",
        "* Inside the definition of the `send_messages()` function, call your `send_message()` function with `pending` and `msg` as arguments."
      ],
      "metadata": {
        "id": "rsQIq9QEjAH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpret(message):\n",
        "    msg = message.lower()\n",
        "    if 'order' in msg:\n",
        "        return 'order'\n",
        "    elif 'yes' in msg:\n",
        "        return 'affirm'\n",
        "    elif 'no' in msg:\n",
        "        return 'deny'\n",
        "    return 'none'\n",
        "\n",
        "def policy(intent):\n",
        "    if intent == 'affirm':\n",
        "        return \"do_pending\", None\n",
        "    elif intent == 'deny':\n",
        "        return \"Ok\", None\n",
        "    elif intent == 'order':\n",
        "        return \"Unfortunately, the Kenyan coffee is currently out of stock, would you like to order the Brazilian beans?\", \"Alright, I've ordered that for you!\"\n",
        "\n",
        "#############################################################################\n",
        "\n",
        "# Define send_message()\n",
        "def send_message(pending, message):\n",
        "    print(\"USER : {}\".format(message))\n",
        "    action, pending_action = policy(interpret(message))\n",
        "    if action == \"do_pending\" and pending is not None:\n",
        "        print(\"BOT : {}\".format(pending))\n",
        "    else:\n",
        "        print(\"BOT : {}\".format(action))\n",
        "    return pending_action\n",
        "    \n",
        "# Define send_messages()\n",
        "def send_messages(messages):\n",
        "    pending = None\n",
        "    for msg in messages:\n",
        "        pending = send_message(pending, msg)\n",
        "\n",
        "# Send the messages\n",
        "send_messages([\"I'd like to order some coffee\",\n",
        "               \"ok yes please\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAQhh6Ofi9GE",
        "outputId": "9ddf45f6-907f-45af-ac9c-fd38f08cd2a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USER : I'd like to order some coffee\n",
            "BOT : Unfortunately, the Kenyan coffee is currently out of stock, would you like to order the Brazilian beans?\n",
            "USER : ok yes please\n",
            "BOT : Alright, I've ordered that for you!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Pending state transitions***\n",
        "\n",
        "You'll often need to briefly deviate from the flow of a conversation, for example to authenticate a user, before returning to the topic of discussion.\n",
        "\n",
        "In these cases, it's often simpler - and easier to debug - if you save some actions/states as pending rather than adding ever more complicated rules.\n",
        "\n",
        "Here, you're going to define a `policy_rules` dictionary, where the keys are tuples of the current `state` and the received `intent`, and the values are tuples of the next `state`, the bot's `response`, and a `state` for which to set a pending transition.\n",
        "\n",
        "* Complete the `policy_rules` dictionary by filling in the values:\n",
        "\n",
        "  - A user starts in the `INIT` state.\n",
        "  - If the user is in the `INIT` state and tries to place an order, you should ask for their number and create a pending transition to the `AUTHED` state.\n",
        "  - This is the only policy rule which creates a pending transition, so the others simply have a pending state value of `None`.\n",
        "\n",
        "* The `pending` state has been added as the second argument of the `send_message()` function, which now returns the new state as well as the pending state. Call this `send_message()` function inside `send_messages()`, unpacking the output into the variables `state` and `pending`."
      ],
      "metadata": {
        "id": "uuYs2UkMmNQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def interpret(message):\n",
        "    msg = message.lower()\n",
        "    if 'order' in msg:\n",
        "        return 'order'\n",
        "    if 'kenyan' in msg or 'colombian' in msg:\n",
        "        return 'specify_coffee'\n",
        "    if any([d in msg for d in string.digits]):\n",
        "        return 'number'    \n",
        "    return 'none'\n",
        "\n",
        "def send_message(state, pending, message):\n",
        "    print(\"USER : {}\".format(message))\n",
        "    new_state, response, pending_state = policy_rules[(state, interpret(message))]\n",
        "    print(\"BOT : {}\".format(response))\n",
        "    if pending is not None:\n",
        "        new_state, response, pending_state = policy_rules[pending]\n",
        "        print(\"BOT : {}\".format(response))        \n",
        "    if pending_state is not None:\n",
        "        pending = (pending_state, interpret(message))\n",
        "    return new_state, pending\n",
        "\n",
        "###########################################################################\n",
        "\n",
        "# Define the states\n",
        "INIT=0\n",
        "AUTHED=1\n",
        "CHOOSE_COFFEE=2\n",
        "ORDERED=3\n",
        "\n",
        "# Define the policy rules\n",
        "policy_rules = {\n",
        "    (INIT, \"order\"): (INIT, \"you'll have to log in first, what's your phone number?\", AUTHED),\n",
        "    (INIT, \"number\"): (AUTHED, \"perfect, welcome back!\", None),\n",
        "    (AUTHED, \"order\"): (CHOOSE_COFFEE, \"would you like Colombian or Kenyan?\", None),    \n",
        "    (CHOOSE_COFFEE, \"specify_coffee\"): (ORDERED, \"perfect, the beans are on their way!\", None)\n",
        "}\n",
        "\n",
        "# Define send_messages()\n",
        "def send_messages(messages):\n",
        "    state = INIT\n",
        "    pending = None\n",
        "    for msg in messages:\n",
        "        state, pending = send_message(state, pending, msg)\n",
        "\n",
        "# Send the messages\n",
        "send_messages([\"I'd like to order some coffee\",\n",
        "               \"555-1234\",\n",
        "               \"kenyan\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf1MRtNTl6Ls",
        "outputId": "1b0cc93b-2486-446f-a5a7-a5a6e3b8f469"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USER : I'd like to order some coffee\n",
            "BOT : you'll have to log in first, what's your phone number?\n",
            "USER : 555-1234\n",
            "BOT : perfect, welcome back!\n",
            "BOT : would you like Colombian or Kenyan?\n",
            "USER : kenyan\n",
            "BOT : perfect, the beans are on their way!\n",
            "BOT : would you like Colombian or Kenyan?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Putting it all together I***\n",
        "It's time to put everything you've learned in the course together by combining the coffee ordering bot with the ELIZA rules from chapter 1.\n",
        "\n",
        "To begin, you'll define a function called `chitchat_response()`, which calls the predefined function `match_rule()` from back in chapter 1. This returns a response if the message matched an ELIZA template, and otherwise, `None`.\n",
        "\n",
        "The ELIZA rules are contained in a dictionary called `eliza_rules`.\n",
        "\n",
        "* Define a `chitchat_response()` function which takes in a `message` argument.\n",
        "* Call the `match_rule()` function with `eliza_rules` and `message` as arguments. Unpack the output into `response` and `phrase`.\n",
        "* If the response is `\"default\"`, return `None`.\n",
        "* If `\"{0}\"` is in the response, replace the pronouns of the `phrase` using `replace_pronouns()`, and then include the `phrase` in the `response` by using **`.format()`** on `response`."
      ],
      "metadata": {
        "id": "BVGyOJzEpft7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INIT=0\n",
        "AUTHED=1\n",
        "CHOOSE_COFFEE=2\n",
        "ORDERED=3\n",
        "\n",
        "eliza_rules = {'I want (.*)': ['What would it mean if you got {0}',\n",
        "                               'Why do you want {0}',\n",
        "                               \"What's stopping you from getting {0}\"],\n",
        "                'do you remember (.*)': ['Did you think I would forget {0}',\n",
        "                                         \"Why haven't you been able to forget {0}\",\n",
        "                                         'What about {0}',\n",
        "                                         'Yes .. and?'],\n",
        "                'do you think (.*)': ['if {0}? Absolutely.', 'No chance'],\n",
        "                'if (.*)': [\"Do you really think it's likely that {0}\",\n",
        "                            'Do you wish that {0}',\n",
        "                            'What do you think about {0}',\n",
        "                            'Really--if {0}']}\n",
        "\n",
        "policy_rules = {(0, 'number'): (1, 'perfect, welcome back!', None),\n",
        "                (0, 'order'): (0,  \"you'll have to log in first, what's your phone number?\",  1),\n",
        "                (1, 'order'): (2, 'would you like Colombian or Kenyan?', None),\n",
        "                (2, 'specify_coffee'): (3, 'perfect, the beans are on their way!', None)}\n",
        "\n",
        "def interpret(message):\n",
        "    msg = message.lower()\n",
        "    if 'order' in msg:\n",
        "        return 'order'\n",
        "    if 'kenyan' in msg or 'colombian' in msg:\n",
        "        return 'specify_coffee'\n",
        "    if any([d in msg for d in string.digits]):\n",
        "        return 'number'    \n",
        "    return 'none'\n",
        "\n",
        "def match_rule(rules, message):\n",
        "    for pattern, responses in rules.items():\n",
        "        match = re.search(pattern, message)\n",
        "        if match is not None:\n",
        "            response = random.choice(responses)\n",
        "            var = match.group(1) if '{0}' in response else None\n",
        "            return response, var\n",
        "    return \"default\", None\n",
        "\n",
        "def replace_pronouns(message):\n",
        "\n",
        "    message = message.lower()\n",
        "    if 'me' in message:\n",
        "        return re.sub('me', 'you', message)\n",
        "    if 'i' in message:\n",
        "        return re.sub('i', 'you', message)\n",
        "    elif 'my' in message:\n",
        "        return re.sub('my', 'your', message)\n",
        "    elif 'your' in message:\n",
        "        return re.sub('your', 'my', message)\n",
        "    elif 'you' in message:\n",
        "        return re.sub('you', 'me', message)\n",
        "\n",
        "    return message\n",
        "\n",
        "########################################################################\n",
        "\n",
        "# Define chitchat_response()\n",
        "def chitchat_response(message):\n",
        "    # Call match_rule()\n",
        "    response, phrase = match_rule(eliza_rules, message)\n",
        "    # Return none if response is \"default\"\n",
        "    if response == \"default\":\n",
        "        return None\n",
        "    if '{0}' in response:\n",
        "        # Replace the pronouns of phrase\n",
        "        phrase = replace_pronouns(phrase)\n",
        "        # Calculate the response\n",
        "        response = response.format(phrase)\n",
        "    return response"
      ],
      "metadata": {
        "id": "cPK3oQ7opPe3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You've put it all together and your bot can now interleave chit-chat and functional conversation.\n",
        "\n",
        "## ***Putting it all together II***\n",
        "\n",
        "With your `chitchat_response(message)` function defined, the next step is to define a `send_message()` function. This function should first call `chitchat_response(message)` and only use the coffee bot policy if there is no matching message.\n",
        "\n",
        "\n",
        "* Define a `send_message()` function which takes in 3 arguments: `state`, `pending`, and `message`.\n",
        "\n",
        "* Call `chitchat_response(message)`, storing the result in `response`. If there is a response, print it and return the `state` along with `None`.\n",
        "\n",
        "* Unpack the `policy_rules` dictionary into the variables `new_state`, `response`, and `pending_state`. To do this, pass in a tuple consisting of `state` and `interpret(message)`.\n",
        "\n",
        "* If `pending` is not none, extract the new states and response by using `pending` as the key of `policy_rules`.\n",
        "\n"
      ],
      "metadata": {
        "id": "7hX-V4zJsYLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Define send_message()\n",
        "def send_message(state, pending, message):\n",
        "    print(\"USER : {}\".format(message))\n",
        "    response = chitchat_response(message)\n",
        "    if response is not None:\n",
        "        print(\"BOT : {}\".format(response))\n",
        "        return state, None\n",
        "    \n",
        "    # Calculate the new_state, response, and pending_state\n",
        "    new_state, response, pending_state =policy_rules[(state, interpret(message))]\n",
        "    print(\"BOT : {}\".format(response))\n",
        "    if pending is not None:\n",
        "        new_state, response, pending_state = policy_rules[pending]\n",
        "        print(\"BOT : {}\".format(response))        \n",
        "    if pending_state is not None:\n",
        "        pending = (pending_state, interpret(message))\n",
        "    return new_state, pending\n",
        "\n",
        "# Define send_messages()\n",
        "def send_messages(messages):\n",
        "    state = INIT\n",
        "    pending = None\n",
        "    for msg in messages:\n",
        "        state, pending = send_message(state, pending, msg)\n",
        "\n",
        "# Send the messages\n",
        "send_messages([\n",
        "    \"I'd like to order some coffee\",\n",
        "    \"555-12345\",\n",
        "    \"do you remember when I ordered 1000 kilos by accident?\",\n",
        "    \"kenyan\"\n",
        "])  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXH730fasW6m",
        "outputId": "35912d6c-a58e-4354-826d-7d9418f36b91"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USER : I'd like to order some coffee\n",
            "BOT : you'll have to log in first, what's your phone number?\n",
            "USER : 555-12345\n",
            "BOT : perfect, welcome back!\n",
            "BOT : would you like Colombian or Kenyan?\n",
            "USER : do you remember when I ordered 1000 kilos by accident?\n",
            "BOT : What about when you ordered 1000 kyoulos by accyoudent?\n",
            "USER : kenyan\n",
            "BOT : perfect, the beans are on their way!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Generating text with neural networks***\n",
        "\n",
        "In this final exercise of the course, you're going to generate text using a neural network trained on the scripts of every episode of The Simpsons. Specifically, you'll use a simplified version of the `sample_text()` function described in the video.\n",
        "\n",
        "It takes in two arguments: `seed` and `temperature`. The `seed` argument is the initial sequence that the network uses to generate the subsequent text, while the `temperature` argument controls how risky the network is when generating text. At very low temperatures, it just repeats the most common combinations of letters, and at very high temperatures, it generates complete gibberish. In order to ensure fast runtimes, the network in this exercise will only work for a subset of temperature values.\n",
        "\n",
        "\n",
        "- Set the seed to be `\"i'm gonna punch lenny in the back of the\"`.\n",
        "- For each of the riskiness values `[0.2, 0.5, 1.0, 1.2]`, call the `sample_text()` function with the arguments `seed` and `temperature`."
      ],
      "metadata": {
        "id": "oFj3ROs8-Y5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated = {0.2: \"i'm gonna punch lenny in the back of the been a to the on the man to the mother and the father to simpson the father to with the marge in the for the like the fame to the been to the for my bart the don't was in the like the for the father the father a was the father been a say the been to me the do it and the father been to go. i want to the boy i can the from a man to be the for the been a like the father to make my bart of the father\",\n",
        "             0.5: \"i'm gonna punch lenny in the back of the kin't she change and i'm all better it and the was the fad a drivera it? what i want to did hey, he would you would in your bus who know is the like and this don't are for your this all for your manset the for it a man is on the see the will they want to know i'm are for one start of that and i got the better this is. it whoce and i don't are on the mater stop in the from a for the be your mileat\",\n",
        "             1.0: \"i'm gonna punch lenny in the back of the to to macks how screath. firl done we wouldn't wil that kill. of this torshmobote since, i know i ord did, can give crika of sintenn prescoam.whover my me after may? there's right. that up. there's ruining isay.oh.solls.nan'h those off point chuncing car your anal medion.hey, are exallies a off while bea dolk of sure, hello, no in her, we'll rundems... i'm eventy taving me to too the letberngonce\",\n",
        "             1.2: \"i'm gonna punch lenny in the back of the burear prespe-nakes, 'lisa to isn't that godios.and when be the bowniday' would lochs meine, mind crikvin' suhle ovotaci!..... hey, a poielyfd othe flancer, this in are rightplouten of of we doll hurrs, truelturone? rake inswaydan justy!we scrikent.ow.. by back hous, smadge, the lighel irely.yes, homer. wel'e esasmoy ryelalrs all wronencay...... nank. i wenth makedyk. come on help cerzind, now, n\"}\n",
        "\n",
        "# Feed the seed text into the neural network\n",
        "seed = \"i'm gonna punch lenny in the back of the\"\n",
        "def sample_text(seed, temperature):\n",
        "    return generated[temperature]\n",
        "# Iterate over the different temperature values\n",
        "for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
        "    print(\"\\nGenerating text with riskiness : {}\\n\".format(temperature))\n",
        "    # Call the sample_text function\n",
        "    print(sample_text(seed, temperature))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSqZk8rwuV6f",
        "outputId": "f8c4f7de-a45a-401d-f9fc-5fd874f2ed22"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating text with riskiness : 0.2\n",
            "\n",
            "i'm gonna punch lenny in the back of the been a to the on the man to the mother and the father to simpson the father to with the marge in the for the like the fame to the been to the for my bart the don't was in the like the for the father the father a was the father been a say the been to me the do it and the father been to go. i want to the boy i can the from a man to be the for the been a like the father to make my bart of the father\n",
            "\n",
            "Generating text with riskiness : 0.5\n",
            "\n",
            "i'm gonna punch lenny in the back of the kin't she change and i'm all better it and the was the fad a drivera it? what i want to did hey, he would you would in your bus who know is the like and this don't are for your this all for your manset the for it a man is on the see the will they want to know i'm are for one start of that and i got the better this is. it whoce and i don't are on the mater stop in the from a for the be your mileat\n",
            "\n",
            "Generating text with riskiness : 1.0\n",
            "\n",
            "i'm gonna punch lenny in the back of the to to macks how screath. firl done we wouldn't wil that kill. of this torshmobote since, i know i ord did, can give crika of sintenn prescoam.whover my me after may? there's right. that up. there's ruining isay.oh.solls.nan'h those off point chuncing car your anal medion.hey, are exallies a off while bea dolk of sure, hello, no in her, we'll rundems... i'm eventy taving me to too the letberngonce\n",
            "\n",
            "Generating text with riskiness : 1.2\n",
            "\n",
            "i'm gonna punch lenny in the back of the burear prespe-nakes, 'lisa to isn't that godios.and when be the bowniday' would lochs meine, mind crikvin' suhle ovotaci!..... hey, a poielyfd othe flancer, this in are rightplouten of of we doll hurrs, truelturone? rake inswaydan justy!we scrikent.ow.. by back hous, smadge, the lighel irely.yes, homer. wel'e esasmoy ryelalrs all wronencay...... nank. i wenth makedyk. come on help cerzind, now, n\n"
          ]
        }
      ]
    }
  ]
}