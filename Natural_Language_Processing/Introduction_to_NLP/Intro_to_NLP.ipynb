{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bzWZtgZuRm5a",
        "outputId": "42e763bf-6959-4cd9-cb72-ed3d88cd3137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting polyglot\n",
            "  Downloading polyglot-16.7.4.tar.gz (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 24.7 MB/s \n",
            "\u001b[?25hCollecting PyICU\n",
            "  Downloading PyICU-2.9.tar.gz (305 kB)\n",
            "\u001b[K     |████████████████████████████████| 305 kB 46.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycld2\n",
            "  Downloading pycld2-0.41.tar.gz (41.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 41.4 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Building wheels for collected packages: polyglot, PyICU, pycld2\n",
            "  Building wheel for polyglot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for polyglot: filename=polyglot-16.7.4-py2.py3-none-any.whl size=52577 sha256=a44402901f2314f1f1a459211bfcc44a13c77cee343d9ac2f8035f1cbb6fee25\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/bc/67/75c9de8e9726460bc0b101ad225ad025cb8ce9e0759beb9d52\n",
            "  Building wheel for PyICU (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyICU: filename=PyICU-2.9-cp37-cp37m-linux_x86_64.whl size=1375434 sha256=3ab54837c02923be1296d24397dce14b7c284907e655b0f516b4c15cfd35806d\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/88/93/6c1b06361e4cbd4e7f793fb456729f69798f9aa3fc2a791cd7\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp37-cp37m-linux_x86_64.whl size=9834229 sha256=ed91b7bdfa668ec1acdf26245aff92a12729792f0e74d927b9b024b25fc6037c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/e4/58/ed2e9f43c07d617cc81fe7aff0fc6e42b16c9cf6afe960b614\n",
            "Successfully built polyglot PyICU pycld2\n",
            "Installing collected packages: PyICU, pycld2, polyglot, morfessor\n",
            "Successfully installed PyICU-2.9 morfessor-2.0.6 polyglot-16.7.4 pycld2-0.41\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 9.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (7.1.2)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.37.2-py3-none-any.whl (959 kB)\n",
            "\u001b[K     |████████████████████████████████| 959 kB 53.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
            "Installing collected packages: fonttools, matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "Successfully installed fonttools-4.37.2 matplotlib-3.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install polyglot PyICU pycld2 morfessor\n",
        "!pip install matplotlib --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt, numpy as np, seaborn as sns, pandas as pd, re, nltk, itertools, spacy\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tree import Tree\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models.tfidfmodel import TfidfModel\n",
        "from polyglot.text import Text, Word\n",
        "from polyglot.downloader import downloader\n",
        "downloader.download(\"embeddings2.es\")\n",
        "downloader.download(\"ner2.es\")\n",
        "downloader.download(\"embeddings2.fr\")\n",
        "downloader.download(\"ner2.fr\")\n",
        "from collections import Counter, defaultdict\n",
        "from string import punctuation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a25Y_OgMRtEJ",
        "outputId": "a733d35a-b0a8-4707-dbaa-6ccb84d29b55"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[polyglot_data] Downloading package embeddings2.es to\n",
            "[polyglot_data]     /root/polyglot_data...\n",
            "[polyglot_data]   Package embeddings2.es is already up-to-date!\n",
            "[polyglot_data] Downloading package ner2.es to /root/polyglot_data...\n",
            "[polyglot_data]   Package ner2.es is already up-to-date!\n",
            "[polyglot_data] Downloading package embeddings2.fr to\n",
            "[polyglot_data]     /root/polyglot_data...\n",
            "[polyglot_data]   Package embeddings2.fr is already up-to-date!\n",
            "[polyglot_data] Downloading package ner2.fr to /root/polyglot_data...\n",
            "[polyglot_data]   Package ner2.fr is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Review of Regular Expressions**"
      ],
      "metadata": {
        "id": "e7j8119tUBeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Are they bots?***\n",
        "\n",
        "Some tweets contain user mentions. Some of these mentions follow a very strange pattern. A few examples that you notice: `@robot3!`, `@robot5&` and `@robot7#`\n",
        "\n",
        "To these, you will do a proof of concept with one tweet and extract them using the **`re.findall()`** method.\n",
        "\n",
        " \n",
        "* `\\d`: digit\n",
        "* `\\w`: word character\n",
        "* `\\W`: non-word character\n",
        "* `\\s`: whitespace\n",
        "\n",
        "The text of one tweet was saved in the variable `sentiment_analysis`. \n",
        "\n",
        "* Write a regex that matches the user mentions that starts with `@` and follows the pattern, e.g. `@robot3!`.\n",
        "\n",
        "* Find all the matches of the pattern in the `sentiment_analysis` variable."
      ],
      "metadata": {
        "id": "EhafGp0MpiLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analysis = '@robot9! @robot4& I have a good feeling that the show isgoing to be amazing! @robot9$ @robot7%'\n",
        "\n",
        "print(re.findall(r\"@robot\\d\\W\", sentiment_analysis))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjY9Gi_eUEos",
        "outputId": "a6a462c3-fcc4-4dc2-9a29-941b6691ad2f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@robot9!', '@robot4&', '@robot9$', '@robot7%']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Find the numbers ( always specify whitespaces as `\\s`)***\n",
        "\n",
        "Some tweets contain the number of retweets, user mentions, and likes. You decide to extract this important information that is given as in this example:\n",
        "\n",
        "```\n",
        "Agh,snow! User_mentions:9, likes: 5, number of retweets: 4\n",
        "```\n",
        "\n",
        "You pull a list of metacharacters:`\\d` digit,`\\w` word character,`\\s` whitespace.\n",
        "\n",
        "Always indicate whitespace with metacharacters.\n",
        "\n",
        "The variable `sentiment_analysis` containing the text of one tweet.\n",
        "\n",
        "\n",
        "* Write a regex that matches the number of user mentions given as, for example, `User_mentions:9` in `sentiment_analysis`.\n",
        "\n",
        "* Write a regex that matches the number of likes given as, for example, `likes: 5` in `sentiment_analysis`.\n",
        "\n",
        "* Write a regex that matches the number of retweets given as, for example, `number of retweets: 4` in `sentiment_analysis`."
      ],
      "metadata": {
        "id": "3EZA3KXZqzIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analysis = \"Unfortunately one of those moments wasn't a giant squid monster. User_mentions:2, likes: 9, number of retweets: 7\"\n",
        "\n",
        "# Write a regex to obtain user mentions\n",
        "print(re.findall(r\"User\\Smentions\\S\\d\", sentiment_analysis))\n",
        "\n",
        "# Write a regex to obtain number of likes\n",
        "print(re.findall(r\"likes\\S\\s\\d\", sentiment_analysis))\n",
        "\n",
        "# Write a regex to obtain number of retweets\n",
        "print(re.findall(r\"number\\sof\\sretweets\\S\\s\\d\", sentiment_analysis))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BAbCl-7qvg5",
        "outputId": "2046bc91-b612-4c6c-c8f5-85dc2e27541a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['User_mentions:2']\n",
            "['likes: 9']\n",
            "['number of retweets: 7']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Match and split***\n",
        "\n",
        "Some of the tweets, instead of having spaces to separate words, have strange characters.  You notice that \n",
        "\n",
        "* the *sentences* are always separated by a special character, followed by a number, the word `break`, and after that, another special character, e.g `&4break!`,\n",
        "* the *words* are always separated by a special character, the word new, and a normal random character, e.g `#newH`.\n",
        "\n",
        "The variable `sentiment_analysis` containing the text of one tweet\n",
        "\n",
        "* Write a regex that matches the pattern separating the sentences in `sentiment_analysis`, e.g. `&4break!`.\n",
        "* Replace `regex_sentence` with a space `\" \"` in the variable `sentiment_analysis`. Assign it to `sentiment_sub`.\n",
        "* Write a regex that matches the pattern separating the words in `sentiment_analysis`, e.g. `#newH`.\n",
        "* Replace `regex_words` with a space in the variable `sentiment_sub`. Assign it to `sentiment_final` and print out the result."
      ],
      "metadata": {
        "id": "YlSPSp4HuHv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analysis = 'He#newHis%newTin love with$newPscrappy. #8break%He is&newYmissing him@newLalready'\n",
        "print(sentiment_analysis, '\\n')\n",
        "\n",
        "# Replace the regex_sentence with a space\n",
        "sentiment_sub = re.sub(r\"\\W\\dbreak\\W\", \" \", sentiment_analysis)\n",
        "\n",
        "# Replace the regex_words and print the result\n",
        "sentiment_final = re.sub(r\"\\Wnew\\w\", \" \", sentiment_sub)\n",
        "\n",
        "print(sentiment_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzLfGqhsthJM",
        "outputId": "22c2fbb2-144d-4da5-8906-d05807cf6eff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He#newHis%newTin love with$newPscrappy. #8break%He is&newYmissing him@newLalready \n",
            "\n",
            "He is in love with scrappy.  He is missing him already\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Repetitions**\n",
        "\n",
        "### ***Everything clean***\n",
        "\n",
        "There are several types of strings that increase your sentiment analysis complexity. But these strings do not provide any useful sentiment. Among them, we can have links and user mentions.\n",
        "\n",
        "In order to clean the tweets, you want to extract some examples first. You know that \n",
        "\n",
        "* most of the times links start with `http` and do not contain any whitespace, e.g. `https://www.datacamp.com`. \n",
        "* User mentions start with `@` and can have letters and numbers only, e.g. `@johnsmith3`.\n",
        "\n",
        "You write down some helpful quantifiers to help you: `*` zero or more times, `+` once or more, `?` zero or once.\n",
        "\n",
        "The list `sentiment_analysis` containing the text of three tweets are already loaded in your session. \n",
        "\n",
        "* Write a regex to find all the matches of `http` links appearing in each `tweet` in `sentiment_analysis`. Print out the result.\n",
        "\n",
        "* Write a regex to find all the matches of user mentions appearing in each `tweet` in `sentiment_analysis`. Print out the result.\n",
        "\n"
      ],
      "metadata": {
        "id": "qFLSmxjs1js3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = ['Boredd. Colddd @blueKnight39 Internet keeps stuffing up. Save me! https://www.tellyourstory.com', \n",
        "        \"I had a horrible nightmare last night @anitaLopez98 @MyredHat31 which affected my sleep, now I'm really tired\",\n",
        "        'im lonely  keep me company @YourBestCompany! @foxRadio https://radio.foxnews.com 22 female, new york']\n",
        "\n",
        "sentiment_analysis = pd.Series(data=data, index=[545, 546, 547])\n",
        "\n",
        "for tweet in sentiment_analysis:\n",
        "\t# Write regex to match http links and print out result\n",
        "\tprint(re.findall(r\"http.+\\.com\", tweet))\n",
        "\n",
        "\t# Write regex to match user mentions and print out result\n",
        "\tprint(re.findall(r\"@\\w+\\d?\", tweet))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEOX9Lclvr9H",
        "outputId": "899dc56d-4729-4c1a-cc63-97a4a9fdea61"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://www.tellyourstory.com']\n",
            "['@blueKnight39']\n",
            "[]\n",
            "['@anitaLopez98', '@MyredHat31']\n",
            "['https://radio.foxnews.com']\n",
            "['@YourBestCompany', '@foxRadio']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`\\S` is very useful to use when you know a pattern doesn't contain spaces and you have reached the end when you do find one.\n",
        "\n",
        "### ***Some time ago***\n",
        "\n",
        "You are interested in knowing when the tweets were posted. After reading a little bit more, you learn that dates are provided in different ways. You decide to extract the dates using **`.findall()`** so you can normalize them afterwards to make them all look the same.\n",
        "\n",
        "You realize that the dates are always presented in one of the following ways:\n",
        "\n",
        "`27 minutes ago`\n",
        "\n",
        "`4 hours ago`\n",
        "\n",
        "`23rd june 2018`\n",
        "\n",
        "`1st september 2019 17:25`\n",
        "\n",
        "The list `sentiment_analysis` containing the text of three tweets, as well as the `re` module are already loaded in your session. \n",
        "\n",
        "* Complete the for-loop with a regex that finds all dates in a format similar to `27 minutes ago` or `4 hours ago`.\n",
        "\n",
        "* Complete the for-loop with a regex that finds all dates in a format similar to `23rd june 2018`.\n",
        "\n",
        "* Complete the for-loop with a regex that finds all dates in a format similar to `1st september 2019 17:25`."
      ],
      "metadata": {
        "id": "YgLqUXErGz6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analysis = pd.Series(data=['I would like to apologize for the repeated Video Games Live related tweets. 32 minutes ago',\n",
        "                                     '@zaydia but i cant figure out how to get there / back / pay for a hotel 1st May 2019',\n",
        "                                     'FML: So much for seniority, bc of technological ineptness 23rd June 2018 17:54'], index=[232, 233, 234])\n",
        "\n",
        "# Complete the for loop with a regex to find dates\n",
        "for date in sentiment_analysis:\n",
        "  \tprint(re.findall(r\"\\d{1,2}\\s\\w+\\sago\", date))\n",
        "\n",
        "\n",
        "# Complete the for loop with a regex to find dates\n",
        "for date in sentiment_analysis:\n",
        "\t  print(re.findall(r\"\\d{1,2}\\w*\\s\\w+\\s\\d{4}\", date))\n",
        "   \n",
        "# Complete the for loop with a regex to find dates\n",
        "for date in sentiment_analysis:\n",
        "\tprint(re.findall(r\"\\d{1,2}\\w*\\s\\w+\\s\\d{4}\\s\\d{1,2}:\\d{2}\", date))\n",
        " \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M_Ag69IFmfE",
        "outputId": "b5a2a7ce-3f2f-4c27-f430-2ab7e0168617"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['32 minutes ago']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "['1st May 2019']\n",
            "['23rd June 2018']\n",
            "[]\n",
            "[]\n",
            "['23rd June 2018 17:54']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Getting tokens***\n",
        "\n",
        "Your next step is to ***tokenize*** the text of your tweets. *Tokenization is the process of breaking a string into lexical units or, in simpler terms, words*. But first, you need to remove hashtags so they do not cloud your process. You realize that hashtags start with a `#` symbol and contain letters and numbers but never whitespace. After that, you plan to split the text at whitespace matches to get the tokens.\n",
        "\n",
        "You bring your list of quantifiers to help you: `*` zero or more times, `+` once or more, `?` zero or once, `{n, m}` minimum `n`, maximum `m`.\n",
        "\n",
        "The variable `sentiment_analysis` containing the text of one tweet as well as the `re` module are already loaded in your session.\n",
        "\n",
        "\n",
        "* Replace all the matches of the regex with an empty string `\"\"`. Assign it to `no_hashtag` variable.\n",
        "\n",
        "* Split the text in the `no_hashtag` variable at every match of one or more consecutive whitespace.\n"
      ],
      "metadata": {
        "id": "211fhdJfJduq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analysis = 'ITS NOT ENOUGH TO SAY THAT IMISS U #MissYou #SoMuch #Friendship #Forever'\n",
        "\n",
        "# Write a regex matching the hashtag pattern\n",
        "regex = r\"#\\w+\"\n",
        "\n",
        "# Replace the regex by an empty string\n",
        "no_hashtag = re.sub(r\"#\\w+\", \"\", sentiment_analysis)\n",
        "\n",
        "# Get tokens by splitting text\n",
        "print(re.findall(r\"\\w+\", no_hashtag))\n",
        "print(re.split(r\"\\s+\", no_hashtag))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGHwGOchIAfF",
        "outputId": "02a488ab-22c0-4bcd-cf24-30ab0b8f625a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ITS', 'NOT', 'ENOUGH', 'TO', 'SAY', 'THAT', 'IMISS', 'U']\n",
            "['ITS', 'NOT', 'ENOUGH', 'TO', 'SAY', 'THAT', 'IMISS', 'U', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **`re.search()` and `re.match()`**\n",
        "\n",
        "* re.search() หาทางหมด\n",
        "* re.match() หาเฉพาะตอนต้นประโยค\n",
        "\n",
        "## **Special characters**\n",
        "\n",
        "* `.` matches *ANY* character except newline.\n",
        "* `^` หาเฉพาะต้นประโยค\n",
        "* `$` หาเฉพาะท้ายประโยค\n",
        "* Escape special characters ใช้ `\\`\n",
        "\n",
        "## **OR operator**\n",
        "\n",
        "* `|`\n",
        "\n",
        "## **Set of characters**\n",
        "\n",
        "* `[]` ทุกตัวในวงเล็บ\n",
        "* `[^]` ทุกตัวที่ไม่อยู่ในวงเล็บ\n",
        "\n",
        "### ***Finding files***\n",
        "\n",
        "You are not satisfied with your tweets dataset cleaning. There are still extra strings that do not provide any sentiment. Among them are strings that refer to text file names.\n",
        "\n",
        "You also find a way to detect them:\n",
        "\n",
        "* They appear at the start of the string.\n",
        "* They always start with a sequence of 2 or 3 upper or lowercase vowels (a e i o u).\n",
        "* They always finish with the `txt` ending.\n",
        "\n",
        "You are not sure if you should remove them directly. So you write a script to find and store them in a separate dataset.\n",
        "\n",
        "You write down some metacharacters to help you: `^` anchor to beginning, `.` any character.\n",
        "\n",
        "The variable `sentiment_analysis` containing the text of two tweets as well as the `re` module are already loaded in your session. \n",
        "\n",
        "* Write a regex that matches the pattern of the text file names, e.g. `aemyfile.txt`.\n",
        "\n",
        "* Find all matches of the regex in the elements of `sentiment_analysis`. Print out the result.\n",
        "\n",
        "* Replace all matches of the regex with an empty string `\"\"`. Print out the result."
      ],
      "metadata": {
        "id": "1Iak9bZWD6g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analysis = pd.Series(data=['AIshadowhunters.txt aaaaand back to my literature review. At least i have a friendly cup of coffee to keep me company',\n",
        "                                     \"ouMYTAXES.txt I am worried that I won't get my $900 even though I paid tax last year\"], index=[780, 781])\n",
        "\n",
        "for text in sentiment_analysis:\n",
        "    # Find all matches of the regex\n",
        "    print(re.findall(r\"^[aeiouAEIOU]{2,3}.+txt\", text))\n",
        "      \n",
        "    # Replace all matches with empty string\n",
        "    print(re.sub(r\"^[aeiouAEIOU]{2,3}.+txt\", \"\", text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9F0Cog6K7ZJ",
        "outputId": "bf3381bd-88f9-4116-fc4f-7025d7e32ee6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AIshadowhunters.txt']\n",
            " aaaaand back to my literature review. At least i have a friendly cup of coffee to keep me company\n",
            "['ouMYTAXES.txt']\n",
            " I am worried that I won't get my $900 even though I paid tax last year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Give me your email***\n",
        "\n",
        "The company puts some rules to verify that the given email address is valid:\n",
        "\n",
        "* The first part can contain: \n",
        "   * Upper `A-Z` or lowercase letters `a-z`\n",
        "   * Numbers\n",
        "   * Characters: `!`, `#`, `%`, `&`, `*`, `$`, `.`\n",
        "* Must have `@`\n",
        "* Domain:\n",
        "   * Can contain any word characters\n",
        "   * But only `.com` ending is allowed\n",
        "\n",
        "The list `emails` as well as the `re` module are loaded in your session. \n",
        "\n",
        "* Write a regular expression to match valid email addresses as described.\n",
        "* Match the regex to the elements contained in `emails`.\n",
        "* To print out the message indicating if it is a valid email or not, complete `.format()` statement."
      ],
      "metadata": {
        "id": "eteJNYKaHiYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emails = ['n.john.smith@gmail.com', '87victory@hotmail.com', '!#mary-=@msca.net']\n",
        "\n",
        "for example in emails:\n",
        "  \t# Match the regex to the string\n",
        "    if re.match(r\"[A-Za-z0-9!#%&*$.]+@\\w+\\.com\", example):\n",
        "        # Complete the format method to print out the result\n",
        "      \tprint(\"The email {email_example} is a valid email\".format(email_example=example))\n",
        "    else:\n",
        "      \tprint(\"The email {email_example} is invalid\".format(email_example=example))   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LUfo2ioHalE",
        "outputId": "3b97cb93-7a3e-46cb-efba-be4eb688af95"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The email n.john.smith@gmail.com is a valid email\n",
            "The email 87victory@hotmail.com is a valid email\n",
            "The email !#mary-=@msca.net is invalid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Square brackets are very useful for optional characters. \n",
        "\n",
        "### ***Invalid password***\n",
        "\n",
        "* It can contain lowercase `a-z` and uppercase letters `A-Z`\n",
        "* It can contain numbers\n",
        "* It can contain the symbols: `*#$%!&.`\n",
        "* It must be at least `8` characters long but not more than `20`\n",
        "\n",
        "The list `passwords` and the module `re` are loaded in your session. \n",
        "\n",
        "* Write a regular expression to check if the passwords are valid according to the description.\n",
        "* Search the elements in the `passwords` list to find out if they are valid passwords.\n",
        "* To print out the message indicating if it is a valid password or not, complete `.format()` statement."
      ],
      "metadata": {
        "id": "R2But5oiJSV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "passwords = ['Apple34!rose', 'My87hou#4$', 'abc123']\n",
        "\n",
        "for example in passwords:\n",
        "  \t# Scan the strings to find a match\n",
        "    if re.match(r\"[a-zA-Z0-9*#$%!&.]{8,20}\", example):\n",
        "        # Complete the format method to print out the result\n",
        "      \tprint(\"The password {pass_example} is a valid password\".format(pass_example=example))\n",
        "    else:\n",
        "      \tprint(\"The password {pass_example} is invalid\".format(pass_example=example))   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBogF9GkJK7a",
        "outputId": "d7590fda-a906-41e3-9ea7-7a824b897ada"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The password Apple34!rose is a valid password\n",
            "The password My87hou#4$ is a valid password\n",
            "The password abc123 is invalid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how metacharacters help you perform complex pattern matching in a few lines of code. For that reason, Regex has many applications in Data Science, particularly in Text Mining and <ins>*Natural Language Processing*</ins>.\n",
        "\n",
        "## **Greedy vs Non-greedy matchings**\n",
        "\n",
        "* Greedy คือ เขียนตามปกติ\n",
        "* Non-greedy คือ เขียนต่อท้าย (ต้องตามหลังพวก `+` หรือ `*`) ด้วย `?`\n",
        "\n",
        "### ***Understanding the difference***\n",
        "\n",
        "Remove HTML tag but keep the inside content as they are useful for analysis.\n",
        "\n",
        "Let's take a look at this sentence containing an HTML tag:\n",
        "\n",
        "`I want to see that <strong>amazing show</strong> again!.`\n",
        "\n",
        "You know that to get the HTML tag you need to match anything that sits inside angle brackets `<` `>`. But the biggest problem is that the closing tag has the same structure. If you match too much, you will end up removing key information. So you need to decide whether to use a greedy or a lazy quantifier.\n",
        "\n",
        "The string is already loaded as `string` to your session.\n",
        "\n",
        "\n",
        "* Write a regex expression to replace HTML tags with an empty string.\n",
        "* Print out the result.\n"
      ],
      "metadata": {
        "id": "2L8VHpzfKoL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string = 'I want to see that <strong>amazing show</strong> again!'\n",
        "\n",
        "string_notags = re.sub(r\"<\\w+>|</\\w+>\", \"\", string)\n",
        "print(string_notags)\n",
        "\n",
        "string_notags = re.sub(r\"<.+?>\", \"\", string)\n",
        "print(string_notags)\n",
        "\n",
        "string_tags x= re.sub(r\"<.+>\", \"\", string)\n",
        "print(string_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o30GEK6rKins",
        "outputId": "a4851f9a-43a2-4235-c435-cf6247fb90f0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want to see that amazing show again!\n",
            "I want to see that amazing show again!\n",
            "I want to see that  again!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that a greedy quantifier will try to match as much as possible while a non-greedy quantifier will do it as few times as needed, expanding one character at a time and giving us the match we are looking for.\n",
        "\n",
        "### ***Greedy matching***\n",
        "\n",
        "Next, you see that numbers still appear in the text of the tweets. So, you decide to find all of them.\n",
        "\n",
        "Let's imagine that you want to extract the number contained in the sentence `I was born on April 24th`. A lazy quantifier will make the regex return `2` and `4`, because they will match as few characters as needed. However, a greedy quantifier will return the entire `24` due to its need to match as much as possible.\n",
        "\n",
        "The `re` module as well as the variable `sentiment_analysis` are already loaded in your session. \n",
        "\n",
        "* Use a lazy quantifier to match all numbers that appear in the variable `sentiment_analysis`.\n",
        "\n",
        "* Now, use a greedy quantifier to match all numbers that appear in the variable `sentiment_analysis`."
      ],
      "metadata": {
        "id": "XxhTmp8zOVg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analysis = 'Was intending to finish editing my 536-page novel manuscript tonight, but that will probably not happen. And only 12 pages are left '\n",
        "\n",
        "# Write a lazy regex expression \n",
        "numbers_found_lazy = re.findall(r\"\\d+?\", sentiment_analysis)\n",
        "\n",
        "# Print out the result\n",
        "print(numbers_found_lazy)\n",
        "\n",
        "# Write a greedy regex expression \n",
        "numbers_found_greedy = re.findall(r\"\\d+\", sentiment_analysis)\n",
        "\n",
        "# Print out the result\n",
        "print(numbers_found_greedy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXSDIkv7Ncik",
        "outputId": "4ad279df-5f7e-4374-919e-9d3680b391ad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['5', '3', '6', '1', '2']\n",
            "['536', '12']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though greedy quantifiers lead to longer matches, they are sometimes the best option. Because lazy quantifiers match as few as possible, they return a shorter match than we expected.\n",
        "\n",
        "### ***Lazy approach***\n",
        "\n",
        "You have done some cleaning in your dataset but you are worried that there are sentences encased in parentheses that may cloud your analysis.\n",
        "\n",
        "Again, a greedy or a lazy quantifier may lead to different results.\n",
        "\n",
        "For example, if you want to extract a word starting with `a` and ending with `e` in the string `I like apple pie`, you may think that applying the greedy regex `r\"a.+e\"` will return `apple`. However, your match will be `apple pie`. A way to overcome this is to make it lazy by using `?` which will return `apple`.\n",
        "\n",
        "The `re` module and the variable `sentiment_analysis` are already loaded in your session.\n",
        "\n",
        "* Use a greedy quantifier to match text that appears within parentheses in the variable `sentiment_analysis`.\n",
        "\n",
        "* Now, use a lazy quantifier to match text that appears within parentheses in the variable `sentiment_analysis`."
      ],
      "metadata": {
        "id": "WWtHH_hmPgS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analysis = \"Put vacation photos online (They were so cute) a few yrs ago. PC crashed, and now I forget the name of the site (I'm crying). \"\n",
        "\n",
        "# Write a greedy regex expression to match \n",
        "sentences_found_greedy = re.findall(r\"\\(.+\\)\", sentiment_analysis)\n",
        "\n",
        "# Print out the result\n",
        "print(sentences_found_greedy)\n",
        "\n",
        "# Write a lazy regex expression\n",
        "sentences_found_lazy = re.findall(r\"\\(.+?\\)\", sentiment_analysis)\n",
        "\n",
        "# Print out the results\n",
        "print(sentences_found_lazy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qyL8YZpPbkv",
        "outputId": "ec7789db-1141-4e11-d2f3-8124788c27cb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"(They were so cute) a few yrs ago. PC crashed, and now I forget the name of the site (I'm crying)\"]\n",
            "['(They were so cute)', \"(I'm crying)\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that using greedy quantifiers always leads to longer matches that sometimes are not desired. Making quantifiers lazy by adding `?` to match a shorter pattern is a very important consideration to keep in mind when handling data for text mining. \n",
        "\n",
        "## **Capturing groups**\n",
        "\n",
        "Capture a repeated group, e.g., `(\\d+)`"
      ],
      "metadata": {
        "id": "Q5uJm6fuQ1Xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(re.findall(r\"(\\d+)\", 'My lucky numbers are 8755 and 33'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvF2LOScQvnm",
        "outputId": "24ff942f-61dd-43e2-c2ad-4fa5d4776d04"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['8755', '33']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat a capturing group, e.g., `(\\d)+`"
      ],
      "metadata": {
        "id": "MEpaVHxYVNB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(re.findall(r\"(\\d)+\", 'My lucky numbers are 8755 and 33'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxOYCZjWVEk2",
        "outputId": "7dd721fe-5791-484c-c0af-544b82f90555"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['5', '3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Try another name***\n",
        "\n",
        "There are email addresses inserted in some tweets. Now, you are curious to find out which is the most common name.\n",
        "\n",
        "You want to extract the first part of the email. E.g. if you have the email `marysmith90@gmail.com`, you are only interested in `marysmith90`.\n",
        "You need to match the entire expression. So you make sure to extract only names present in emails. Also, you are only interested in names containing upper (e.g. A,B, Z) or lowercase letters (e.g. a, d, z) and numbers.\n",
        "\n",
        "The list `sentiment_analysis` containing the text of three tweets as well as the `re` module were loaded in your session. \n",
        "\n",
        "* Complete the regex to match the email capturing only the name part. The name part appears before the `@`.\n",
        "\n",
        "* Find all matches of the regex in each element of `sentiment_analysis` analysis. Assign it to the variable `email_matched`.\n",
        "\n",
        "* Complete the `.format()` method to print the results captured in each element of `sentiment_analysis`."
      ],
      "metadata": {
        "id": "ccKDCSC9Vuk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analysis = ['Just got ur newsletter, those fares really are unbelievable. Write to statravelAU@gmail.com or statravelpo@hotmail.com. They have amazing prices', 'I should have paid more attention when we covered photoshop in my webpage design class in undergrad. Contact me Hollywoodheat34@msn.net.', 'hey missed ya at the meeting. Read your email! msdrama098@hotmail.com']\n",
        "\n",
        "for tweet in sentiment_analysis:\n",
        "    # Find all matches of regex in each tweet\n",
        "    email_matched = re.findall(r\"([A-Za-z0-9]+)@\", tweet)\n",
        "\n",
        "    # Complete the format method to print the results\n",
        "    print(\"Lists of users found in this tweet: {}\".format(email_matched))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJcodImaVZSB",
        "outputId": "b034d809-3e61-4e2b-f5ba-bc117117c6c7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lists of users found in this tweet: ['statravelAU', 'statravelpo']\n",
            "Lists of users found in this tweet: ['Hollywoodheat34']\n",
            "Lists of users found in this tweet: ['msdrama098']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Flying home***\n",
        "\n",
        "You are given a dataset with only the email subjects for each of the people traveling. You learn that the text followed a pattern. Here is an example:\n",
        "\n",
        "`Here you have your boarding pass LA4214 AER-CDB 06NOV.`\n",
        "\n",
        "You need to extract the information about the flight:\n",
        "\n",
        "* The two letters indicate the airline (e.g `LA`),\n",
        "* The 4 numbers are the flight number (e.g. `4214`).\n",
        "* The three letters correspond to the departure (e.g `AER`),\n",
        "* The destination (`CDB`),\n",
        "* The date (`06NOV`) of the flight.\n",
        "\n",
        "All letters are always uppercase.\n",
        "\n",
        "The variable `flight` containing one email subject was loaded in your session. \n",
        "\n",
        "* Find all the matches corresponding to each piece of information about the flight. Assign it to `flight_matches`."
      ],
      "metadata": {
        "id": "2EJvMDHyXjOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flight = 'Subject: You are now ready to fly. Here you have your boarding pass IB3723 AMS-MAD 06OCT'\n",
        "\n",
        "# Find all matches of the flight information\n",
        "flight_matches = re.findall(r\"([A-Z]{2})(\\d{4})\\s([A-Z]{3})-([A-Z]{3})\\s(\\d{2}[A-Z]{3})\", flight)\n",
        "    \n",
        "#Print the matches\n",
        "print(\"Airline: {} Flight number: {}\".format(flight_matches[0][0], flight_matches[0][1]))\n",
        "print(\"Departure: {} Destination: {}\".format(flight_matches[0][2], flight_matches[0][3]))\n",
        "print(\"Date: {}\".format(flight_matches[0][4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHxSrlHaWeWE",
        "outputId": "93218aae-1e9e-40d4-d375-7bd8c6be5dd3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Airline: IB Flight number: 3723\n",
            "Departure: AMS Destination: MAD\n",
            "Date: 06OCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`findall()`** returns a list of tuples. The n*th* element of each tuple is the element corresponding to group `n`. This provides us with an easy way to access and organize our data.\n",
        "\n",
        "## **Alternation**\n",
        "\n",
        "Use groups to choose between optional patterns"
      ],
      "metadata": {
        "id": "K22hq4TcaDn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(re.findall(r\"\\d+\\s(cat|dog|bird)\", \"I want to have a pet. But I don't know if I want 2 cats, 1 dog or a bird.\"))\n",
        "print(re.findall(r\"(\\d+)\\s(cat|dog|bird)\", \"I want to have a pet. But I don't know if I want 2 cats, 1 dog or a bird.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4v3cY5JZ_wt",
        "outputId": "026ac7dd-d0cf-4b6e-d4d9-561d3a1f2e56"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat', 'dog']\n",
            "[('2', 'cat'), ('1', 'dog')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Non-capturing groups**\n",
        "\n",
        "Match but not capture a group\n",
        "\n",
        "* `(?:regex)`"
      ],
      "metadata": {
        "id": "zswLppY9bwMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(re.findall(r\"(?:\\d+)\\s(cat|dog|bird)\", \"I want to have a pet. But I don't know if I want 2 cats, 1 dog or a bird.\"))\n",
        "print(re.findall(r\"(\\d+)\\s(?:cat|dog|bird)\", \"I want to have a pet. But I don't know if I want 2 cats, 1 dog or a bird.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnFGo3VlbXLa",
        "outputId": "e3fc27f3-8c4e-487d-d382-c92ebec74c0e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat', 'dog']\n",
            "['2', '1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Love it!***\n",
        "\n",
        "You want to identify positive tweets about movies and concerts.\n",
        "\n",
        "You plan to find all the sentences that contain the words `love`, `like`, or `enjoy` and capture that word. You will limit the tweets by focusing on those that contain the words `movie` or `concert` by keeping the word in another group. You will also save the movie or concert name.\n",
        "\n",
        "For example, if you have the sentence: `I love the movie Avengers.` You match and capture `love`. You need to match and capture `movie`. Afterwards, you match and capture anything until the dot.\n",
        "\n",
        "The list `sentiment_analysis` containing the text of three tweets and the `re` module are loaded in your session. \n",
        "\n",
        "* Complete the regular expression to capture the words `love` or `like` or `enjoy`. Match and capture the words `movie` or `concert`. Match and capture anything appearing until the `.`.\n",
        "\n",
        "* Find all matches of the regex in each element of `sentiment_analysis`. Assign them to `positive_matches`.\n",
        "\n",
        "* Complete the `.format()` method to print out the results contained in `positive_matches` for each element in `sentiment_analysis`."
      ],
      "metadata": {
        "id": "MHUMsNCdc6Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analysis = ['I totally love the concert The Book of Souls World Tour. It kinda amazing!',\n",
        "                      'I enjoy the movie Wreck-It Ralph. I watched with my boyfriend.',\n",
        "                      \"I still like the movie Wish Upon a Star. Too bad Disney doesn't show it anymore.\"]\n",
        "\n",
        "for tweet in sentiment_analysis:\n",
        "\t# Find all matches of regex in tweet\n",
        "    positive_matches = re.findall(r\"(love|like|enjoy).+?(movie|concert)\\s(.+?)\\.\", tweet)\n",
        "    \n",
        "    # Complete format to print out the results\n",
        "    print(\"Positive comments found {}\".format(positive_matches))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuVIbeAvcCP4",
        "outputId": "5b58b9dc-42ef-4a96-b6e0-f3e5db17f126"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive comments found [('love', 'concert', 'The Book of Souls World Tour')]\n",
            "Positive comments found [('enjoy', 'movie', 'Wreck-It Ralph')]\n",
            "Positive comments found [('like', 'movie', 'Wish Upon a Star')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Ugh! Not for me!***\n",
        "\n",
        "After finding positive tweets, you want to do it for negative tweets. Your plan now is to find sentences that contain the words `hate`, `dislike` or `disapprove`. You will again save the `movie` or `concert` name. You will get the tweet containing the words `movie` or `concert` but this time, you don't plan to save the word.\n",
        "\n",
        "For example, if you have the sentence: `I dislike the movie Avengers a lot.`. You match and capture `dislike`. You will match but not capture the word `movie`. Afterwards, you match and capture anything until the dot.\n",
        "\n",
        "The list `sentiment_analysis` containing the text of three tweets as well as the `re` module are loaded in your session. \n",
        "\n",
        "* Complete the regular expression to capture the words `hate` or `dislike` or `disapprove`. Match but don't capture the words `movie` or `concert`. Match and capture anything appearing until the `.`.\n",
        "\n",
        "* Find all matches of the regex in each element of `sentiment_analysis`. Assign them to `negative_matches`.\n",
        "\n",
        "* Complete the `.format()` method to print out the results contained in `negative_matches` for each element in `sentiment_analysis`."
      ],
      "metadata": {
        "id": "vPdp08DPeg8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analysis = ['That was horrible! I really dislike the movie The cabin and the ant. So boring.',\n",
        "                      \"I disapprove the movie Honest with you. It's full of cliches.\",\n",
        "                      'I dislike very much the concert After twelve Tour. The sound was horrible.']\n",
        "\n",
        "for tweet in sentiment_analysis:\n",
        "\t# Find all matches of regex in tweet\n",
        "    negative_matches = re.findall(r\"(hate|dislike|disapprove).+?(?:movie|concert)\\s(.+?)\\.\", tweet)\n",
        "    \n",
        "    # Complete format to print out the results\n",
        "    print(\"Negative comments found {}\".format(negative_matches))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha47699ieZGw",
        "outputId": "8809866c-940e-4591-d27b-a667759bd61f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative comments found [('dislike', 'The cabin and the ant')]\n",
            "Negative comments found [('disapprove', 'Honest with you')]\n",
            "Negative comments found [('dislike', 'After twelve Tour')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Backreferences**\n",
        "\n",
        "### ***Parsing PDF files***\n",
        "\n",
        "You now need to work on another small project you have been delaying. Your company gave you some PDF files of signed contracts. The goal of the project is to create a database with the information you parse from them. Three of these columns should correspond to the day, month, and year when the contract was signed.\n",
        "The dates appear as `Signed on 05/24/2016` (`05` indicating the month, `24` the day). You decide to use capturing groups to extract this information. Also, you would like to retrieve that information so you can store it separately in different variables.\n",
        "\n",
        "You decide to do a proof of concept.\n",
        "\n",
        "The variable `contract` containing the text of one contract and the `re` module are already loaded in your session.\n",
        "\n",
        "* Write a regex that captures the month, day, and year in which the `contract` was signed. Scan `contract` for matches.\n",
        "* Assign each captured group to the corresponding keys in the dictionary.\n",
        "* Complete the positional method to print out the captured groups. Use the values corresponding to each key in the dictionary."
      ],
      "metadata": {
        "id": "WXxrLNGshMOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contract = 'Provider will invoice Client for Services performed within 30 days of performance.  Client will pay Provider as set forth in each Statement of Work within 30 days of receipt and acceptance of such invoice. It is understood that payments to Provider for services rendered shall be made in full as agreed, without any deductions for taxes of any kind whatsoever, in conformity with Provider’s status as an independent contractor. Signed on 03/25/2001.'\n",
        "dates = re.search(r\"Signed\\son\\s(\\d{2})/(\\d{2})/(\\d{4})\", contract)\n",
        "\n",
        "# Assign to each key the corresponding match\n",
        "signature = {\"day\": dates.group(2),\n",
        "             \"month\": dates.group(1),\n",
        "             \"year\": dates.group(3)}\n",
        "             \n",
        "# Complete the format method to print-out\n",
        "print(\"Our first contract is dated back to {data[year]}. Particularly, the day {data[day]} of the month {data[month]}.\".format(data=signature))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntUiZNrKf4Gy",
        "outputId": "c83428aa-f9c1-4450-d845-1eb5bc6da648"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our first contract is dated back to 2001. Particularly, the day 25 of the month 03.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only if you use **`.search()`** and **`.match()`**, can you use **`.group()`** to retrieve the groups.\n",
        "\n",
        "### ***Close the tag, please!***\n",
        "\n",
        "You need to write a short script for checking that every HTML tag that is open has its proper closure.\n",
        "\n",
        "You have an example of a string containing HTML tags:\n",
        "\n",
        "`<title>The Data Science Company</title>`\n",
        "\n",
        "You learn that an opening HTML tag is always at the beginning of the string. It appears inside `<>`. A closing tag also appears inside `<>`, but it is preceded by `/`.\n",
        "\n",
        "You also remember that capturing groups can be referenced using numbers, e.g `\\4`.\n",
        "\n",
        "The list `html_tags`, containing three strings with HTML tags, and the `re` module are loaded in your session. \n",
        "\n",
        "* Complete the regex in order to match closed HTML tags. Find if there is a match in each string of the list `html_tags`. Assign the result to `match_tag`.\n",
        "\n",
        "* If a match is found, print the first group captured and saved in `match_tag`.\n",
        "\n",
        "* If no match is found, complete the regex to match only the text inside the HTML tag. Assign it to `notmatch_tag`.\n",
        "\n",
        "* Print the first group captured by the regex and save it in `notmatch_tag`."
      ],
      "metadata": {
        "id": "oeaLmrUoiw2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "html_tags = ['<body>Welcome to our course! It would be an awesome experience</body>',\n",
        "             '<article>To be a data scientist, you need to have knowledge in statistics and mathematics</article>',\n",
        "             '<nav>About me Links Contact me!']\n",
        "\n",
        "for string in html_tags:\n",
        "    # Complete the regex and find if it matches a closed HTML tags\n",
        "    match_tag =  re.match(r\"<(\\w+)>.*?</\\1>\", string)\n",
        " \n",
        "    if match_tag:\n",
        "        # If it matches print the first group capture\n",
        "        print(\"Your tag {} is closed\".format(match_tag.group(1))) \n",
        "    else:\n",
        "        # If it doesn't match capture only the tag \n",
        "        notmatch_tag = re.match(r\"<(\\w+)>\", string)\n",
        "        # Print the first group capture\n",
        "        print(\"Close your {} tag!\".format(notmatch_tag.group(1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6MUnOegipBE",
        "outputId": "0dad2da7-8bd9-4922-9c2c-bd47930dadd1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your tag body is closed\n",
            "Your tag article is closed\n",
            "Close your nav tag!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Reeepeated characters***\n",
        "\n",
        "Back to your sentiment analysis! Your next task is to replace elongated words that appear in the tweets. We define an elongated word as a word that contains a repeating character twice or more times. e.g. `\"Awesoooome\"`.\n",
        "\n",
        "Replacing those words is very important since a classifier will treat them as a different term from the source words lowering their frequency.\n",
        "\n",
        "To find them, you will use capturing groups and reference them back using numbers. E.g `\\4`.\n",
        "\n",
        "If you want to find a match for `Awesoooome`, you first need to capture `Awes`. Then, match `o` and reference the same character back, and then, `me`.\n",
        "\n",
        "The list `sentiment_analysis`, containing the text of three tweets, and the `re` module are loaded in your session. \n",
        "\n",
        "* Complete the regular expression to match an elongated word as described.\n",
        "\n",
        "* Search the elements in `sentiment_analysis` list to find out if they contain elongated words. Assign the result to `match_elongated`.\n",
        "\n",
        "* Assign the captured group number zero to the variable `elongated_word`.\n",
        "\n",
        "* Print the result contained in the variable `elongated_word`."
      ],
      "metadata": {
        "id": "NCkvE5plk2hU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analysis = ['@marykatherine_q i know! I heard it this morning and wondered the same thing. Moscooooooow is so behind the times',\n",
        "                      'Staying at a friends house...neighborrrrrrrs are so loud-having a party',\n",
        "                      'Just woke up an already have read some e-mail']\n",
        "              \n",
        "for tweet in sentiment_analysis:\n",
        "    # Find if there is a match in each tweet \n",
        "    match_elongated = re.search(r\"\\w+(\\w)\\1\\w*\", tweet)\n",
        "      \n",
        "    if match_elongated:\n",
        "        # Assign the captured group zero \n",
        "        elongated_word = match_elongated.group(0)\n",
        "            \n",
        "        # Complete the format method to print the word\n",
        "        print(\"Elongated word found: {word}\".format(word=elongated_word))\n",
        "    else:\n",
        "        print(\"No elongated word found\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyH-MQfEkxQV",
        "outputId": "c1322386-f783-4dbd-badc-f455e48d62d6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elongated word found: Moscooooooow\n",
            "Elongated word found: neighborrrrrrrs\n",
            "No elongated word found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should remember that the group zero stands for the entire expression matched. It's always helpful to keep that in mind.\n",
        "\n",
        "## **Look ahead**\n",
        "\n",
        "* ## **Look positive `?=` and negative `?!`**"
      ],
      "metadata": {
        "id": "G7fG5rULmo_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(re.findall(r\"\\w+\\.txt \", \"tweets.txt transferred, mypass.txt transferred, keywords.txt error\"))\n",
        "\n",
        "# เอาเฉพาะตัวที่ตามด้วยคำว่า transferred (Look positive)\n",
        "print(re.findall(r\"\\w+\\.txt(?=\\stransferred) \", \"tweets.txt transferred, mypass.txt transferred, keywords.txt error\"))\n",
        "\n",
        "# เอาเฉพาะตัวที่  \"ไม่ได้\"  ตามด้วยคำว่า transferred (Look negative)\n",
        "print(re.findall(r\"\\w+\\.txt(?!\\stransferred) \", \"tweets.txt transferred, mypass.txt transferred, keywords.txt error\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bp-3QqZnmhBd",
        "outputId": "329a0fec-1882-4848-f302-7478514e8230"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tweets.txt ', 'mypass.txt ', 'keywords.txt ']\n",
            "['tweets.txt ', 'mypass.txt ']\n",
            "['keywords.txt ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Look-behind**\n",
        "\n",
        "* positive `?<=`\n",
        "* negative `?<!`"
      ],
      "metadata": {
        "id": "_G6C4GbQVu8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(re.findall(r\"(?<=Member:\\s)\\w+\\s\\w+\", \"Member: Angus Young, Member: Chris Slade, Past: Malcolm Young, Past: Cliff Williams.\"))\n",
        "print(re.findall(r\"(?<=white\\s)(cat|dog)\", \"My white cat sat at the table. However, my brown dog was lying on the couch.\"))\n",
        "print(re.findall(r\"(?<!white\\s)(cat|dog)\", \"My white cat sat at the table. However, my brown dog was lying on the couch.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwiKzcrSRH4a",
        "outputId": "d5b40a07-1b47-4ee5-8089-b41f6794d8d2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Angus Young', 'Chris Slade']\n",
            "['cat']\n",
            "['dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Surrounding words***\n",
        "\n",
        "Now, you want to perform some visualizations with your `sentiment_analysis` dataset. You are interested in the words surrounding python. You want to count how many times a specific words appears right before and after it.\n",
        "\n",
        "***Positive lookahead*** `(?=)` makes sure that first part of the expression is followed by the lookahead expression. ***Positive lookbehind*** `(?<=)` returns all matches that are preceded by the specified pattern.\n",
        "\n",
        "The variable `sentiment_analysis`, containing the text of one tweet, and the `re` module are loaded in your session. \n",
        "\n",
        "\n",
        "* Get all the words that are followed by the word `python` in `sentiment_analysis`. Print out the word found.\n",
        "\n",
        "* Get all the words that are preceded by the word `python` or `Python` in `sentiment_analysis`. Print out the words found."
      ],
      "metadata": {
        "id": "RkKDgWf2crjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analysis = 'You need excellent python skills to be a data scientist. Must be! Excellent python'\n",
        "\n",
        "# Positive lookahead\n",
        "look_ahead = re.findall(r\"\\w+\\s(?=python)\", sentiment_analysis)\n",
        "\n",
        "# Print out\n",
        "print(look_ahead)\n",
        "\n",
        "# Positive lookahead\n",
        "look_ahead = re.findall(r\"\\w+(?=\\spython)\", sentiment_analysis)\n",
        "\n",
        "# Print out\n",
        "print(look_ahead)\n",
        "\n",
        "# Positive lookbehind\n",
        "look_behind = re.findall(r\"(?<=Python\\s|python\\s)\\w+\", sentiment_analysis)\n",
        "\n",
        "# Print out\n",
        "print(look_behind)\n",
        "\n",
        "# Positive lookbehind\n",
        "look_behind = re.findall(r\"(?<=[Pp]ython\\s)\\w+\", sentiment_analysis)\n",
        "\n",
        "# Print out\n",
        "print(look_behind)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5LFFzO_WfR9",
        "outputId": "defa41e5-c9f7-4e22-850a-85419aaceabd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['excellent ', 'Excellent ']\n",
            "['excellent', 'Excellent']\n",
            "['skills']\n",
            "['skills']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to know that positive lookahead will return the text matched by the first part of the expression after asserting that it is followed by the lookahead expression while positive lookbehind will return all matches that follow a specific pattern. \n",
        "\n",
        "### ***Filtering phone numbers***\n",
        "\n",
        "Now, you need to write a script for a cell-phone searcher. It should scan a list of phone numbers and return those that meet certain characteristics.\n",
        "\n",
        "The phone numbers in the list have the structure:\n",
        "\n",
        "* Optional area code: 3 numbers\n",
        "* Prefix: 4 numbers\n",
        "* Line number: 6 numbers\n",
        "* Optional extension: 2 numbers\n",
        "\n",
        "E.g. `654-8764-439434-01`.\n",
        "\n",
        "You decide to use `.findall()` and the non-capturing group's negative lookahead `(?!)` and negative lookbehind `(?<!)`.\n",
        "\n",
        "The list `cellphones`, containing three phone numbers, and the `re` module are loaded in your session. \n",
        "\n",
        "* Get all cell phones numbers that are not preceded by the optional area code.\n",
        "* Get all the cell phones numbers that are not followed by the optional extension."
      ],
      "metadata": {
        "id": "EOX2dzvQeZvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cellphones = ['4564-646464-01', '345-5785-544245', '6476-579052-01']\n",
        "\n",
        "for phone in cellphones:\n",
        "    # Get all phone numbers not preceded by area code\n",
        "    number = re.findall(r\"(?<!\\d{3}-)\\d{4}-\\d{6}-\\d{2}\", phone)\n",
        "    print(number)\n",
        "\n",
        "for phone in cellphones:\n",
        "    # Get all phone numbers not followed by optional extension\n",
        "    number = re.findall(r\"\\d{3}-\\d{4}-\\d{6}(?!-\\d{2})\", phone)\n",
        "    print(number)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vIXm_Udd1BD",
        "outputId": "4b617976-0afc-48fe-a822-588e17600df5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['4564-646464-01']\n",
            "[]\n",
            "['6476-579052-01']\n",
            "[]\n",
            "['345-5785-544245']\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative lookarounds work in a similar way to positive lookarounds. They are very helpful when we are looking to exclude certain patterns from our analysis.\n",
        "\n",
        "# **Intoduction to Regex**\n",
        "\n",
        "## ***Practicing regular expressions: `re.split()` and `re.findall()`***\n",
        "\n",
        "Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at `my_string` first by printing it in the IPython Shell, to determine how you might best match the different steps.\n",
        "\n",
        "Note: It's important to prefix your regex patterns with `r` to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, `\"\\n\"` in Python is used to indicate a new line, but if you use the `r` prefix, it will be interpreted as the raw string `\"\\n\"` - that is, the character `\"\\\"` followed by the character `\"n\"` - and not as a new line.\n",
        "\n",
        "The regular expression module `re` has already been imported for you.\n",
        "\n",
        "*Remember that the syntax for the regex library is to always to pass the pattern first, and then the string second.*\n",
        "\n",
        "* Split `my_string` on each sentence ending. To do this:\n",
        "\n",
        "   * Write a pattern called `sentence_endings` to match sentence endings (`.?!`).\n",
        "   * Use `re.split()` to split `my_string` on the pattern and print the result.\n",
        "\n",
        "* Find and print all capitalized words in `my_string` by writing a pattern called `capitalized_words` and using `re.findall()`.\n",
        "\n",
        "   * Remember the `[a-z]` pattern shown in the video to match lowercase groups? Modify that pattern appropriately in order to match uppercase groups.\n",
        "\n",
        "* Write a pattern called spaces to match one or more spaces (`\"\\s+\"`) and then use `re.split()` to split my_string on this pattern, keeping all punctuation intact. Print the result.\n",
        "\n",
        "* Find all digits in `my_string` by writing a pattern called digits (`\"\\d+\"`) and using `re.findall()`. Print the result."
      ],
      "metadata": {
        "id": "9f4an29Xgwno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
        "\n",
        "print(re.split(r\"[.?!]\", my_string))\n",
        "\n",
        "print(re.findall(r\"[A-Z]\\w+\", my_string))\n",
        "\n",
        "print(re.split(r'\\s+', my_string))\n",
        "\n",
        "print(re.findall(r'\\d+', my_string))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSAhTzvlgn9h",
        "outputId": "1193fc3d-b9b9-4e65-fe37-35573f006ab8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
            "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
            "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
            "['4', '19']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Tokenization**\n",
        "\n",
        "## ***Word tokenization with NLTK***\n",
        "\n",
        "Here, you'll be using the first scene of Monty Python's Holy Grail, which has been pre-loaded as `scene_one`. \n",
        "\n",
        "Your job in this exercise is to utilize **`word_tokenize`** and **`sent_tokenize`** from **`nltk.tokenize`** to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail.\n",
        "\n",
        "\n",
        "* Tokenize all the sentences in `scene_one` using the `sent_tokenize()` function.\n",
        "\n",
        "* Tokenize the fourth sentence in `sentences`, which you can access as `sentences[3]`, using the `word_tokenize()` function.\n",
        "\n",
        "* Find the unique tokens in the entire scene by using `word_tokenize()` on `scene_one` and then converting it into a set using `set()`.\n",
        "\n",
        "* Print the unique tokens found. \n",
        "\n"
      ],
      "metadata": {
        "id": "dSGBYwZCQ0oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "scene_one = \"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\"\n",
        "\n",
        "# Split scene_one into sentences: sentences\n",
        "sentences = sent_tokenize(scene_one)\n",
        "\n",
        "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
        "tokenized_sent = word_tokenize(sentences[3])\n",
        "\n",
        "# Make a set of unique tokens in the entire scene: unique_tokens\n",
        "unique_tokens = set(word_tokenize(scene_one))\n",
        "\n",
        "# Print the unique tokens result\n",
        "print(unique_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSjzW31FNZtK",
        "outputId": "b995936e-2a84-4c13-845e-74f9fc25d582"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'But', 'swallow', 'found', 'a', 'simple', 'south', 'That', 'husk', 'house', 'coconuts', '#', 'Yes', 'In', 'wings', 'speak', 'them', 'course', 'SCENE', 'that', 'is', 'together', 'length', 'five', 'wants', 'maybe', 'or', 'lord', 'clop', 'creeper', 'you', 'suggesting', 'mean', 'just', 'join', 'Britons', \"'d\", 'may', ':', 'sovereign', 'its', 'ask', 'must', 'anyway', 'Mercea', 'with', 'pound', 'in', 'non-migratory', 'held', 'our', 'feathers', 'Ridden', 'carrying', 'King', 'temperate', 'needs', 'and', '2', 'Supposing', 'under', 'England', 'they', 'get', 'here', 'strangers', 'your', 'be', 'European', 'one', 'then', 'seek', 'Saxons', 'I', 'winter', 'zone', 'to', \"n't\", 'why', 'but', 'minute', 'my', \"'ve\", 'go', 'since', 'climes', 'have', 'me', 'tell', 'kingdom', 'ounce', 'We', 'of', 'are', 'No', 'matter', 'dorsal', 'it', 'there', 'he', 'at', 'land', 'weight', 'It', 'warmer', 'covered', 'martin', 'Pull', 'trusty', 'fly', 'bring', 'got', 'not', 'bangin', 'other', 'every', 'all', 'yet', 'Am', 'A', 'velocity', \"'re\", 'carried', 'strand', 'Pendragon', 'am', 'use', 'on', '!', 'if', 'Camelot', 'the', 'who', 'court', 'KING', 'ratios', 'guiding', '...', 'could', 'times', 'maintain', 'sun', 'question', 'Arthur', 'Halt', 'grips', 'Are', 'from', 'does', 'order', 'this', 'castle', 'air-speed', 'an', 'SOLDIER', 'two', 'Uther', 'What', 'wind', 'swallows', 'grip', 'empty', 'plover', 'these', 'tropical', 'Not', 'Who', 'right', 'bird', 'breadth', 'Patsy', 'They', 'son', 'second', 'Please', 'servant', 'back', 'yeah', 'Whoa', \"'em\", '?', 'You', 'Found', 'goes', 'horse', 'snows', 'Oh', 'migrate', 'Wait', 'Will', 'knights', 'interested', 'will', 'search', 'carry', 'Listen', 'through', \"'m\", 'Where', 'line', 'ARTHUR', 'agree', 'forty-three', 'Court', 'So', '.', 'ridden', 'master', '1', 'Well', 'defeator', 'by', ',', 'coconut', 'The', 'do', 'where', 'African', '[', 'point', \"'s\", '--', 'using', ']', \"'\", 'halves', 'beat'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is fundamental to NLP, and you'll end up using it a lot in text mining and information retrieval projects.\n",
        "\n",
        "## ***More regex with `re.search()`***\n",
        "\n",
        "In this exercise, you'll utilize `re.search()` and `re.match()` to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the nltk corpora.\n",
        "\n",
        "You have both `scene_one` and `sentences` available from the last exercise; now you can use them with `re.search()` and `re.match()` to extract and match more text.\n",
        "\n",
        "* Use `re.search()` to search for the first occurrence of the word `\"coconuts\"` in `scene_one`. Store the result in `match`.\n",
        "\n",
        "* Print the start and end indexes of match using its **`.start()`** and **`.end()`** methods, respectively.\n",
        "\n",
        "* Write a regular expression called `pattern1` to find anything in square brackets.\n",
        "\n",
        "* Use **`re.search()`** with the pattern to find the first text in `scene_one` in square brackets in the scene. Print the result.\n",
        "\n",
        "* Create a pattern to match the script notation (e.g. `Character:`), assigning the result to `pattern2`. Remember that you will want to match any words or spaces that precede the `:` (such as the space within `SOLDIER #1:`).\n",
        "\n",
        "* Use `re.match()` with your new pattern to find and print the script notation in the fourth line. The tokenized sentences are available in your namespace as `sentences`."
      ],
      "metadata": {
        "id": "l7vBXYinSef-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
        "match = re.search(\"coconuts\", scene_one)\n",
        "\n",
        "# Print the start and end indexes of match\n",
        "print(match.start(), match.end())\n",
        "\n",
        "# Write a regular expression to search for anything in square brackets: pattern1\n",
        "pattern1 = r\"\\[.*\\]\"\n",
        "\n",
        "# Use re.search to find the first text in square brackets\n",
        "print(re.search(pattern1, scene_one))\n",
        "\n",
        "# Find the script notation at the beginning of the fourth sentence and print it\n",
        "pattern2 = r\"[A-Za-z]+:\"\n",
        "print(re.match(pattern2, sentences[3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0Dbr3eJSPRL",
        "outputId": "375b628f-f69c-4587-e04d-f7334b9a270f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "580 588\n",
            "<re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n",
            "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Advanced tokenization with NLTK and regex**\n",
        "\n",
        "## ***Choosing a tokenizer***\n",
        "\n",
        "Given the following string, which of the below patterns is the best tokenizer? If possible, you want to retain sentence punctuation as separate tokens, but have `'#1'` remain a single token.\n",
        "\n",
        "```\n",
        "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
        "```\n",
        "\n",
        "The string is available in your workspace as `my_string`\n",
        "\n",
        "Additionally, **`regexp_tokenize`** has been imported from `nltk.tokenize`. You can use **`regexp_tokenize(string, pattern)`** with `my_string`.\n",
        "\n"
      ],
      "metadata": {
        "id": "XY9tRR2jVx2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
        "\n",
        "print(regexp_tokenize(my_string, r\"(\\w+|#\\d|\\?|!)\"))\n",
        "print(re.findall(r\"(\\w+|#\\d|\\?|!)\", my_string))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43pabpBDUsQo",
        "outputId": "d7cbf1b7-7914-44e0-f083-b3ed6f518d74"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SOLDIER', '#1', 'Found', 'them', '?', 'In', 'Mercea', '?', 'The', 'coconut', 's', 'tropical', '!']\n",
            "['SOLDIER', '#1', 'Found', 'them', '?', 'In', 'Mercea', '?', 'The', 'coconut', 's', 'tropical', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Regex with NLTK tokenization***\n",
        "\n",
        "Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using `nltk` and `regex`. The **`nltk.tokenize.TweetTokenizer`** class gives you some extra methods and attributes for parsing tweets.\n",
        "\n",
        "Here, you're given some example tweets to parse using both **`TweetTokenizer`** and **`regexp_tokenize`** from the `nltk.tokenize` module. These example tweets have been pre-loaded into the variable `tweets`.\n",
        "\n",
        "*Unlike the syntax for the regex library, with `nltk_tokenize()` you pass the pattern as the second argument.*\n",
        "\n",
        "* Call `regexp_tokenize()` with this regex pattern on the **first tweet** in `tweets` and assign the result to `hashtags`.\n",
        "\n",
        "* Print `hashtags`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0QZEctorbVZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
        "          '#NLP is super fun! <3 #learning',\n",
        "          'Thanks @datacamp :) #nlp #python']\n",
        "\n",
        "from nltk.tokenize import regexp_tokenize, TweetTokenizer\n",
        "\n",
        "hashtags = regexp_tokenize(tweets[0], r\"#\\w+\")\n",
        "print(hashtags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPXX_ZGpbDcI",
        "outputId": "444f0b2c-ae09-4c72-a3d2-5314d840796e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#nlp', '#python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Write a new pattern to match mentions and hashtags. A mention is something like `@DataCamp`.\n",
        "\n",
        "Then, call **`regexp_tokenize()`** with your new hashtag pattern on the *last* tweet in tweets and assign the result to `mentions_hashtags`.\n",
        "\n",
        "   * You can access the last element of a list using `-1` as the index, for example, `tweets[-1]`.\n",
        "\n",
        "Print `mentions_hashtags`."
      ],
      "metadata": {
        "id": "Eg5dVb-fc9ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the pattern on the last tweet in the tweets list\n",
        "mentions_hashtags = regexp_tokenize(tweets[-1], r\"([#@]\\w+)\")\n",
        "print(mentions_hashtags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPZEE9eTc1Nh",
        "outputId": "b13ce7a9-9e79-4869-8629-ecae295c921b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@datacamp', '#nlp', '#python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Create an instance of **`TweetTokenizer`** called `tknzr` and use it inside a list comprehension to tokenize each tweet into a new list called `all_tokens`.\n",
        "\n",
        "   * To do this, use the `.tokenize()` method of `tknzr`, with `t` as your iterator variable.\n",
        "\n",
        "* Print `all_tokens`."
      ],
      "metadata": {
        "id": "EyWEKT4Jd9ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the TweetTokenizer to tokenize all tweets into one list\n",
        "tknzr = TweetTokenizer()\n",
        "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXyIFyfXd7GL",
        "outputId": "a08010a5-8d80-41d6-c0de-51151cee81a4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Non-ascii tokenization***\n",
        "\n",
        "In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!\n",
        "\n",
        "Here, you have access to a string called `german_text`, which has been printed for you in the Shell. Notice the emoji and the German characters!\n",
        "\n",
        "The following modules have been pre-imported from `nltk.tokenize`: **`regexp_tokenize`** and **`word_tokenize`**.\n",
        "\n",
        "Unicode ranges for emoji are:\n",
        "\n",
        "`('\\U0001F300'-'\\U0001F5FF')`, \n",
        "`('\\U0001F600-\\U0001F64F')`, \n",
        "`('\\U0001F680-\\U0001F6FF')`, and \n",
        "`('\\u2600'-\\u26FF-\\u2700-\\u27BF')`.\n",
        "\n",
        "* Tokenize all the words in `german_text` using `word_tokenize()`, and print the result.\n",
        "\n",
        "* Tokenize only the capital words in `german_text`.\n",
        "\n",
        "   * First, write a pattern called `capital_words` to match only capital words. Make sure to check for the German `Ü`! To use this character in the exercise, copy and paste it from these instructions.\n",
        "   * Then, tokenize it using **`regexp_tokenize()`**.\n",
        "\n",
        "* Tokenize only the emoji in `german_text`. Use **`regexp_tokenize()`** to tokenize the emoji."
      ],
      "metadata": {
        "id": "ip2suTMMexbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "german_text = 'Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕'\n",
        "\n",
        "# Tokenize and print all words in german_text\n",
        "all_words = word_tokenize(german_text)\n",
        "print(all_words)\n",
        "\n",
        "# Tokenize and print only capital words\n",
        "print(regexp_tokenize(german_text, r\"[A-ZÜ]\\w+\"))\n",
        "\n",
        "# Tokenize and print only emoji\n",
        "print(regexp_tokenize(german_text, \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp-EUmdEem5f",
        "outputId": "d629adb4-5574-4c85-f76d-01793ca72d12"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n",
            "['Wann', 'Pizza', 'Und', 'Über']\n",
            "['🍕', '🚕']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Charting word length with NLTK**\n",
        "\n",
        "## ***Charting practice***\n",
        "\n",
        "Try using your new skills to find and chart the number of words per line in the script using `matplotlib`. The Holy Grail script is loaded for you, and you need to use regex to find the words per line.\n",
        "\n",
        "Using list comprehensions here will speed up your computations. For example: `my_lines = [tokenize(l) for l in lines]` will call a function tokenize on each line in the list lines. The new transformed list will be saved in the `my_lines` variable.\n",
        "\n",
        "You have access to the entire script in the variable `holy_grail`. Go for it!\n",
        "\n",
        "* Split the script `holy_grail` into lines using the newline (`'\\n'`) character.\n",
        "\n",
        "* Use `re.sub()` inside a list comprehension to replace the prompts such as `ARTHUR:` and `SOLDIER #1`.\n",
        "\n",
        "* Use a list comprehension to tokenize `lines` with `regexp_tokenize()`, keeping **only words**. Recall that the pattern for words is `\"\\w+\"`.\n",
        "\n",
        "* Use a list comprehension to create a list of line lengths called `line_num_words`.\n",
        "\n",
        "   * Use `t_line` as your iterator variable to iterate over `tokenized_lines`, and then `len()` function to compute line lengths.\n",
        "\n",
        "* Plot a histogram of `line_num_words` using `plt.hist()`. Don't forgot to use `plt.show()` as well to display the plot."
      ],
      "metadata": {
        "id": "H4ji5wmugxzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('grail.txt') as h:\n",
        "    holy_grail = h.read()\n",
        "\n",
        "    # Split the script into lines: lines\n",
        "    lines = holy_grail.split('\\n')\n",
        "\n",
        "    # Replace all script lines for speaker\n",
        "    lines = [re.sub(r\"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\", '', l) for l in lines]\n",
        "\n",
        "    # Tokenize each line: tokenized_lines\n",
        "    tokenized_lines = [regexp_tokenize(s, r\"\\w+\") for s in lines]\n",
        "\n",
        "    # Make a frequency list of lengths: line_num_words\n",
        "    line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
        "\n",
        "    # Plot a histogram of the line lengths\n",
        "    plt.hist(line_num_words, edgecolor='k')\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "sOTlCBQ6jo1z",
        "outputId": "dc63942b-3bac-4b17-e7d3-91a7dd93dbdf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOMklEQVR4nO3cbWyd5XnA8f+1mJdCNRLASlM74ExEVKhSB7JYKqZqIv0AFDVMohVTVaIqU77QlZZKJd0+VPu2SKgUpAkpIu3ChBhdikYUVa1YoJr2gbQOIN5Ch0sXYishbgdp1QpB6LUP585mgo2P42MffOX/kyw/b8fP/ehBf45vPzmRmUiSavmjfg9AktR7xl2SCjLuklSQcZekgoy7JBU00O8BAFx88cU5MjLS72FI0rJy4MCBX2Xm4Ez7PhBxHxkZYWxsrN/DkKRlJSIOzbbPaRlJKsi4S1JBxl2SCjLuklSQcZekgoy7JBVk3CWpIOMuSQUZd0kqaNnHfc3wJUREX77WDF/S78uXpBl9ID5+YCGOTh7m0jv39uXch7bf2JfzStJclv07d0nSexl3SSrIuEtSQcZdkgoy7pJUkHGXpIKMuyQVZNwlqSDjLkkFGXdJKsi4S1JBxl2SCjLuklSQcZekgoy7JBVk3CWpIOMuSQUZd0kqyLhLUkHGXZIKMu6SVFBXcY+Ir0XECxHxfEQ8FBHnRsS6iNgfEeMR8XBEnN2OPaetj7f9I4t6BZKk95gz7hExBHwFGM3MjwMrgFuA7cDdmXkZ8Dqwpb1kC/B62353O06StIS6nZYZAD4UEQPAecAR4Fpgd9u/C7ipLW9q67T9GyMiejJaSVJX5ox7Zk4CdwGv0on6ceAA8EZmnmiHTQBDbXkIONxee6Idf9GpPzcitkbEWESMTU1NLfQ6JEnTdDMts4rOu/F1wEeB84HrFnrizNyRmaOZOTo4OLjQHydJmqabaZlPA7/MzKnMfBt4BLgGWNmmaQCGgcm2PAmsBWj7LwB+3dNRS5LeVzdxfxXYEBHntbnzjcCLwBPAze2YzcCjbXlPW6ftfzwzs3dDliTNpZs59/10/jD6FPBce80O4E7gjogYpzOnvrO9ZCdwUdt+B7BtEcYtSXofA3MfApn5LeBbp2x+Bbh6hmPfBD638KFJkk6X/0JVkgoy7pJUkHGXpIKMuyQVZNwlqSDjLkkFGXdJKsi4S1JBxl2SCjLuklSQcZekgoy7JBVk3CWpIOMuSQUZd0kqyLhLUkHGXZIKMu6SVJBxl6SCjLskFWTcJakg4y5JBRl3SSrIuEtSQcZdkgoy7pJUkHGXpIKMuyQVZNwlqSDjLkkFGXdJKsi4S1JBxl2SCjLuklSQcZekgoy7JBXUVdwjYmVE7I6IlyLiYER8MiIujIjHIuLl9n1VOzYi4t6IGI+IZyPiqsW9BEnSqbp9534P8KPM/BjwCeAgsA3Yl5nrgX1tHeB6YH372grc19MRS5LmNGfcI+IC4FPAToDMfCsz3wA2AbvaYbuAm9ryJuCB7HgSWBkRa3o8bknS++jmnfs6YAr4XkQ8HRH3R8T5wOrMPNKOOQqsbstDwOFpr59o294lIrZGxFhEjE1NTZ3+FUiS3qObuA8AVwH3ZeaVwO/4/ykYADIzgZzPiTNzR2aOZubo4ODgfF4qSZpDN3GfACYyc39b300n9q+dnG5p34+1/ZPA2mmvH27bJElLZM64Z+ZR4HBEXN42bQReBPYAm9u2zcCjbXkPcGt7amYDcHza9I0kaQkMdHnc3wAPRsTZwCvAl+j8j+H7EbEFOAR8vh37Q+AGYBz4fTtWkrSEuop7Zj4DjM6wa+MMxyZw28KGJUlaCP+FqiQVZNwlqSDjLkkFGXdJKsi4S1JBxl2SCjLuklSQcZekgoy7JBVk3CWpIOMuSQUZd0kqyLhLUkHGXZIKMu6SVJBxl6SCjLskFWTcJakg4y5JBRl3SSrIuEtSQcZdkgoy7pJUkHGXpIKMuyQVZNwlqSDjLkkFGXdJKsi4S1JBxl2SCjLuklSQcZekgoy7JBVk3CWpIOMuSQUZd0kqqOu4R8SKiHg6Iva29XURsT8ixiPi4Yg4u20/p62Pt/0jizR2SdIs5vPO/Xbg4LT17cDdmXkZ8DqwpW3fArzett/djpMkLaGu4h4Rw8BngPvbegDXArvbIbuAm9ryprZO27+xHS9JWiLdvnP/DvAN4A9t/SLgjcw80dYngKG2PAQcBmj7j7fj3yUitkbEWESMTU1Nnd7oJUkzmjPuEXEjcCwzD/TyxJm5IzNHM3N0cHCwlz9aks54A10ccw3w2Yi4ATgX+GPgHmBlRAy0d+fDwGQ7fhJYC0xExABwAfDrno9ckjSrOd+5Z+Y3M3M4M0eAW4DHM/MLwBPAze2wzcCjbXlPW6ftfzwzs6ejliS9r4U8534ncEdEjNOZU9/Ztu8ELmrb7wC2LWyIkqT56mZa5v9k5k+An7TlV4CrZzjmTeBzPRibJOk0+S9UJakg4y5JBRl3SSrIuEtSQcZdkgoy7pJUkHGXpIKMuyQVZNwlqSDjLkkFGXdJKsi4S1JBxl2SCjLuklSQcZekgoy7JBVk3CWpIOMuSQUZd0kqyLhLUkHGXZIKMu6SVJBxl6SCjLskFWTcJakg4y5JBRl3SSrIuEtSQcZdkgoy7pJUkHGXpIKMuyQVZNwlqSDjLkkFGXdJKsi4S1JBc8Y9ItZGxBMR8WJEvBARt7ftF0bEYxHxcvu+qm2PiLg3IsYj4tmIuGqxL0KS9G7dvHM/AXw9M68ANgC3RcQVwDZgX2auB/a1dYDrgfXtaytwX89HLUl6X3PGPTOPZOZTbfm3wEFgCNgE7GqH7QJuasubgAey40lgZUSs6fXAJUmzm9ece0SMAFcC+4HVmXmk7ToKrG7LQ8DhaS+baNskSUuk67hHxIeBHwBfzczfTN+XmQnkfE4cEVsjYiwixqampubzUknSHLqKe0ScRSfsD2bmI23zayenW9r3Y237JLB22suH27Z3ycwdmTmamaODg4OnO35J0gy6eVomgJ3Awcz89rRde4DNbXkz8Oi07be2p2Y2AMenTd9IkpbAQBfHXAN8EXguIp5p2/4W+Afg+xGxBTgEfL7t+yFwAzAO/B74Ui8HLEma25xxz8z/BGKW3RtnOD6B2xY4LknSAnTzzl2zWXEWnVmrpfWRobUcmXh1yc8rafkw7gvxzttceufeJT/toe03Lvk5JS0vfraMJBVk3CWpIOMuSQUZd0kqyLhLUkHGXZIKMu6SVJBxl6SCjLskFWTcJakg4y5JBRl3SSrIuEtSQcZdkgoy7pJUkHGXpIKMuyQVZNwlqSDjLkkFGXdJKsi4S1JBxl2SCjLuklSQcZekgoy7JBVk3CWpoIF+D0CnYcVZRERfTv2RobUcmXi1L+eW1D3jvhy98zaX3rm3L6c+tP3GvpxX0vw4LSNJBRl3SSrIuEtSQcZdkgoy7pJUkE/LaH769Bimj2BK82PcNT99egzTRzCl+VmUaZmIuC4ifh4R4xGxbTHOIUmaXc/jHhErgH8ErgeuAP4qIq7o9Xl0hmnTQUv9tWb4kn5fuXRaFmNa5mpgPDNfAYiIfwE2AS8uwrl0pujXdNBdf9m3j3pYcfa5vPPWm2fMeaF/f1tZM3wJRycPL/l5YfGuOTKztz8w4mbgusz867b+ReDPMvPLpxy3FdjaVi8Hfn6ap7wY+NVpvna58VrrOpOu12vtnUszc3CmHX37g2pm7gB2LPTnRMRYZo72YEgfeF5rXWfS9XqtS2Mx/qA6Caydtj7ctkmSlshixP1nwPqIWBcRZwO3AHsW4TySpFn0fFomM09ExJeBHwMrgO9m5gu9Ps80C57aWUa81rrOpOv1WpdAz/+gKknqPz9bRpIKMu6SVNCyjnvljzmIiLUR8UREvBgRL0TE7W37hRHxWES83L6v6vdYeyUiVkTE0xGxt62vi4j97f4+3P5Av+xFxMqI2B0RL0XEwYj4ZNX7GhFfa//9Ph8RD0XEuVXua0R8NyKORcTz07bNeB+j4952zc9GxFWLPb5lG/cz4GMOTgBfz8wrgA3Abe36tgH7MnM9sK+tV3E7cHDa+nbg7sy8DHgd2NKXUfXePcCPMvNjwCfoXHO5+xoRQ8BXgNHM/DidByxuoc59/SfgulO2zXYfrwfWt6+twH2LPbhlG3emfcxBZr4FnPyYgxIy80hmPtWWf0snAEN0rnFXO2wXcFNfBthjETEMfAa4v60HcC2wux1S4loj4gLgU8BOgMx8KzPfoOh9pfNE3ociYgA4DzhCkfuamf8B/M8pm2e7j5uAB7LjSWBlRKxZzPEt57gPAdM/DGKibSsnIkaAK4H9wOrMPNJ2HQVW92tcPfYd4BvAH9r6RcAbmXmirVe5v+uAKeB7bQrq/og4n4L3NTMngbuAV+lE/ThwgJr39aTZ7uOS92o5x/2MEBEfBn4AfDUzfzN9X3aeY132z7JGxI3Ascw80O+xLIEB4Crgvsy8Evgdp0zBFLqvq+i8Y10HfBQ4n/dOY5TV7/u4nONe/mMOIuIsOmF/MDMfaZtfO/nrXPt+rF/j66FrgM9GxH/TmV67ls689Mr26zzUub8TwERm7m/ru+nEvuJ9/TTwy8ycysy3gUfo3OuK9/Wk2e7jkvdqOce99McctDnnncDBzPz2tF17gM1teTPw6FKPrdcy85uZOZyZI3Tu4+OZ+QXgCeDmdliVaz0KHI6Iy9umjXQ+DrvcfaUzHbMhIs5r/z2fvNZy93Wa2e7jHuDW9tTMBuD4tOmbxZGZy/YLuAH4L+AXwN/1ezw9vrY/p/Mr3bPAM+3rBjpz0fuAl4F/By7s91h7fN1/Aexty38C/BQYB/4VOKff4+vRNf4pMNbu7b8Bq6reV+DvgZeA54F/Bs6pcl+Bh+j8LeFtOr+RbZntPgJB5+m+XwDP0XmCaFHH58cPSFJBy3laRpI0C+MuSQUZd0kqyLhLUkHGXZIKMu6SVJBxl6SC/hewNlVRRa/DuAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word counts with bag-of-words**\n",
        "\n",
        "## ***Building a Counter with bag-of-words***\n",
        "\n",
        "In this exercise, you'll build your first (in this course) bag-of-words counter using a Wikipedia article, which has been pre-loaded as `article`. Try doing the bag-of-words without looking at the full article text, and guessing what the topic is! If you'd like to peek at the title at the end, we've included it as `article_title`. Note that this article text has had very little preprocessing from the raw Wikipedia database entry.\n",
        "\n",
        "`word_tokenize` has been imported for you.\n",
        "\n",
        "\n",
        "* Use `word_tokenize()` to split the article into tokens.\n",
        "\n",
        "* Use a list comprehension with `t` as the iterator variable to convert all the tokens into lowercase. The **`.lower()`** method converts text into lowercase.\n",
        "\n",
        "* Create a bag-of-words counter called `bow_simple` by using **`Counter()`** with `lower_tokens` as the argument.\n",
        "\n",
        "* Use the **`.most_common()`** method of `bow_simple` to print the 10 most common tokens."
      ],
      "metadata": {
        "id": "YUAkFrOFnGfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "with open('wiki_text_debugging.txt') as f:\n",
        "\n",
        "    article = f.read()\n",
        "\n",
        "    # Tokenize the article: tokens\n",
        "    tokens = word_tokenize(article)\n",
        "\n",
        "    # Convert the tokens into lowercase: lower_tokens\n",
        "    lower_tokens = [t.lower() for t in tokens]\n",
        "\n",
        "    # Create a Counter with the lowercase tokens: bow_simple\n",
        "    bow_simple = Counter(lower_tokens)\n",
        "\n",
        "    # Print the 10 most common tokens\n",
        "    print(bow_simple.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEYikfu0lgrn",
        "outputId": "edb8f21c-cb32-4a27-e1c5-286f4ce52d04"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 69), ('to', 63), ('a', 60), ('``', 47), ('in', 44), ('and', 41)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Simple text preprocessing**\n",
        "\n",
        "## ***Text preprocessing practice***\n",
        "\n",
        "Now, it's your turn to apply the techniques you've learned to help clean up text for better NLP results. You'll need to remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text.\n",
        "\n",
        "You start with the same tokens you created in the last exercise: `lower_tokens`. You also have the **`Counter`** class imported.\n",
        "\n",
        "\n",
        "* Create a list `alpha_only` that contains only alphabetical characters. You can use the `.isalpha()` method to check for this.\n",
        "\n",
        "* Create another list called `no_stops` consisting of words from `alpha_only` that are not contained in `english_stops`.\n",
        "\n",
        "* Initialize a `WordNetLemmatizer` object called `wordnet_lemmatizer` and use its **`.lemmatize()`** method on the tokens in `no_stops` to create a new list called `lemmatized`.\n",
        "\n",
        "* Create a new **`Counter`** called bow with the lemmatized words.\n",
        "\n",
        "* Lastly, print the 10 most common tokens."
      ],
      "metadata": {
        "id": "4F3wnDi7ISMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Retain alphabetic words: alpha_only\n",
        "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
        "\n",
        "# Remove all stop words: no_stops\n",
        "no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
        "\n",
        "# Instantiate the WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize all tokens into a new list: lemmatized\n",
        "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
        "\n",
        "# Create the bag-of-words: bow\n",
        "bow = Counter(lemmatized)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "japCKSyzKs4X",
        "outputId": "52bd1b33-ef91-42f2-f0db-cbf35529c791"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('debugging', 39), ('system', 25), ('bug', 17), ('software', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('debugger', 13)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gensim**\n",
        "\n",
        "* *Gensim dictionary* เป็นการเอาคำที่ผ่านการ tokenize มาแล้วมาใส่ id \n",
        "   * **`.token2id()`** เป็นการดูทั้ง tokens และ id\n",
        "   * **`.get(token_id)`** จะได้คำออกมา\n",
        "   * **`.token2id().get(token)`** จะได้ id ออกมา\n",
        "   * **`.doc2bow(เอกสาร)`** สร้าง corpus"
      ],
      "metadata": {
        "id": "lY3xx3ypO8LN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora.dictionary import Dictionary\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "my_documents = ['The movie was about a spaceship and aliens.', 'I really liked the movie!', 'Awesome action scenes, but boring characters.', 'The movie was awful! I hate alien films.', 'Space is cool! I liked the movie.', 'More space films, please!',]\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
        "print(tokenized_docs, '\\n')\n",
        "dictionary = Dictionary(tokenized_docs)\n",
        "dictionary.token2id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G8blvn5Ma86",
        "outputId": "be981146-2276-4ab6-a036-47a0942a708c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['the', 'movie', 'was', 'about', 'a', 'spaceship', 'and', 'aliens', '.'], ['i', 'really', 'liked', 'the', 'movie', '!'], ['awesome', 'action', 'scenes', ',', 'but', 'boring', 'characters', '.'], ['the', 'movie', 'was', 'awful', '!', 'i', 'hate', 'alien', 'films', '.'], ['space', 'is', 'cool', '!', 'i', 'liked', 'the', 'movie', '.'], ['more', 'space', 'films', ',', 'please', '!']] \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': 0,\n",
              " 'a': 1,\n",
              " 'about': 2,\n",
              " 'aliens': 3,\n",
              " 'and': 4,\n",
              " 'movie': 5,\n",
              " 'spaceship': 6,\n",
              " 'the': 7,\n",
              " 'was': 8,\n",
              " '!': 9,\n",
              " 'i': 10,\n",
              " 'liked': 11,\n",
              " 'really': 12,\n",
              " ',': 13,\n",
              " 'action': 14,\n",
              " 'awesome': 15,\n",
              " 'boring': 16,\n",
              " 'but': 17,\n",
              " 'characters': 18,\n",
              " 'scenes': 19,\n",
              " 'alien': 20,\n",
              " 'awful': 21,\n",
              " 'films': 22,\n",
              " 'hate': 23,\n",
              " 'cool': 24,\n",
              " 'is': 25,\n",
              " 'space': 26,\n",
              " 'more': 27,\n",
              " 'please': 28}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gensim corpus**\n",
        "\n",
        "* A corpus is a set of texts to help perform NLP.\n",
        "* ตัวเลขตัวแรก คือ *token id* ตัวเลขตัวที่สองคือ *token frequency* ในเอกสารนั้นๆ"
      ],
      "metadata": {
        "id": "_1FtxxJ2pfnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "print(dictionary.doc2bow(tokenized_docs[1]), '\\n')\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWfxAOc1QbAS",
        "outputId": "079ce4bc-bbde-4fd3-d3f3-6ebb3da9d196"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (12, 1)] \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
              " [(5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
              " [(0, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)],\n",
              " [(0, 1),\n",
              "  (5, 1),\n",
              "  (7, 1),\n",
              "  (8, 1),\n",
              "  (9, 1),\n",
              "  (10, 1),\n",
              "  (20, 1),\n",
              "  (21, 1),\n",
              "  (22, 1),\n",
              "  (23, 1)],\n",
              " [(0, 1), (5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (24, 1), (25, 1), (26, 1)],\n",
              " [(9, 1), (13, 1), (22, 1), (26, 1), (27, 1), (28, 1)]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Creating and querying a corpus with gensim***\n",
        "\n",
        "It's time to apply the methods you learned in the previous video to create your first gensim dictionary and corpus!\n",
        "\n",
        "You'll use these data structures to investigate word trends and potential interesting topics in your document set. To get started, we have imported a few additional messy articles from Wikipedia, which were preprocessed by lowercasing all words, tokenizing them, and removing stop words and punctuation. These were then stored in a list of document tokens called `articles`. You'll need to do some light preprocessing and then generate the `gensim` dictionary and corpus.\n",
        "\n",
        "\n",
        "* Initialize a `gensim` `Dictionary` with the tokens in `articles`.\n",
        "\n",
        "* Obtain the id for `\"computer\"` from dictionary. To do this, use its **`.token2id`** method which returns ids from text, and then chain **`.get()`** which returns tokens from ids. Pass in `\"computer\"` as an argument to `.get()`.\n",
        "\n",
        "* Use a list comprehension in which you iterate over `articles` to create a `gensim` `MmCorpus` from `dictionary`.\n",
        "\n",
        "   * In the output expression, use the `.doc2bow()` method on `dictionary` with `article` as the argument.\n",
        "\n",
        "* Print the first 10 word ids with their frequency counts from the fifth document. "
      ],
      "metadata": {
        "id": "jLEYzF0bQMfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "articles = []\n",
        "for article in ['wiki_text_bug.txt', 'wiki_text_computer.txt', 'wiki_text_crash.txt', 'wiki_text_debugger.txt', 'wiki_text_debugging.txt', 'wiki_text_exception.txt', 'wiki_text_hopper.txt', 'wiki_text_language.txt', 'wiki_text_malware.txt', 'wiki_text_program.txt', 'wiki_text_reversing.txt', 'wiki_text_software.txt']:\n",
        "    with open(article) as f:\n",
        "        tokens = word_tokenize(f.read())\n",
        "        lower_tokens = [t.lower() for t in tokens]\n",
        "        alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
        "        no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
        "        no_punc = [t for t in no_stops if t not in punctuation]\n",
        "        wordnet_lemmatizer = WordNetLemmatizer()\n",
        "        lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_punc]\n",
        "        articles.append(lemmatized)\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(articles)\n",
        "\n",
        "# Select the id for \"computer\": computer_id\n",
        "computer_id = dictionary.token2id.get('computer')\n",
        "\n",
        "# Use computer_id with the dictionary to print the word\n",
        "print(dictionary.get(computer_id), '\\n')\n",
        "\n",
        "# Create a MmCorpus: corpus\n",
        "corpus = [dictionary.doc2bow(article) for article in articles]\n",
        "\n",
        "# Print the first 10 word ids with their frequency counts from the fifth document\n",
        "print(corpus[4][:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdH376woP-jR",
        "outputId": "1100db53-f23a-4c92-bc7a-3fb92f40bb2d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "computer \n",
            "\n",
            "[(1, 1), (13, 1), (14, 1), (17, 1), (24, 1), (27, 1), (33, 1), (34, 4), (42, 2), (43, 7)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Gensim bag-of-words***\n",
        "\n",
        "Now, you'll use your new `gensim` corpus and dictionary to see the most common terms per document and across all documents. You can use your dictionary to look up the terms. Take a guess at what the topics are.\n",
        "\n",
        "You have access to the `dictionary` and `corpus` objects you created in the previous exercise, as well as the Python `defaultdict` and `itertools` to help with the creation of intermediate data structures for analysis.\n",
        "\n",
        "   * `defaultdict` allows us to initialize a dictionary that will assign a default value to non-existent keys. By supplying the argument `int`, we are able to ensure that any non-existent keys are automatically assigned a default value of `0`. This makes it ideal for storing the counts of words in this exercise.\n",
        "\n",
        "   * `itertools.chain.from_iterable()` allows us to iterate through a set of sequences as if they were one continuous sequence. Using this function, we can easily iterate through our `corpus` object (which is a list of lists).\n",
        "\n",
        "The fifth document from `corpus` is stored in the variable `doc`, which has been sorted in ascending order.\n",
        "\n",
        "\n",
        "* Using the first for loop, print the top five words of `bow_doc` using each `word_id` with the dictionary alongside `word_count`.\n",
        "\n",
        "   * The `word_id` can be accessed using the `.get()` method of `dictionary`.\n",
        "\n",
        "* Create a `defaultdict` called `total_word_count` in which the keys are all the token ids (`word_id`) and the values are the sum of their occurrence across all documents (`word_count`).\n",
        "\n",
        "   * Remember to specify `int` when creating the `defaultdict`, and inside the second `for` loop, increment each `word_id` of `total_word_count` by `word_count`.\n",
        "\n",
        "* Create a sorted list from the `defaultdict`, using words across the entire corpus. To achieve this, use the `.items()` method on `total_word_count` inside `sorted()`.\n",
        "\n",
        "* Similar to how you printed the top five words of `bow_doc` earlier, print the top five words of `sorted_word_count` as well as the number of occurrences of each word across all the documents."
      ],
      "metadata": {
        "id": "mFDU-hNqZQ3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fifth document: doc\n",
        "doc = corpus[4]\n",
        "\n",
        "# Sort the doc for frequency: bow_doc\n",
        "# เรียงด้วยตัวเลขตัวที่สอง จึงใช้ w[1]\n",
        "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 words of the document alongside the count\n",
        "# corpus ตัวแรกเป็น id อยากรู้ว่าเป็นคำว่าอะไร ก็ต้องโยงกลับไปที่ dictionary ที่สร้างมันมา\n",
        "for word_id, word_count in bow_doc[:5]:\n",
        "    print(dictionary.get(word_id), word_count)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Create the defaultdict: total_word_count\n",
        "total_word_count = defaultdict(int)\n",
        "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
        "    total_word_count[word_id] += word_count\n",
        "    \n",
        "# Create a sorted list from the defaultdict: sorted_word_count\n",
        "# ใช้ .items() เพื่อให้ sorted ได้\n",
        "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
        "\n",
        "# Print the top 5 words across all documents alongside the count\n",
        "print(\"Sorted word counts\\n\")\n",
        "for i, j in sorted_word_count[:5]:\n",
        "    print(dictionary.get(i), j)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFoeKuo6aK8V",
        "outputId": "fc43c5fc-a081-4344-dd4f-f10c212833c8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "debugging 39\n",
            "system 25\n",
            "bug 17\n",
            "software 16\n",
            "problem 15\n",
            "\n",
            "\n",
            "Sorted word counts\n",
            "\n",
            "computer 753\n",
            "software 451\n",
            "program 341\n",
            "cite 322\n",
            "language 320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tf-idf with gensim**\n",
        "\n",
        "## ***What is tf-idf?***\n",
        "\n",
        "You want to calculate the tf-idf weight for the word `\"computer\"`, which appears 5 times in a document containing 100 words. Given a corpus containing 200 documents, with 20 documents mentioning the word `\"computer\"`, tf-idf can be calculated by multiplying term frequency with inverse document frequency.\n",
        "\n",
        "Term frequency = percentage share of the word compared to all tokens in the document Inverse document frequency = logarithm of the total number of documents in a corpora divided by the number of documents containing the term\n",
        "\n",
        "$$\\frac{5}{100} \\times \\log\\left(\\frac{200}{20}\\right)$$\n",
        "\n",
        "## ***Tf-idf with Wikipedia***\n",
        "\n",
        "* Initialize a new **`TfidfModel`** called `tfidf` using `corpus`.\n",
        "* Print the first five term ids with weights.\n",
        "* Sort the term ids and weights in a new list from highest to lowest weight. \n",
        "* Using your pre-existing dictionary, print the top five weighted words (`term_id`) from `sorted_tfidf_weights`, along with their weighted score (`weight`)."
      ],
      "metadata": {
        "id": "Y0fqWPBvtUVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.tfidfmodel import TfidfModel\n",
        "\n",
        "tfidf = TfidfModel(corpus)\n",
        "print(tfidf[corpus[4]][:5], \"\\n\")\n",
        "\n",
        "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
        "sorted_tfidf_weights = sorted(tfidf[corpus[4]], key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 weighted words\n",
        "for term_id, weight in sorted_tfidf_weights[:5]:\n",
        "    print(dictionary.get(term_id), weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhLt-lUMtZ52",
        "outputId": "63d4b492-e9df-4694-b6ce-dce9eb204eee"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, 0.012844137985779271), (13, 0.012844137985779271), (14, 0.012844137985779271), (17, 0.012844137985779271), (24, 0.02035747706154831)] \n",
            "\n",
            "wolf 0.23022876516553425\n",
            "debugging 0.20790115749039909\n",
            "fence 0.1841830121324274\n",
            "squeeze 0.13813725909932056\n",
            "tron 0.13813725909932056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Named Entity Recognition**\n",
        "\n",
        "## ***NER with NLTK***\n",
        "\n",
        "You're now going to have some fun with named-entity recognition! A scraped news article has been pre-loaded into your workspace. Your task is to use **`nltk`** to find the named entities in this article.\n",
        "\n",
        "What might the article be about, given the names you found?\n",
        "\n",
        "Along with **`nltk`**, **`sent_tokenize`** and **`word_tokenize`** from **`nltk.tokenize`** have been pre-imported.\n",
        "\n",
        "\n",
        "* Tokenize `article` into sentences.\n",
        "* Tokenize each sentence in `sentences` into words using a list comprehension.\n",
        "Inside a list comprehension, tag each tokenized sentence into parts of speech using `nltk.pos_tag()`.\n",
        "\n",
        "* Chunk each tagged sentence into named-entity chunks using **`nltk.ne_chunk_sents()`. Along with `pos_sentences`, specify the additional keyword argument `binary=True`.\n",
        "\n",
        "* Loop over each sentence and each chunk, and test whether it is a named-entity chunk by testing if it has the attribute `label`, and if the `chunk.label()` is equal to `\"NE\"`. If so, print that chunk."
      ],
      "metadata": {
        "id": "FPoul_kUwz3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('uber_apple.txt') as f:\n",
        "    article = f.read()\n",
        "\n",
        "    # Tokenize the article into sentences: sentences\n",
        "    sentences = sent_tokenize(article)\n",
        "\n",
        "    # Tokenize each sentence into words: token_sentences\n",
        "    token_sentences = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "    # Tag each tokenized sentence into parts of speech: pos_sentences\n",
        "    pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
        "\n",
        "    # Create the named entity chunks: chunked_sentences\n",
        "    chunked_sentences = nltk.ne_chunk_sents(pos_sentences)#, binary=True)\n",
        "\n",
        "    # Test for stems of the tree with 'NE' tags\n",
        "    for sent in chunked_sentences:\n",
        "        for chunk in sent:\n",
        "            if hasattr(chunk, \"label\"):\n",
        "                print(chunk)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRmPuT3vvxz2",
        "outputId": "5bc825ca-c14e-4b05-c7f0-08ced18c186d"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(GPE Beyond/NN)\n",
            "(PERSON Apple/NNP)\n",
            "(PERSON Travis/NNP Kalanick/NNP)\n",
            "(PERSON Tim/NNP Cook/NNP)\n",
            "(PERSON Apple/NNP)\n",
            "(PERSON Silicon/NNP)\n",
            "(PERSON Valley/NNP)\n",
            "(ORGANIZATION CEO/NNP)\n",
            "(GPE Yahoo/NNP)\n",
            "(PERSON Marissa/NNP Mayer/NNP)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Charting practice***\n",
        "\n",
        "In this exercise, you'll use some extracted named entities and their groupings from a series of newspaper articles to chart the diversity of named entity types in the articles.\n",
        "\n",
        "You'll use a **`defaultdict`** called `ner_categories`, with keys representing every named entity group type, and values to count the number of each different named entity type. You have a chunked sentence list called `chunked_sentences` similar to the last exercise, but this time with *non-binary* category names.\n",
        "\n",
        "You can use **`hasattr()`** to determine if each chunk has a `'label'` and then simply use the chunk's **.label()** method as the dictionary key.\n",
        "\n",
        "* Create a **`defaultdict`** called `ner_categories`, with the default type set to `int`.\n",
        "\n",
        "* Fill up the dictionary with values for each of the keys. Remember, the keys will represent the `label()`.\n",
        "\n",
        "   * In the outer for loop, iterate over `chunked_sentences`, using `sent` as your iterator variable.\n",
        "\n",
        "   * In the inner for loop, iterate over `sent`. If the condition is `true`, increment the value of each key by `1`.\n",
        "\n",
        "   * *Remember to use the chunk's `.label()` method as the key!*\n",
        "\n",
        "* For the pie chart labels, create a list called `labels` from the keys of `ner_categories`, which can be accessed using `.keys()`.\n",
        "\n",
        "* Use a list comprehension to create a list called values, using the **`.get()`** method on `ner_categories` to compute the values of each label `v`.\n",
        "\n",
        "* Use `plt.pie()` to create a pie chart for each of the NER categories. Along with values and `labels=labels`, pass the extra keyword arguments `autopct='%1.1f%%'` and `startangle=140` to add percentages to the chart and rotate the initial start angle. \n",
        "* Display your pie chart. Was the distribution what you expected?"
      ],
      "metadata": {
        "id": "0k9G9GbW1FiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('uber_apple.txt') as f:\n",
        "    article = f.read()\n",
        "    sentences = sent_tokenize(article)\n",
        "    token_sentences = [word_tokenize(sent) for sent in sentences]\n",
        "    pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
        "    chunked_sentences = nltk.ne_chunk_sents(pos_sentences)\n",
        "    chunked_sentences = list(chunked_sentences)\n",
        "\n",
        "# Create the defaultdict: ner_categories\n",
        "ner_categories = defaultdict(int)\n",
        "\n",
        "# Create the nested for loop\n",
        "for sent in chunked_sentences:\n",
        "    for chunk in sent:\n",
        "        if hasattr(chunk, 'label'):\n",
        "            ner_categories[chunk.label()] += 1\n",
        "            \n",
        "# Create a list from the dictionary keys for the chart labels: labels\n",
        "labels = list(ner_categories.keys())\n",
        "\n",
        "# Create a list of the values: values\n",
        "values = [ner_categories.get(v) for v in labels]\n",
        "\n",
        "# Create the pie chart\n",
        "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "\n",
        "# Display the chart\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "cUyDrTek0qUM",
        "outputId": "f4c6c04d-e28a-4c52-df58-fad9e13c4f95"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Comparing NLTK with spaCy NER***\n",
        "\n",
        "Using the same text you used in the first exercise of this chapter, you'll now see the results using spaCy's NER annotator. How will they compare?\n",
        "\n",
        "The article has been pre-loaded as `article`. To minimize execution times, you'll be asked to specify the keyword argument `disable=['tagger', 'parser', 'matcher']` when loading the spaCy model, because you only care about the entity in this exercise.\n",
        "\n",
        "* Load the `'en_core_web_sm'` model using **`spacy.load()`**. Specify the additional keyword arguments `disable=['tagger', 'parser', 'matcher']`.\n",
        "\n",
        "* Create a spacy document object by passing article into `nlp()`.\n",
        "\n",
        "*  Using `ent` as your iterator variable, iterate over the entities of `doc` and print out the labels (`ent.label_`) and text (`ent.text`)."
      ],
      "metadata": {
        "id": "Q8oe1-dZToxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('uber_apple.txt') as f:\n",
        "    article = f.read()\n",
        "\n",
        "    # Instantiate the English model: nlp\n",
        "    nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'matcher'])\n",
        "\n",
        "    # Create a new document: doc\n",
        "    doc = nlp(article)\n",
        "\n",
        "    # Print all of the found entities and their labels\n",
        "    for ent in doc.ents:\n",
        "        print(ent.label_, ent.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaPGtad288Bh",
        "outputId": "c03c026e-ea73-4290-c9e3-bda246c1fe5f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORG Apple\n",
            "PERSON Travis Kalanick\n",
            "PERSON Tim Cook\n",
            "ORG Apple\n",
            "CARDINAL Millions\n",
            "LOC Silicon Valley’s\n",
            "ORG Yahoo, Marissa Mayer\n",
            "MONEY 186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multilingual NER with polyglot**\n",
        "\n",
        "* ใช้ระบุ entity ในภาษาอื่นๆ"
      ],
      "metadata": {
        "id": "URHHhEeZFbNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from polyglot.text import Text, Word\n",
        "from polyglot.downloader import downloader\n",
        "downloader.download(\"embeddings2.es\")\n",
        "downloader.download(\"ner2.es\")\n",
        "\n",
        "text = \"\"\"El presidente de la Generalitat de Cataluña, \n",
        "          Carles Puigdemont, ha afirmado hoy a la alcaldesa\n",
        "          de Madrid, Manuela Carmena, que en su etapa de\n",
        "          alcalde de Girona (de julio de 2011 a enero de 2016)\n",
        "          hizo una gran promoción de Madrid.\"\"\"\n",
        "ptext = Text(text)\n",
        "ptext.entities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp3XhK-oVkS6",
        "outputId": "fb79cc9a-5757-4387-9f84-bc056636b1ed"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[I-ORG(['Generalitat', 'de']),\n",
              " I-LOC(['Generalitat', 'de', 'Cataluña']),\n",
              " I-PER(['Carles', 'Puigdemont']),\n",
              " I-LOC(['Madrid']),\n",
              " I-PER(['Manuela', 'Carmena']),\n",
              " I-LOC(['Girona']),\n",
              " I-LOC(['Madrid'])]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***French NER with polyglot I***\n",
        "\n",
        "In this exercise and the next, you'll use the **`polyglot`** library to identify French entities. The library functions slightly differs from **`spacy`**, so you'll use a few of the new things you learned in the last video to display the named entity text and category.\n",
        "\n",
        "You have access to the full article string in `article`. Additionally, the **`Text`** class of polyglot has been imported from **`polyglot.text`**.\n",
        "\n",
        "* Using the article string in `article`, create a new **`Text`** object called `txt`.\n",
        "\n",
        "* Iterate over `txt.entities` and print each entity, `ent`.\n",
        "\n",
        "* Print the `type()` of `ent`. "
      ],
      "metadata": {
        "id": "JKv6u1fcIpla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloader.download('embeddings2.fr')\n",
        "# downloader.download('ner2.fr')\n",
        "\n",
        "with open('french.txt') as f:\n",
        "    article = f.read()\n",
        "\n",
        "# Create a new text object using Polyglot's Text class: txt\n",
        "txt = Text(article)\n",
        "\n",
        "# Print each of the entities found\n",
        "for ent in txt.entities:\n",
        "    print(ent)\n",
        "    \n",
        "# Print the type of ent\n",
        "print(type(ent))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4DDGpGWFuO1",
        "outputId": "5e028f7d-360d-4e32-f9c1-849a2425bfc4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Charles', 'Cuvelliez']\n",
            "['Charles', 'Cuvelliez']\n",
            "['Bruxelles']\n",
            "['l’IA']\n",
            "['Julien', 'Maldonato']\n",
            "['Deloitte']\n",
            "['Ethiquement']\n",
            "['l’IA']\n",
            "['.']\n",
            "<class 'polyglot.text.Chunk'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***French NER with polyglot II***\n",
        "\n",
        "Here, you'll complete the work you began in the previous exercise.\n",
        "\n",
        "Your task is to use a list comprehension to create a list of tuples, in which the first element is the entity tag, and the second element is the full string of the entity text.\n",
        "\n",
        "\n",
        "* Use a list comprehension to create a list of tuples called `entities`.\n",
        "\n",
        "* The output expression of your list comprehension should be a tuple.\n",
        "  * The first element of each tuple is the entity tag, which you can access using its **`.tag`** attribute.\n",
        "  * The second element is the full string of the entity text, which you can access using `.join(ent)`.\n",
        "\n",
        "* Your iterator variable should be `ent`, and you should iterate over all of the entities of the **`polyglot Text`** object, `txt`.\n",
        "* Print `entities` to see what you've created."
      ],
      "metadata": {
        "id": "mqXG7tAJKr7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the list of tuples: entities\n",
        "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
        "\n",
        "# Print entities\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fWOuNJ3L59t",
        "outputId": "1c7fb7ef-7c16-4a24-c77e-d789fd2ab46d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I-PER', 'Charles Cuvelliez'), ('I-PER', 'Charles Cuvelliez'), ('I-ORG', 'Bruxelles'), ('I-PER', 'l’IA'), ('I-PER', 'Julien Maldonato'), ('I-ORG', 'Deloitte'), ('I-PER', 'Ethiquement'), ('I-LOC', 'l’IA'), ('I-PER', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Spanish NER with polyglot***\n",
        "\n",
        "You'll continue your exploration of **`polyglot`** now with some Spanish annotation. This article is not written by a newspaper, so it is your first example of a more blog-like text. How do you think that might compare when finding entities?\n",
        "\n",
        "The **`Text`** object has been created as `txt`, and each entity has been printed.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "['Lina']\n",
        "['Castillo']\n",
        "['Teresa', 'Lozano', 'Long']\n",
        "['Universidad', 'de', 'Texas']\n",
        "['Austin']\n",
        "['Austin', '.']\n",
        "['Austin', '.', 'Ella']\n",
        "['Gabriel', 'García', 'Márquez']\n",
        "['Gabriel', 'García', 'Márquez']\n",
        "['LIna']\n",
        "['Castillo']\n",
        "['colombiano']\n",
        "['Colombia']\n",
        "['Estados', 'Unidos']\n",
        "['Castillo']\n",
        "['Nation']\n",
        "['Kenneth', 'Nebenzahl']\n",
        "['Jr']\n",
        "['Library']\n",
        "['Society']\n",
        "['Humboldt']\n",
        "['América', 'Latina']\n",
        "['.', 'García', 'Márquez']\n",
        "['colombiano']\n",
        "['Gabriel', 'García', 'Márquez']\n",
        "['Gabo']\n",
        "['Fidel', 'Castro']\n",
        "['Gabo']\n",
        "['Castro']\n",
        "['colombianos']\n",
        "['Gabo']\n",
        "['colombiano']\n",
        "['colombiano', 'Belisario', 'Betancur']\n",
        "['Betancur']\n",
        "['Fuerzas', 'Armadas', 'Revolucionarias', 'de', 'Colombia']\n",
        "['FARC']\n",
        "['Gabo']\n",
        "['Cuba']\n",
        "['Betancur']\n",
        "['Gabo']\n",
        "['Cuba']\n",
        "['.', 'Betancur']\n",
        "['Gabriel', 'García', 'Márquez']\n",
        "['Gabo']\n",
        "['San', 'Vicente', 'del', 'Caguán']\n",
        "['Andrés', 'Patrana']\n",
        "['FARC']\n",
        "['Gabo']\n",
        "['Pastrana']\n",
        "['Ejército', 'de', 'Liberación', 'Nacional']\n",
        "['ELN']\n",
        "['García', 'Márquez']\n",
        "['Mercedes', 'Barcha']\n",
        "['ELN']\n",
        "['La', 'Habana']\n",
        "['Gabo']\n",
        "['La', 'Habana']\n",
        "['Gabo']\n",
        "['Fidel', 'Castro']\n",
        "['Gabriel', 'García', 'Márquez']\n",
        "['Fidel', 'Castro']\n",
        "['Gabriel', 'García', 'Márquez']\n",
        "['Carmen', 'Balcells']\n",
        "['La', 'Habana']\n",
        "['Estados', 'Unidos']\n",
        "['Colombia']\n",
        "['.', 'Gabriel', 'García', 'Márquez']\n",
        "['Gabo']\n",
        "['Salman', 'Rushdie']\n",
        "['Elena', 'Poniatowska']\n",
        "['Gabo']\n",
        "['Gabo']\n",
        "['Gabriel', 'García', 'Márquez']\n",
        "['García', 'Márquez']\n",
        "['Macondo']\n",
        "['colombiana']\n",
        "['United', 'Fruit', 'Company']\n",
        "['colombiano']\n",
        "['Ciénaga']\n",
        "['Santa']\n",
        "['Santa', 'Marta']\n",
        "['García', 'Márquez']\n",
        "['York']\n",
        "['Español']\n",
        "['José', 'Arcadio', 'Segundo']\n",
        "['García', 'Márquez']\n",
        "['United', 'Fruit']\n",
        "['Colombia']\n",
        "['Macondo']\n",
        "['Gabo']\n",
        "['Gabo']\n",
        "['Ciénaga']\n",
        "['colombiana']\n",
        "['José', 'Arcadio', 'Segundo']\n",
        "['García', 'Márquez']\n",
        "['Gabriel', 'García', 'Márquez']\n",
        "['Harry', 'Ransom']\n",
        "['Harry', 'Ransom', 'Center']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Your specific task is to determine how many of the entities contain the words `\"Márquez\"` or `\"Gabo\"` - these refer to the same person in different ways!\n",
        "\n",
        "* Iterate over all of the entities of `txt`, using `ent` as your iterator variable.\n",
        "\n",
        "* Check whether the entity contains `\"Márquez\"` or `\"Gabo\"`. If it does, increment count. *Don't forget to include the accented `á` in `\"Márquez\"`!*\n"
      ],
      "metadata": {
        "id": "yyxkuFF7M2GU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloader.download('embeddings2.es')\n",
        "# downloader.download('ner2.es')\n",
        "\n",
        "s = '''Lina del Castillo es profesora en el Instituto de Estudios Latinoamericanos Teresa Lozano Long (LLILAS) y el Departamento de Historia de la Universidad de Texas en Austin. Ella será la moderadora del panel “Los Mundos Políticos de Gabriel García Márquez” este viernes, Oct. 30, en el simposio Gabriel García Márquez: Vida y Legado.\n",
        "\n",
        "\n",
        "LIna del Castillo\n",
        "\n",
        "\n",
        "Actualmente, sus investigaciones abarcan la intersección de cartografía, disputas a las demandas de tierra y recursos, y la formación del n...el tren de medianoche que lleva a miles y miles de cadáveres uno encima del otro como tantos racimos del banano que acabarán tirados al mar. Ningún recuento periodístico podría provocar nuestra imaginación y nuestra memoria como este relato de García Márquez.\n",
        "\n",
        "\n",
        "Contenido Relacionado\n",
        "\n",
        "\n",
        "Lea más artículos sobre el archivo de Gabriel García Márquez\n",
        "\n",
        "\n",
        "Reciba mensualmente las últimas noticias e información del Harry Ransom Center con eNews, nuestro correo electrónico mensual. ¡Suscríbase hoy!'''\n",
        "\n",
        "txt = Text(s)\n",
        "\n",
        "# Initialize the count variable: count\n",
        "count = 0\n",
        "\n",
        "# Iterate over all the entities\n",
        "for ent in txt.entities:\n",
        "    # Check whether the entity contains 'Márquez' or 'Gabo'\n",
        "    if ('Márquez' in ent) | ('Gabo' in ent):\n",
        "        # Increment count\n",
        "        count += 1\n",
        "\n",
        "# Print count\n",
        "print(count)\n",
        "\n",
        "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
        "percentage = count / len(txt.entities)\n",
        "print(percentage)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXAXiyC-L_Zw",
        "outputId": "917a5d0a-f469-4ce5-a910-ddf343cb9525"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "0.26666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building word count vectors with scikit-learn**\n",
        "\n",
        "มีข้อควรระวังต้องทำ train test split ตรงที่ถ้าข้อมูลใน train มีน้อยไป แล้วมีคำใน test ที่ไม่ปรากฎใน train จะเกิด error ได้ ต้องเพิ่มขนาดของ train หรือเอาคำที่มีปัญหาออกไปจาก test\n",
        "\n",
        "## ***CountVectorizer for text classification***\n",
        "\n",
        "It's time to begin building your text classifier! The data has been loaded into a DataFrame called `df`. Explore it in the IPython Shell to investigate what columns you can use. The `.head()` method is particularly informative.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "                                               title                                               text label\n",
        "0                       You Can Smell Hillary’s Fear  Daniel Greenfield, a Shillman Journalism Fello...  FAKE\n",
        "1  Watch The Exact Moment Paul Ryan Committed Pol...  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE\n",
        "2        Kerry to go to Paris in gesture of sympathy  U.S. Secretary of State John F. Kerry said Mon...  REAL\n",
        "3  Bernie supporters on Twitter erupt in anger ag...  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE\n",
        "4   The Battle of New York: Why This Primary Matters  It's primary day in New York and front-runners...  REAL\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "In this exercise, you'll use `pandas` alongside `scikit-learn` to create a sparse text vectorizer you can use to train and test a simple supervised model. To begin, you'll set up a **`CountVectorizer`** and investigate some of its features.\n",
        "\n",
        "* Create a Series `y` to use for the labels by assigning the **`.label`** attribute of `df` to `y`.\n",
        "\n",
        "* Using `df[\"text\"]` (features) and `y` (labels), create training and test sets using **`train_test_split()`**. Use a `test_size` of `0.33` and a `random_state` of `53`.\n",
        "\n",
        "* Create a **`CountVectorizer`** object called `count_vectorizer`. Ensure you specify the keyword argument `stop_words=\"english\"` so that stop words are removed.\n",
        "\n",
        "* Fit and transform the training data `X_train` using the **`.fit_transform()`** method of your **`CountVectorizer`** object. Do the same with the test data `X_test`, except using the **`.transform()`** method.\n",
        "\n",
        "* Print the first 10 features of the `count_vectorizer` using its **`.get_feature_names()`** method."
      ],
      "metadata": {
        "id": "C-mCzWoyXFQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create a series to store the labels: y\n",
        "y = df['label']\n",
        "\n",
        "# Create training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=53)\n",
        "\n",
        "# Initialize a CountVectorizer object: count_vectorizer\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Transform the training data using only the 'text' column values: count_train \n",
        "count_train = count_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using only the 'text' column values: count_test \n",
        "count_test = count_vectorizer.transform(X_test)\n",
        "\n",
        "# Print the first 10 features of the count_vectorizer\n",
        "print(count_vectorizer.get_feature_names()[:10])"
      ],
      "metadata": {
        "id": "hm9fmgTSbrkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "A6lyfB5Tbuqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***TfidfVectorizer for text classification***\n",
        "\n",
        "Similar to the sparse **`CountVectorizer`** created in the previous exercise, you'll work on creating tf-idf vectors for your documents. You'll set up a `TfidfVectorizer` and investigate some of its features.\n",
        "\n",
        "In this exercise, you'll use `pandas` and `sklearn` along with the same `X_train`, `y_train` and `X_test`, `y_test` DataFrames and Series you created in the last exercise.\n",
        "\n",
        "* Create a **`TfidfVectorizer`** object called `tfidf_vectorizer`. When doing so, specify the keyword arguments `stop_words=\"english\"` and `max_df=0.7`.\n",
        "\n",
        "* Fit and transform the training data.\n",
        "* Transform the test data.\n",
        "* Print the first 10 features of `tfidf_vectorizer`.\n",
        "* Print the first 5 vectors of the tfidf training data using slicing on the **`.A`** (or array) attribute of `tfidf_train`."
      ],
      "metadata": {
        "id": "IC24yA1ycRzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
        "\n",
        "# Transform the training data: tfidf_train \n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data: tfidf_test \n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Print the first 10 features\n",
        "print(tfidf_vectorizer.get_feature_names()[:10])\n",
        "\n",
        "# Print the first 5 vectors of the tfidf training data\n",
        "print(tfidf_train.A[:5])\n"
      ],
      "metadata": {
        "id": "SI_sa_WcgAmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "['00', '000', '001', '008s', '00am', '00pm', '01', '01am', '02', '024'] \n",
        "    \n",
        "    \n",
        "    [[0.         0.01928563 0.         ... 0.         0.         0.        ]\n",
        "     [0.         0.         0.         ... 0.         0.         0.        ]\n",
        "     [0.         0.02895055 0.         ... 0.         0.         0.        ]\n",
        "     [0.         0.03056734 0.         ... 0.         0.         0.        ]\n",
        "     [0.         0.         0.         ... 0.         0.         0.        ]]\n",
        "```\n",
        "## ***Inspecting the vectors***\n",
        "\n",
        "To get a better idea of how the vectors work, you'll investigate them by converting them into `pandas` DataFrames.\n",
        "\n",
        "Here, you'll use the same data structures you created in the previous two exercises (`count_train`, `count_vectorizer`, `tfidf_train`, `tfidf_vectorizer`) as well as `pandas`, which is imported as `pd`.\n",
        "\n",
        "* Create the DataFrames `count_df` and `tfidf_df` by using **`pd.DataFrame()`** and specifying the values as the first argument and the columns (or features) as the second argument.\n",
        "  * The values can be accessed by using the **`.A`** attribute of, respectively, `count_train` and `tfidf_train`.\n",
        "\n",
        "  * The columns can be accessed using the **`.get_feature_names()`** methods of `count_vectorizer` and `tfidf_vectorizer`.\n",
        "\n",
        "* Print the head of each DataFrame to investigate their structure. \n",
        "\n",
        "* Test if the column names are the same for each DataFrame by creating a new object called difference to see the difference between the columns that `count_df` has from `tfidf_df`. Columns can be accessed using the **`.columns`** attribute of a DataFrame. Subtract the set of `tfidf_df.columns` from the set of `count_df.columns`.\n",
        "\n",
        "* Test if the two DataFrames are equivalent by using the **`.equals()`** method on `count_df` with `tfidf_df` as the argument.\n"
      ],
      "metadata": {
        "id": "4sbQWJPPeBDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the CountVectorizer DataFrame: count_df\n",
        "count_df = pd.DataFrame(data=count_train.A, columns=count_vectorizer.get_feature_names())\n",
        "\n",
        "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
        "tfidf_df = pd.DataFrame(data=tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
        "\n",
        "# Print the head of count_df\n",
        "print(count_df.head())\n",
        "\n",
        "# Print the head of tfidf_df\n",
        "print(tfidf_df.head())\n",
        "\n",
        "# Calculate the difference in columns: difference\n",
        "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
        "print(difference)\n",
        "\n",
        "# Check whether the DataFrames are equal\n",
        "print(count_df.equals(tfidf_df))\n"
      ],
      "metadata": {
        "id": "esTTjpdSb7RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "       000  00am  0600  10  100  ...  yuan  zawahiri  zeitung  zero  zerohedge\n",
        "    0    0     0     0   0    0  ...     0         0        0     1          0\n",
        "    1    0     0     0   3    0  ...     0         0        0     0          0\n",
        "    2    0     0     0   0    0  ...     0         0        0     0          0\n",
        "    3    0     0     0   0    0  ...     0         0        0     0          0\n",
        "    4    0     0     0   0    0  ...     0         0        0     0          0\n",
        "    \n",
        "    [5 rows x 5111 columns]\n",
        "       000  00am  0600     10  100  ...  yuan  zawahiri  zeitung   zero  zerohedge\n",
        "    0  0.0   0.0   0.0  0.000  0.0  ...   0.0       0.0      0.0  0.034        0.0\n",
        "    1  0.0   0.0   0.0  0.106  0.0  ...   0.0       0.0      0.0  0.000        0.0\n",
        "    2  0.0   0.0   0.0  0.000  0.0  ...   0.0       0.0      0.0  0.000        0.0\n",
        "    3  0.0   0.0   0.0  0.000  0.0  ...   0.0       0.0      0.0  0.000        0.0\n",
        "    4  0.0   0.0   0.0  0.000  0.0  ...   0.0       0.0      0.0  0.000        0.0\n",
        "    \n",
        "    [5 rows x 5111 columns]\n",
        "    set()\n",
        "    False\n",
        "```\n",
        "\n",
        "# **Training and testing a classification model with scikit-learn**\n",
        "\n",
        "## ***Training and testing the \"fake news\" model with CountVectorizer***\n",
        "\n",
        "Now it's your turn to train the \"fake news\" model using the features you identified and extracted. In this first exercise you'll train and test a Naive Bayes model using the **`CountVectorizer`** data.\n",
        "\n",
        "The training and test sets have been created, and `count_vectorizer`, `count_train`, and `count_test` have been computed.\n",
        "\n",
        "* Instantiate a **`MultinomialNB`** classifier called `nb_classifier`.\n",
        "\n",
        "* Fit the classifier to the training data.\n",
        "\n",
        "* Compute the predicted tags for the test data.\n",
        "\n",
        "* Calculate and print the accuracy score of the classifier.\n",
        "\n",
        "* Compute the confusion matrix. To make it easier to read, specify the keyword argument `labels=['FAKE', 'REAL']`."
      ],
      "metadata": {
        "id": "FQ3nmQSLgSET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "nb_classifier.fit(count_train, y_train)\n",
        "\n",
        "# Create the predicted tags: pred\n",
        "pred = nb_classifier.predict(count_test)\n",
        "\n",
        "# Calculate the accuracy score: score\n",
        "score = accuracy_score(y_test, pred)\n",
        "print(score)\n",
        "\n",
        "# Calculate the confusion matrix: cm\n",
        "cm = confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "h3eTAd3qghfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "    0.893352462936394\n",
        "    [[ 865  143]\n",
        "     [  80 1003]]\n",
        "```\n",
        "\n",
        "## ***Training and testing the \"fake news\" model with TfidfVectorizer***\n",
        "\n",
        "Now that you have evaluated the model using the **`CountVectorizer`**, you'll do the same using the **`TfidfVectorizer`** with a Naive Bayes model.\n",
        "\n",
        "The training and test sets have been created, and `tfidf_vectorizer`, `tfidf_train`, and `tfidf_test` have been computed. Additionally, **`MultinomialNB`** and **`metrics`** have been imported from, respectively, **`sklearn.naive_bayes`** and **`sklearn`**.\n",
        "\n",
        "* Instantiate a **`MultinomialNB`** classifier called `nb_classifier`.\n",
        "* Fit the classifier to the training data.\n",
        "* Compute the predicted tags for the test data.\n",
        "* Calculate and print the accuracy score of the classifier.\n",
        "* Compute the confusion matrix. As in the previous exercise, specify the keyword argument `labels=['FAKE', 'REAL']` so that the resulting confusion matrix is easier to read.\n",
        "\n"
      ],
      "metadata": {
        "id": "4m2pqWERQ4DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "nb_classifier.fit(tfidf_train, y_train)\n",
        "\n",
        "# Create the predicted tags: pred\n",
        "pred = nb_classifier.predict(tfidf_test)\n",
        "\n",
        "# Calculate the accuracy score: score\n",
        "score = accuracy_score(y_test, pred)\n",
        "print(score)\n",
        "\n",
        "# Calculate the confusion matrix: cm\n",
        "cm = confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "O4PD5Ac1RixP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "    0.8565279770444764\n",
        "    [[ 739  269]\n",
        "     [  31 1052]]\n",
        "```\n",
        "# **Simple NLP, complex problems**\n",
        "\n",
        "## ***Improving your model***\n",
        "\n",
        "Your job in this exercise is to test a few different alpha levels using the **`Tfidf`** vectors to determine if there is a better performing combination.\n",
        "\n",
        "The training and test sets have been created, and `tfidf_vectorizer`, `tfidf_train`, and `tfidf_test` have been computed.\n",
        "\n",
        "* Create a list of alphas to try using **`np.arange()`**. Values should range from `0` to `1` with steps of `0.1`.\n",
        "\n",
        "* Create a function `train_and_predict()` that takes in one argument: `alpha`. The function should:\n",
        "  * Instantiate a **`MultinomialNB`** classifier with `alpha=alpha`.\n",
        "  * Fit it to the training data.\n",
        "  * Compute predictions on the test data.\n",
        "  * Compute and return the accuracy score.\n",
        "\n",
        "* Using a `for` loop, print the `alpha`, `score` and a newline in between. Use your `train_and_predict()` function to compute the score. Does the score change along with the alpha? What is the best alpha?\n"
      ],
      "metadata": {
        "id": "P96_I0r9SfQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the list of alphas: alphas\n",
        "alphas = np.arange(0, 1, 0.1)\n",
        "\n",
        "# Define train_and_predict()\n",
        "def train_and_predict(alpha):\n",
        "    # Instantiate the classifier: nb_classifier\n",
        "    nb_classifier = MultinomialNB(alpha=alpha)\n",
        "    # Fit to the training data\n",
        "    nb_classifier.fit(tfidf_train, y_train)\n",
        "    # Predict the labels: pred\n",
        "    pred = nb_classifier.predict(tfidf_test)\n",
        "    # Compute accuracy: score\n",
        "    score = metrics.accuracy_score(y_test, pred)\n",
        "    return score\n",
        "\n",
        "# Iterate over the alphas and print the corresponding score\n",
        "for alpha in alphas:\n",
        "    print('Alpha: ', alpha)\n",
        "    print('Score: ', train_and_predict(alpha))\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "IS3sAKMVSo4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "    Alpha:  0.0\n",
        "    Score:  0.8813964610234337\n",
        "    \n",
        "    Alpha:  0.1\n",
        "    Score:  0.8976566236250598\n",
        "    \n",
        "    Alpha:  0.2\n",
        "    Score:  0.8938307030129125\n",
        "    \n",
        "    Alpha:  0.30000000000000004\n",
        "    Score:  0.8900047824007652\n",
        "    \n",
        "    Alpha:  0.4\n",
        "    Score:  0.8857006217120995\n",
        "    \n",
        "    Alpha:  0.5\n",
        "    Score:  0.8842659014825442\n",
        "    \n",
        "    Alpha:  0.6000000000000001\n",
        "    Score:  0.874701099952176\n",
        "    \n",
        "    Alpha:  0.7000000000000001\n",
        "    Score:  0.8703969392635102\n",
        "    \n",
        "    Alpha:  0.8\n",
        "    Score:  0.8660927785748446\n",
        "    \n",
        "    Alpha:  0.9\n",
        "    Score:  0.8589191774270684\n",
        "```\n",
        "\n",
        "## ***Inspecting your model***\n",
        "\n",
        "Now that you have built a \"fake news\" classifier, you'll investigate what it has learned. You can map the important vector weights back to actual words using some simple inspection techniques.\n",
        "\n",
        "You have your well performing tfidf Naive Bayes classifier available as `nb_classifier`, and the vectors as `tfidf_vectorizer`.\n",
        "\n",
        "* Save the class labels as `class_labels` by accessing the **`.classes_`** attribute of `nb_classifier`.\n",
        "\n",
        "* Extract the features using the **`.get_feature_names()`** method of `tfidf_vectorizer`.\n",
        "\n",
        "* Create a zipped array of the classifier coefficients with the feature names and sort them by the coefficients. To do this, first use **`zip()`** with the arguments `nb_classifier.coef_[0]` and `feature_names`. Then, use **`sorted()`** on this.\n",
        "\n",
        "* Print the top 20 weighted features for the first label of `class_labels` and print the bottom 20 weighted features for the second label of `class_labels`."
      ],
      "metadata": {
        "id": "N6-CO5AuY4jD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the class labels: class_labels\n",
        "class_labels = nb_classifier.classes_\n",
        "\n",
        "# Extract the features: feature_names\n",
        "feature_names = tfidf_vectorizer.get_feature_names()\n",
        "\n",
        "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
        "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
        "\n",
        "# Print the first class label and the top 20 feat_with_weights entries\n",
        "print(class_labels[0], feat_with_weights[:20])\n",
        "\n",
        "# Print the second class label and the bottom 20 feat_with_weights entries\n",
        "print(class_labels[1], feat_with_weights[-20:])"
      ],
      "metadata": {
        "id": "nueXCoHuZxWH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}