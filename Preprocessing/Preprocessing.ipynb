{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FXrdJJwvaBgk"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Missing data - rows***\n",
        "\n",
        "Taking a look at the volunteer dataset again, we want to drop rows where the `category_desc` column values are missing. We're going to do this using boolean indexing, by checking to see if we have any null values, and then filtering the dataset so that we only have rows with those values.\n",
        "\n",
        "* Check how many values are missing in the `category_desc` column using `isnull()` and `sum()`.\n",
        "\n",
        "* Subset the `volunteer` dataset by indexing by where `category_desc` is `notnull()`, and store in a new variable called `volunteer_subset`.\n",
        "\n",
        "* Take a look at the `.shape` attribute of the new dataset, to verify it worked correctly."
      ],
      "metadata": {
        "id": "b8fthexjkmpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "volunteer = pd.read_csv('volunteer_opportunities.csv')\n",
        "\n",
        "# Check how many values are missing in the category_desc column\n",
        "print(volunteer['category_desc'].isnull().sum())\n",
        "\n",
        "# Subset the volunteer dataset\n",
        "volunteer_subset = volunteer[volunteer['category_desc'].notnull()]\n",
        "\n",
        "# Print out the shape of the subset\n",
        "print(volunteer_subset.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1ivAazekDD2",
        "outputId": "353f471d-2e6c-4889-9849-5724f480a778"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48\n",
            "(617, 35)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Missing data - columns***\n",
        "\n",
        "We have a dataset comprised of volunteer information from New York City. The dataset has a number of features, but we want to get rid of features that have at least 3 missing values.\n",
        "\n",
        "How many features are in the original dataset, and how many features are in the set after columns with at least 3 missing values are removed?\n",
        "\n",
        "The dataset `volunteer` has been provided.\n",
        "Use the **`dropna()`** function to remove columns.\n",
        "You'll have to set both the **`axis=`** and **`thresh=`** parameters."
      ],
      "metadata": {
        "id": "4TbHRXjCmFH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(volunteer.shape[1], volunteer.dropna(axis=1, thresh=3).shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYB28Q-ykSPR",
        "outputId": "3ba7f83b-418d-4f6a-e8fb-a04e329acac3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Converting a column type***\n",
        "\n",
        "If you take a look at the volunteer dataset types, you'll see that the column hits is type `object`. But, if you actually look at the column, you'll see that it consists of integers. Let's convert that column to type `int`.\n",
        "\n",
        "* Use the **`.astype`** function to convert the column to type `int`.\n",
        "* Take a look at the `dtypes` of the dataset again, and notice that the column type has changed."
      ],
      "metadata": {
        "id": "YMe0pIQNn2G9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "volunteer[\"hits\"] = volunteer[\"hits\"].astype(str)\n",
        "\n",
        "# Print the head of the hits column\n",
        "print(volunteer[\"hits\"].head())\n",
        "print(volunteer[\"hits\"].dtype)\n",
        "# Convert the hits column to type int\n",
        "volunteer[\"hits\"] = volunteer[\"hits\"].astype(int)\n",
        "\n",
        "# Look at the dtypes of the dataset\n",
        "print(volunteer[\"hits\"].dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL2NdTwAmgVl",
        "outputId": "8ad0a05a-e40e-4e87-aefa-8eec895a995b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    737\n",
            "1     22\n",
            "2     62\n",
            "3     14\n",
            "4     31\n",
            "Name: hits, dtype: object\n",
            "object\n",
            "int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Class imbalance***\n",
        "\n",
        "In the volunteer dataset, we're thinking about trying to predict the `category_desc` variable using the other features in the dataset. First, though, we need to know what the class distribution (and imbalance) is for that label.\n",
        "\n",
        "Which descriptions occur less than `50` times in the volunteer dataset?\n",
        "\n",
        "The dataset volunteer has been provided.\n",
        "The column you want to check is `category_desc`.\n",
        "Use the **`.value_counts()`** method to check variable counts."
      ],
      "metadata": {
        "id": "SJlamhrfonZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "volunteer['category_desc'].value_counts()  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TEXTZvfoXxS",
        "outputId": "a77c9d1b-6fc1-4ef3-87ef-5222bbc568a6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Strengthening Communities    307\n",
              "Helping Neighbors in Need    119\n",
              "Education                     92\n",
              "Health                        52\n",
              "Environment                   32\n",
              "Emergency Preparedness        15\n",
              "Name: category_desc, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both Emergency Prepardness and Environment occur less than 50 times.\n",
        "\n",
        "### ***Stratified sampling***\n",
        "\n",
        "We know that the distribution of variables in the `category_desc` column in the volunteer dataset is uneven. If we wanted to train a model to try to predict `category_desc`, we would want to train the model on a sample of data that is representative of the entire dataset. Stratified sampling is a way to achieve this.\n",
        "\n",
        "* Create a `volunteer_X` dataset with all of the columns except `category_desc`.\n",
        "* Create a `volunteer_y` training labels dataset.\n",
        "* Split up the `volunteer_X` dataset using scikit-learn's **`train_test_split`** function and passing volunteer_y into the **`stratify=`** parameter.\n",
        "* Take a look at the `category_desc` value counts on the training labels."
      ],
      "metadata": {
        "id": "kVmU9vd0qWDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a data with all columns except category_desc\n",
        "volunteer_X = volunteer.drop('category_desc', axis=1)\n",
        "\n",
        "# Create a category_desc labels dataset\n",
        "volunteer_y = volunteer[['category_desc']]\n",
        "\n",
        "# Use stratified sampling to split up the dataset according to the volunteer_y dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y)\n",
        "\n",
        "# Print out the category_desc counts on the training y labels\n",
        "print(y_train['category_desc'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "zUjXho0pqT9g",
        "outputId": "6fcba088-cd44-43d1-8a37-5ccb130b9460"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-179d0cde90ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Use stratified sampling to split up the dataset according to the volunteer_y dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvolunteer_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolunteer_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvolunteer_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Print out the category_desc counts on the training y labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2439\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCVClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2441\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2443\u001b[0m     return list(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2021\u001b[0m         \"\"\"\n\u001b[0;32m-> 2022\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2023\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"object\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input contains NaN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "สงสัย scikit-learn version 0.19.1 มันให้ผ่าน"
      ],
      "metadata": {
        "id": "26JRVMYduDNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Log normalization in Python***\n",
        "\n",
        "Proline column in `wine` dataset has a large amount of variance. Let's log normalize it.\n",
        "\n",
        "Numpy has been imported as `np` in your workspace.\n",
        "\n",
        "* Print out the variance of the Proline column for reference.\n",
        "* Use the **`np.log()`** function on the Proline column to create a new, log-normalized column named `Proline_log`.\n",
        "* Print out the variance of the `Proline_log` column to see the difference."
      ],
      "metadata": {
        "id": "oEr70gZHBY4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wine = pd.read_csv('wine_types.csv')\n",
        "\n",
        "# Print out the variance of the Proline column\n",
        "print(wine['Proline'].var())\n",
        "\n",
        "# Apply the log normalization function to the Proline column\n",
        "wine['Proline_log'] = np.log(wine['Proline'])\n",
        "\n",
        "# Check the variance of the normalized Proline column\n",
        "print(wine['Proline_log'].var())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73LdoYM6tl40",
        "outputId": "91bf5a7d-9270-4f87-d370-6aee02b0e520"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99166.71735542428\n",
            "0.17231366191842018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***How to scale data***"
      ],
      "metadata": {
        "id": "JTsbFiKwC2OU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)"
      ],
      "metadata": {
        "id": "RlRrAtFMCDlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Scaling data - standardizing columns***\n",
        "\n",
        "`Ash`, `Alcalinity of ash`, and `Magnesium` columns in the `wine` dataset are all on different scales, let's standardize them in a way that allows for use in a linear model.\n",
        "\n",
        "* Import **`StandardScaler`** from **`sklearn.preprocessing`**.\n",
        "\n",
        "* Create the **`StandardScaler()`** method and store in a variable named `ss`.\n",
        "\n",
        "* Create a subset of the `wine` DataFrame of the `Ash`, `Alcalinity of ash`, and `Magnesium` columns, store in a variable named `wine_subset`.\n",
        "\n",
        "* Apply the **`ss.fit_transform`** method to the `wine_subset` DataFrame."
      ],
      "metadata": {
        "id": "l8Haq1U9DUsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import StandardScaler from scikit-learn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create the scaler\n",
        "ss = StandardScaler()\n",
        "\n",
        "# Take a subset of the DataFrame you want to scale \n",
        "wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]\n",
        "\n",
        "# Apply the scaler to the DataFrame subset\n",
        "wine_subset_scaled = ss.fit_transform(wine_subset)"
      ],
      "metadata": {
        "id": "sBwIvYVeDtDg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In scikit-learn, running **`fit_transform`** during preprocessing will both fit the method to the data as well as transform the data in a single step.\n",
        "\n",
        "### ***KNN on non-scaled data***\n",
        "\n",
        "Let's first take a look at the accuracy of a K-nearest neighbors model on the `wine` dataset without standardizing the data. The `knn` model as well as the `X` and `y` data and labels sets have been created already. Most of this process of creating models in scikit-learn should look familiar to you.\n",
        "\n",
        "* Split the dataset into training and test sets using **`train_test_split()`**.\n",
        "\n",
        "* Use the `knn` model's **`fit()`** method on the `X_train` data and `y_train` labels, to fit the model to the data.\n",
        "\n",
        "* Print out the knn model's **`score()`** on the `X_test` data and `y_test` labels to evaluate the model."
      ],
      "metadata": {
        "id": "5FwYxcXhEFXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wine = pd.read_csv('wine_types.csv')\n",
        "X = wine.drop(columns='Type')\n",
        "y = wine['Type']\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Split the dataset and labels into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "# Fit the k-nearest neighbors model to the training data\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Score the model on the test data\n",
        "print(knn.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4SIf09JEBuF",
        "outputId": "ab2de3be-6512-4683-d86f-bef4d0895d97"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***KNN on scaled data***\n",
        "\n",
        "The accuracy score on the unscaled `wine` dataset was decent, but we can likely do better if we scale the dataset. The process is mostly the same as the previous exercise, with the added step of scaling the data. Once again, the `knn` model as well as the `X` and `y` data and labels set have already been created for you.\n",
        "\n",
        "* Create the **`StandardScaler()`**, stored in a variable named `ss`.\n",
        "* Apply the **`ss.fit_transform`** method to the `X` dataset.\n",
        "* Use the `knn` model's **`.fit()`** method on the `X_train` data and `y_train` labels, to fit the model to the data.\n",
        "* Print out the knn model's `score()` on the `X_test` data and `y_test` labels to evaluate the model."
      ],
      "metadata": {
        "id": "85RpH_nhGOTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wine = pd.read_csv('wine_types.csv')\n",
        "X = wine.drop(columns='Type')\n",
        "y = wine['Type']\n",
        "\n",
        "# Create the scaling method.\n",
        "ss = StandardScaler()\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Apply the scaling method to the dataset used for modeling.\n",
        "X_scaled = ss.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
        "\n",
        "# Fit the k-nearest neighbors model to the training data.\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Score the model on the test data.\n",
        "print(knn.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylYuvvxKFWNs",
        "outputId": "8f31a0ab-6c7d-4058-9a8b-2b5997431b19"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Encoding binary variables - Pandas***\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "0     y\n",
        "1     n\n",
        "2     n\n",
        "3     y\n",
        "Name: subscribed, dtype: object\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "H0xM3SFW7B5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ใช้ .apply\n",
        "df[\"sub_enc\"] = df[\"subscribed\"].apply(lambda x: 1 if x == \"y\" else 0)\n",
        "\n",
        "# ใช้ np.where\n",
        "df[\"sub_enc\"] = np.where(df[df['subscribed'] == 'y']['subscribed'], 1, 0)"
      ],
      "metadata": {
        "id": "ZqeVBuD6HGGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "subscribed sub_enc\n",
        "0     y       1\n",
        "1     n       0\n",
        "2     n       0\n",
        "3     y       1\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "zbiyTQRR71kY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Encoding binary variables - scikit-learn***"
      ],
      "metadata": {
        "id": "8XgeXk928Wi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df[\"sub_enc_le\"] = le.fit_transform(df[\"subscribed\"])"
      ],
      "metadata": {
        "id": "-Sc03A018LkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "subscribed sub_enc_le\n",
        "0     y       1\n",
        "1     n       0\n",
        "2     n       0\n",
        "3     y       1\n",
        "```\n"
      ],
      "metadata": {
        "id": "oZM_Blpx88kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***One-hot encoding***\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "-GV1PU5v9TtH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Encoding categorical variables - binary***\n",
        "\n",
        "Take a look at the `hiking` dataset. There are several columns here that need encoding, one of which is the `Accessible` column, which needs to be encoded in order to be modeled. `Accessible` is a binary feature, so it has two values - either `Y` or `N` - so it needs to be encoded into `1`s and `0`s. Use scikit-learn's **`LabelEncoder`** method to do that transformation.\n",
        "\n",
        "* Store **`LabelEncoder()`** in a variable named `enc`\n",
        "* Using the encoder's **`fit_transform()`** function, encode the `hiking` dataset's `\"Accessible\"` column. Call the new column `Accessible_enc`.\n",
        "\n",
        "* Compare the two columns side-by-side to see the encoding."
      ],
      "metadata": {
        "id": "sUk6ESE_9wus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "hiking = pd.read_json('hiking.json')\n",
        "\n",
        "# Set up the LabelEncoder object\n",
        "enc =  LabelEncoder()\n",
        "\n",
        "# Apply the encoding to the \"Accessible\" column\n",
        "hiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])\n",
        "\n",
        "# Compare the two columns\n",
        "print(hiking[['Accessible', 'Accessible_enc']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsXI68pb8-dX",
        "outputId": "b6130d1e-ac28-41ad-f9aa-d5574f562aea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Accessible  Accessible_enc\n",
            "0          Y               1\n",
            "1          N               0\n",
            "2          N               0\n",
            "3          N               0\n",
            "4          N               0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`.fit_transform()`** is a good way to both fit an encoding and transform the data in a single step.\n",
        "\n",
        "### ***Encoding categorical variables - one-hot***\n",
        "\n",
        "One of the columns in the volunteer dataset, `category_desc`, gives category descriptions for the volunteer opportunities listed. Because it is a categorical variable with more than two categories, we need to use one-hot encoding to transform this column numerically. Use Pandas' **`get_dummies()`** function to do so.\n",
        "\n",
        "* Call **`get_dummies()`** on the `volunteer[\"category_desc\"]` column to create the encoded columns and assign it to `category_enc`.\n",
        "\n",
        "* Print out the `head()` of the `category_enc` variable to take a look at the encoded columns."
      ],
      "metadata": {
        "id": "L_AnXS0n_MuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "volunteer = pd.read_csv('volunteer_opportunities.csv')\n",
        "\n",
        "# Transform the category_desc column\n",
        "category_enc = pd.get_dummies(volunteer[\"category_desc\"])\n",
        "category_enc_2 = pd.get_dummies(volunteer[\"category_desc\"], drop_first=True)\n",
        "# Take a look at the encoded columns\n",
        "print(category_enc.head())  \n",
        "print(\"\\n\\n\")\n",
        "print(category_enc_2.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDbOaL499_9Z",
        "outputId": "97dd7e50-8b57-40fc-b302-cabdff44e0b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Education  Emergency Preparedness  Environment  Health  \\\n",
            "0          0                       0            0       0   \n",
            "1          0                       0            0       0   \n",
            "2          0                       0            0       0   \n",
            "3          0                       0            0       0   \n",
            "4          0                       0            1       0   \n",
            "\n",
            "   Helping Neighbors in Need  Strengthening Communities  \n",
            "0                          0                          0  \n",
            "1                          0                          1  \n",
            "2                          0                          1  \n",
            "3                          0                          1  \n",
            "4                          0                          0  \n",
            "\n",
            "\n",
            "\n",
            "   Emergency Preparedness  Environment  Health  Helping Neighbors in Need  \\\n",
            "0                       0            0       0                          0   \n",
            "1                       0            0       0                          0   \n",
            "2                       0            0       0                          0   \n",
            "3                       0            0       0                          0   \n",
            "4                       0            1       0                          0   \n",
            "\n",
            "   Strengthening Communities  \n",
            "0                          0  \n",
            "1                          1  \n",
            "2                          1  \n",
            "3                          1  \n",
            "4                          0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`get_dummies()`** is a simple and quick way to encode categorical variables.\n",
        "\n",
        "# ***Engineering numerical features - taking an average***\n",
        "\n",
        "A good use case for taking an aggregate statistic to create a new feature is to take the mean of columns. Here, you have a DataFrame of running times named `running_times_5k`. For each name in the dataset, take the mean of their 5 run times.\n",
        "\n",
        "* Create a list of the columns you want to take the average of and store it in a variable named `run_columns`.\n",
        "\n",
        "* Use **`apply`** to take the **`mean()`** of the list of columns and remember to set **`axis=1`**. Use **`lambda row:`** in the **`apply`**.\n"
      ],
      "metadata": {
        "id": "cKxnbCBPAIkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of the columns to average\n",
        "run_columns = ['run1',  'run2',  'run3',  'run4',  'run5']\n",
        "\n",
        "# Use apply to create a mean column\n",
        "running_times_5k[\"mean\"] = running_times_5k.apply(lambda row: row[run_columns].mean(), axis=1)\n",
        "\n",
        "# Take a look at the results\n",
        "print(running_times_5k)"
      ],
      "metadata": {
        "id": "H-1L-w4fCvmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "          name  run1  run2  run3  run4  run5   mean\n",
        "    0      Sue  20.1  18.5  19.6  20.3  18.3  19.36\n",
        "    1     Mark  16.5  17.1  16.9  17.6  17.3  17.08\n",
        "    2     Sean  23.5  25.1  25.2  24.6  23.9  24.46\n",
        "    3     Erin  21.7  21.1  20.9  22.1  22.2  21.60\n",
        "    4    Jenny  25.8  27.1  26.1  26.7  26.9  26.52\n",
        "    5  Russell  30.9  29.6  31.4  30.4  29.9  30.44\n",
        "```\n",
        "\n",
        "Lambdas are especially helpful for operating across columns.\n",
        "\n",
        "### ***Engineering numerical features - datetime***\n",
        "There are several columns in the volunteer dataset comprised of datetimes. Let's take a look at the `start_date_date` column and extract just the month to use as a feature for modeling.\n",
        "\n",
        "* Use Pandas **`to_datetime()`** function on the `volunteer[\"start_date_date\"]` column and store it in a new column called `start_date_converted`.\n",
        "\n",
        "* To retrieve just the month, apply a **`lambda`** function to `volunteer[\"start_date_converted\"]` that grabs the **`.month`** attribute from the `row`. \n",
        "\n",
        "* Store this in a new column called `start_date_month`.\n",
        "\n",
        "* Print the `head()` of just the `start_date_converted` and `start_date_month` columns."
      ],
      "metadata": {
        "id": "egdE2XLOD1ib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, convert string column to date column\n",
        "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer['start_date_date'])\n",
        "\n",
        "# Extract just the month from the converted column\n",
        "volunteer[\"start_date_month\"] = volunteer[\"start_date_converted\"].apply(lambda row: row.month)\n",
        "\n",
        "# Take a look at the converted and new month columns\n",
        "print(volunteer[['start_date_converted', 'start_date_month']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtbFQ_dAEmk-",
        "outputId": "8acd15c4-df18-4b9d-942e-8c39b93991c1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  start_date_converted  start_date_month\n",
            "0           2011-07-30                 7\n",
            "1           2011-02-01                 2\n",
            "2           2011-01-29                 1\n",
            "3           2011-02-14                 2\n",
            "4           2011-02-05                 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also use attributes like `.day` to get the day and `.year` to get the year from datetime columns."
      ],
      "metadata": {
        "id": "-tYkDNDpF8RP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Numeric Extraction***"
      ],
      "metadata": {
        "id": "0WN7inzbc6ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "my_string = \"temperature:75.6 F\"\n",
        "pattern = re.compile(r\"[^0-9]+(\\d+\\.\\d+)[^0-9]+\")\n",
        "temp = re.match(pattern, my_string)\n",
        "print(float(temp.group(1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-ccEGMIF3zL",
        "outputId": "f8f548a0-c432-4266-f310-0d01de8a1a16"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Vectorizing text***"
      ],
      "metadata": {
        "id": "urvDIkY6dqob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "print(documents.head())"
      ],
      "metadata": {
        "id": "bqskpa5udXwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "0 Building on successful events last summer and ...\n",
        "1 Build a website for an Afghan business\n",
        "2 Please join us and the students from Mott Hall...\n",
        "3 The Oxfam Action Corps is a group of dedicated...\n",
        "4 Stop 'N' Swap reduces NYC's waste by finding n...\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "7QxTH2_3d_Qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vec = TfidfVectorizer()\n",
        "text_tfidf = tfidf_vec.fit_transform(documents)"
      ],
      "metadata": {
        "id": "pS8DePwjeIEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Engineering features from strings - extraction***\n",
        "\n",
        "The Length column in the `hiking` dataset is a column of strings, but contained in the column is the mileage for the hike. We're going to extract this mileage using regular expressions, and then use a **`lambda`** in Pandas to apply the extraction to the DataFrame.\n",
        "\n",
        "\n",
        "* Create a pattern that will extract numbers and decimals from text, using **`\\d+`** to get numbers and **`\\.`** to get decimals, and pass it into `re`'s compile function.\n",
        "\n",
        "* Use `re`'s match function to search tlhe text, passing in the pattern and the length text.\n",
        "\n",
        "* Use the matched mile's **`group()`** attribute to extract the matched pattern, making sure to match group `0`, and pass it into float.\n",
        "\n",
        "* Apply the `return_mileage()` function to the `hiking[\"Length\"]` column."
      ],
      "metadata": {
        "id": "HacAWgu9eV7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "hiking = pd.read_json('hiking.json').iloc[:29]\n",
        "\n",
        "# Write a pattern to extract numbers and decimals\n",
        "def return_mileage(length):\n",
        "    pattern = re.compile(r\"\\d+\\.\\d+\")\n",
        "    \n",
        "    # Search the text for matches\n",
        "    mile = re.match(pattern, length)\n",
        "    \n",
        "    # If a value is returned, use group(0) to return the found value\n",
        "    if mile is not None:\n",
        "        return float(mile.group(0))\n",
        "        \n",
        "# Apply the function to the Length column and take a look at both columns\n",
        "hiking[\"Length_num\"] = hiking[\"Length\"].apply(return_mileage)\n",
        "\n",
        "#hiking[\"Length_num\"] = hiking[\"Length\"].apply(lambda row: return_mileage(row)) ก็ได้\n",
        "print(hiking[[\"Length\", \"Length_num\"]].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPW8suUsfvUF",
        "outputId": "f1a95c4b-2480-477a-ed5a-082b162d5efd"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Length  Length_num\n",
            "0   0.8 miles        0.80\n",
            "1    1.0 mile        1.00\n",
            "2  0.75 miles        0.75\n",
            "3   0.5 miles        0.50\n",
            "4   0.5 miles        0.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Engineering features from strings - tf/idf***\n",
        "\n",
        "Let's transform the `volunteer` dataset's title column into a text vector, to use in a prediction task in the next exercise.\n",
        "\n",
        "* Store the `volunteer[\"title\"]` column in a variable named `title_text`.\n",
        "* Use the `tfidf_vec` vectorizer's `fit_transform()` function on `title_text` to transform the text into a `tf-idf` vector."
      ],
      "metadata": {
        "id": "iNfTwyjSljzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the title text\n",
        "title_text = volunteer[\"title\"]\n",
        "\n",
        "# Create the vectorizer method\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "\n",
        "# Transform the text into tf-idf vectors\n",
        "text_tfidf = tfidf_vec.fit_transform(title_text)"
      ],
      "metadata": {
        "id": "1fQlcQJ4kDEb"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-learn provides several methods for text vectorization.\n",
        "\n",
        "### ***Text classification using tf/idf vectors***\n",
        "\n",
        "Now that we've encoded the volunteer dataset's title column into `tf/idf` vectors, let's use those vectors to try to predict the `category_desc` column.\n",
        "\n",
        "* Using **`train_test_split`**, split the **`text_tfidf`** vector, along with your `y` variable, into training and test sets. Set the **`stratify`** parameter equal to `y`, since the class distribution is uneven. Notice that we have to run the **`toarray()`** method on the tf/idf vector, in order to get in it the proper format for scikit-learn.\n",
        "\n",
        "* Use Naive Bayes' **`fit()`** method on the `X_train` and `y_train` variables.\n",
        "\n",
        "* Print out the **`score()`** of the `X_test` and `y_test` variables."
      ],
      "metadata": {
        "id": "VMPKc69hmF0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nb = GaussianNB(priors=None)\n",
        "volunteer = volunteer[[\"title\", 'category_desc']].dropna()\n",
        "title_text = volunteer[\"title\"]\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
        "\n",
        "# Split the dataset according to the class distribution of category_desc\n",
        "y = volunteer[\"category_desc\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y)\n",
        "\n",
        "# Fit the model to the training data\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Print out the model's accuracy\n",
        "print(nb.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMQXvNhamBo9",
        "outputId": "38b3a992-2c50-4995-eb28-1e349b13f215"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5612903225806452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the model doesn't score very well. We'll work on selecting the best features for modeling.\n",
        "\n",
        "### ***Selecting relevant features***\n",
        "\n",
        "Now let's identify the redundant columns in the `volunteer` dataset and perform feature selection on the dataset to return a DataFrame of the relevant features.\n",
        "\n",
        "For example, if you explore the `volunteer` dataset in the console, you'll see three features which are related to location: `locality`, `region`, and `postalcode`. They contain repeated information, so it would make sense to keep only one of the features.\n",
        "\n",
        "There are also features that have gone through the feature engineering process: columns like `Education` and `Emergency Preparedness` are a product of encoding the categorical variable `category_desc`, so `category_desc` itself is redundant now.\n",
        "\n",
        "Take a moment to examine the features of `volunteer` in the console, and try to identify the redundant features.\n",
        "\n",
        "* Create a list of redundant column names and store it in the `to_drop` variable:\n",
        "\n",
        "* Out of all the location-related features, keep only `postcode`.\n",
        "\n",
        "* Features that have gone through the feature engineering process are redundant as well.\n",
        "\n",
        "* Drop the columns from the dataset using **`.drop()`**.\n"
      ],
      "metadata": {
        "id": "5Weo1hY7oZan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "volunteer = pd.read_csv('volunteer_opportunities.csv')\n",
        "\n",
        "# Create a list of redundant column names to drop\n",
        "to_drop = [\"locality\", \"region\", \"created_date\", \"vol_requests\", \"category_desc\"]\n",
        "\n",
        "# Drop those columns from the dataset\n",
        "volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
        "\n",
        "# Print out the head of the new dataset\n",
        "display(volunteer_subset.head())"
      ],
      "metadata": {
        "id": "2o01RAV-tfo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's often easier to collect a list of columns to drop, rather than dropping them individually.\n",
        "\n",
        "### ***Checking for correlated features***\n",
        "\n",
        "Let's take a look at the `wine` dataset again, which is made up of continuous, numerical features. Run Pearson's correlation coefficient on the dataset to determine which columns are good candidates for eliminating. Then, remove those columns from the DataFrame.\n",
        "\n",
        "* Print out the column correlations of the `wine` dataset using **`corr()`**.\n",
        "\n",
        "* Take a minute to look at the correlations. Identify a column where the correlation value is greater than `0.75` at least twice and store it in the `to_drop` variable.\n",
        "\n",
        "* Drop that column from the DataFrame using **`drop()`**."
      ],
      "metadata": {
        "id": "b9b9k1KFtvCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wine = pd.read_csv('wine_types.csv')\n",
        "wine.corr()"
      ],
      "metadata": {
        "id": "uKS78-Muvnzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "                              Flavanoids  Total phenols  Malic acid  OD280/OD315 of diluted wines       Hue\n",
        "Flavanoids                      1.000000       0.864564   -0.411007                      0.787194  0.543479\n",
        "Total phenols                   0.864564       1.000000   -0.335167                      0.699949  0.433681\n",
        "Malic acid                     -0.411007      -0.335167    1.000000                     -0.368710 -0.561296\n",
        "OD280/OD315 of diluted wines    0.787194       0.699949   -0.368710                      1.000000  0.565468\n",
        "Hue                             0.543479       0.433681   -0.561296                      0.565468  1.000000\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "50Ps3P0Xvj2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out the column correlations of the wine dataset\n",
        "print(wine.corr())\n",
        "\n",
        "# Take a minute to find the column where the correlation value is greater than 0.75 at least twice\n",
        "to_drop = \"Flavanoids\"\n",
        "\n",
        "# Drop that column from the DataFrame\n",
        "wine = wine.drop(to_drop, axis=1)"
      ],
      "metadata": {
        "id": "kzTO0Mkmu3Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "                              Total phenols  Malic acid  OD280/OD315 of diluted wines       Hue\n",
        "Total phenols                      1.000000   -0.335167                      0.699949  0.433681\n",
        "Malic acid                        -0.335167    1.000000                     -0.368710 -0.561296\n",
        "OD280/OD315 of diluted wines       0.699949   -0.368710                      1.000000  0.565468\n",
        "Hue                                0.433681   -0.561296                      0.565468  1.000000\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "BnMHCfG7wLjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping correlated features is often an iterative process, so you may need to try different combinations in your model.\n",
        "\n",
        "# ***Looking at word weights***\n",
        "\n",
        "* Use **`.vocabulary_`** attribute"
      ],
      "metadata": {
        "id": "-IvMh3SywXfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title_text = volunteer[\"title\"]\n",
        "display(title_text.head())\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
        "print(tfidf_vec.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "p37TERKXxnqg",
        "outputId": "58d8db92-e360-44b2-d43d-a0042b352ff2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0    Volunteers Needed For Rise Up & Stay Put! Home...\n",
              "1                                         Web designer\n",
              "2        Urban Adventures - Ice Skating at Lasker Rink\n",
              "3    Fight global hunger and support women farmers ...\n",
              "4                                        Stop 'N' Swap\n",
              "Name: title, dtype: object"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'volunteers': 1086, 'needed': 690, 'for': 404, 'rise': 869, 'up': 1061, 'stay': 959, 'put': 822, 'home': 493, 'rescue': 855, 'fair': 375, 'web': 1095, 'designer': 297, 'urban': 1063, 'adventures': 43, 'ice': 515, 'skating': 930, 'at': 98, 'lasker': 587, 'rink': 868, 'fight': 392, 'global': 447, 'hunger': 512, 'and': 75, 'support': 986, 'women': 1108, 'farmers': 380, 'join': 562, 'the': 1012, 'oxfam': 739, 'action': 31, 'corps': 255, 'in': 523, 'nyc': 710, 'stop': 962, 'swap': 989, 'queens': 825, 'staff': 951, 'development': 300, 'trainer': 1037, 'claro': 213, 'brooklyn': 155, 'volunteer': 1084, 'attorney': 101, 'cents': 188, 'ability': 23, 'community': 235, 'health': 480, 'advocates': 48, 'supervise': 984, 'children': 202, 'highland': 491, 'park': 748, 'garden': 433, 'worldofmoney': 1118, 'org': 727, 'youth': 1132, 'amazing': 67, 'race': 826, 'qualified': 824, 'board': 142, 'member': 649, 'seats': 899, 'available': 106, 'young': 1130, 'adult': 38, 'tutor': 1052, 'updated': 1062, '30': 13, '11': 0, 'insurance': 538, 'claims': 212, 'manager': 629, 'timebanksnyc': 1024, 'great': 456, 'exchange': 364, 'clean': 218, 'asbury': 89, 'cementary': 184, 'staten': 958, 'island': 550, 'senior': 908, 'citizen': 207, 'friendly': 416, 'visitor': 1080, 'shop': 920, 'tree': 1045, 'care': 173, 'workshop': 1115, '20': 5, 'movie': 675, 'screener': 895, 'seniors': 909, 'farm': 379, 'graphic': 454, 'general': 437, 'services': 913, 'handyman': 473, 'open': 721, 'house': 505, 'seeking': 905, 'drupal': 324, 'end': 344, 'poverty': 788, 'campaign': 166, 'get': 440, 'tools': 1029, 'to': 1025, 'recycling': 841, 'family': 377, 'event': 360, 'clerical': 220, 'cancer': 169, 'walk': 1088, 'befitnyc': 128, 'physical': 772, 'activity': 35, 'organizers': 731, 'decision': 287, 'day': 284, '2011': 6, 'needs': 691, 'your': 1131, 'help': 484, 'gain': 429, 'valuable': 1067, 'counseling': 260, 'experience': 368, 'on': 717, 'samaritans': 884, '24': 11, 'hour': 504, 'crisis': 271, 'hotline': 503, 'heart': 482, 'gallery': 431, 'our': 734, 'info': 528, 'table': 991, 'finding': 397, 'homes': 496, 'kids': 570, 'yiddish': 1124, 'speaking': 943, 'homework': 497, 'helper': 485, 'skilled': 932, 'rebuilding': 835, 'together': 1027, 'repairs': 853, 'greenteam': 462, 'advetures': 45, 'summer': 982, 'streets': 973, 'tuesday': 1050, 'evenings': 359, 'with': 1107, 'masa': 641, 'lunch': 623, 'program': 805, 'us': 1064, 'outreach': 737, 'meals': 647, 'preparedness': 795, 'compost': 236, 'project': 808, 'master': 642, 'composter': 237, 'certificate': 191, 'course': 264, 'emblemhealth': 341, 'bronx': 154, 'of': 713, 'service': 912, 'jcc': 557, 'manhattan': 630, 'registration': 847, 'girl': 442, 'scout': 894, 'series': 911, 'dorot': 316, 'rosh': 876, 'hashanah': 477, 'package': 740, 'delivery': 293, 'painting': 744, 'instructor': 536, 'jasa': 556, 'hes': 489, 'center': 185, '3rd': 14, 'annual': 77, 'flyny': 401, 'kite': 576, 'festival': 390, 'tomorrow': 1028, 'business': 161, 'leaders': 594, 'teach': 997, 'basics': 118, 'high': 490, 'schoolers': 891, 'jingle': 559, 'bell': 130, 'run': 879, 'arthritis': 85, 'gardening': 434, 'ft': 421, 'tryon': 1049, 'st': 950, 'martin': 639, 'poetry': 781, 'new': 698, 'york': 1126, 'college': 230, 'goal': 448, 'sunday': 983, 'february': 385, '2012': 7, 'dance': 280, '22nd': 9, 'latino': 588, 'march': 633, '17': 3, 'university': 1059, 'saturday': 887, 'tutors': 1054, 'planet': 777, 'human': 510, 'mapping': 631, 'give': 444, 'week': 1097, 'child': 199, 'learn': 597, 'read': 831, 'storytelling': 967, 'costume': 258, 'making': 626, 'stage': 952, 'design': 296, 'emergency': 342, '9th': 21, 'west': 1100, 'side': 926, 'county': 263, 'nutrition': 706, 'educator': 336, 'shape': 918, 'east': 328, '54st': 15, 'rec': 836, 'water': 1093, 'aerobics': 50, 'asser': 91, 'levy': 601, 'paint': 743, 'alongside': 64, 'publicolor': 818, 'students': 978, 'jumpstart': 564, 'readers': 832, 'lead': 592, 'crafts': 267, 'games': 432, 'face': 372, 'popcorn': 784, 'jackie': 553, 'robinson': 872, 'parent': 747, 'fitness': 399, 'starrett': 956, 'city': 210, 'line': 613, 'dancer': 281, 'math': 644, 'literacy': 615, 'be': 122, 'climb': 222, 'top': 1030, 'marketing': 637, 'assistant': 94, 'education': 335, 'nonprofit': 703, 'seeks': 906, 'recruitment': 840, 'mentors': 655, 'register': 845, 'attend': 100, 'breakfast': 152, 'orientation': 732, 'january': 555, 'planning': 778, 'red': 842, 'cross': 273, 'clubs': 224, 'deliver': 291, 'winter': 1105, 'visit': 1078, 'an': 72, 'isolated': 551, 'exercise': 366, 'coach': 227, 'night': 700, 'beach': 123, 'change': 193, 'art': 84, 'programs': 807, 'consumer': 243, 'protection': 814, 'law': 590, 'liver': 617, 'life': 607, 'leader': 593, 'soup': 941, 'kitchen': 575, 'eastern': 329, 'john': 561, 'muir': 680, 'street': 971, 'vendor': 1071, 'monthly': 670, 'team': 1001, 'fiesta': 391, 'throgs': 1022, 'neck': 688, 'computer': 238, 'teacher': 998, 'leadership': 595, 'council': 259, 'opportunity': 724, 'conversation': 245, 'helpers': 486, 'grades': 451, 'pantry': 745, 'distribution': 308, 'earth': 327, 'tech': 1002, 'website': 1096, 'opportunities': 723, 'classroom': 216, 'set': 916, 'brush': 156, 'kindness': 573, 'transportation': 1044, 'alternatives': 65, 'bike': 138, 'valet': 1066, 'video': 1073, 'editing': 333, 'professionals': 802, 'stipend': 961, 'after': 54, 'school': 890, 'mentor': 653, 'networking': 696, 'bowling': 147, 'fun': 422, 'harlem': 474, 'lanes': 583, 'yoga': 1125, 'spanish': 942, 'or': 726, 'french': 413, 'feed': 386, 'hungry': 513, 'yorkers': 1127, '55': 16, 'only': 720, 'phone': 768, 'bank': 114, 'representative': 854, 'reach': 830, 'out': 735, 'morris': 672, 'heights': 483, 'special': 944, 'camp': 165, 'susan': 988, 'komen': 579, 'cure': 277, 'greater': 457, 'affiliate': 52, 'dumbo': 325, 'arts': 87, 'organizational': 729, 'budget': 158, 'money': 668, 'makes': 625, 'sense': 910, 'training': 1039, 'site': 929, 'videographer': 1074, 'fly': 400, 'by': 162, 'theater': 1013, 'grant': 453, 'writer': 1121, 'proposal': 813, 'preparation': 794, 'fund': 423, 'raising': 828, 'harm': 475, 'reduction': 843, 'adv': 40, 'intern': 541, 'serving': 914, 'lgbt': 603, 'adults': 39, 'how': 507, 'ride': 866, 'bikes': 139, 'research': 856, 'fundraising': 425, 'developement': 299, 'cook': 247, 'row': 878, 'afterschool': 55, 'middle': 659, 'shower': 924, 'fundraisers': 424, 'it': 552, 'interpreters': 545, 'lawyers': 591, 'haitian': 470, 'abe': 22, 'pre': 792, 'ged': 436, 'monitor': 669, 'astoria': 97, 'million': 663, 'trees': 1046, 'giveaway': 445, 'do': 310, 'you': 1128, 'want': 1091, 'make': 624, 'difference': 302, 'classwish': 217, 'snow': 936, 'shoveling': 922, 'citizenship': 209, 'press': 796, 'list': 614, 'as': 88, 'public': 816, 'benefit': 131, 'counselor': 261, 'aces': 29, 'relations': 848, 'plan': 776, 'review': 864, 'friendship': 418, 'positive': 787, 'beginnings': 129, 'kit': 574, 'mary': 640, 'recreation': 838, 'does': 311, 'organization': 728, 'need': 689, 'search': 897, 'strategy': 969, 'esl': 355, 'affected': 51, 'storm': 965, 'transform': 1040, 'lives': 618, 'strengthen': 975, 'communities': 234, 'become': 127, 'driver': 323, 'veterans': 1072, 'chinese': 204, 'translator': 1042, 'instructors': 537, 'museum': 683, 'membership': 650, 'department': 294, 'director': 304, 'beautify': 125, 'transitional': 1041, 'residence': 857, 'homeless': 495, 'men': 652, 'tank': 995, 'internship': 543, 'projects': 809, 'wild': 1103, 'boys': 148, 'hope': 500, 'girls': 443, 'communications': 233, 'raise': 827, 'awareness': 108, 'administrative': 36, 'alliance': 63, 'registrar': 846, 'ms': 677, 'word': 1109, 'career': 174, 'passover': 754, 'enjoy': 348, 'outdoor': 736, 'early': 326, 'childhood': 201, 'build': 159, 'plastic': 780, 'bottle': 146, 'sculpture': 896, 'pride': 798, 'is': 549, 'just': 566, 'around': 83, 'corner': 253, 'involved': 546, 'now': 705, 'fresh': 414, 'air': 60, 'teachers': 999, 'find': 396, 'perfect': 761, 'job': 560, 'office': 714, 'writing': 1122, 'data': 282, 'entry': 349, 'activism': 33, 'photography': 771, 'salesforce': 882, 'database': 283, 'customization': 279, 'photo': 769, 'essay': 356, 'legal': 600, 'advisor': 47, 'hike': 492, 'thon': 1019, 'coordinator': 251, 'laser': 586, 'tag': 992, 'dowling': 318, '175th': 4, 'information': 530, 'technology': 1004, 'fall': 376, 'forest': 406, 'restoration': 861, 'kickoff': 569, 'trevor': 1047, 'lifeline': 610, 'counselors': 262, 'thomas': 1018, 'jefferson': 558, 'materials': 643, 'year': 1123, 'founder': 410, 'executive': 365, 'haunted': 478, 'lantern': 585, 'tours': 1034, 'fort': 407, 'totten': 1031, 'national': 687, 'sexual': 917, 'assault': 90, 'online': 719, 'events': 361, 'trainers': 1038, 'african': 53, 'american': 70, 'clothing': 223, 'drive': 322, 'returning': 863, 'seeds': 904, 'success': 981, 'plant': 779, 'today': 1026, 'growth': 467, 'udec': 1055, 'enviromedia': 351, 'mobile': 665, 'maritime': 635, 'bacchanal': 110, 'pirates': 775, 'fest': 389, 'ikea': 517, 'erie': 352, 'basin': 119, 'diabetes': 301, 'association': 96, 'feria': 388, 'de': 285, 'salud': 883, 'nepali': 694, 'bangla': 113, 'punjabi': 819, 'translators': 1043, 'food': 402, 'tent': 1007, 'not': 704, 'profit': 804, 'pioneer': 774, 'capoeira': 171, 'various': 1069, 'positions': 786, 'dispatcher': 307, 'trainee': 1036, 'ing': 531, 'marathon': 632, 'free': 412, 'love': 621, 'books': 144, 'dear': 286, 'authors': 104, 'aide': 59, 'scheuer': 889, 'merchandise': 656, 'donate': 313, 'supplies': 985, 'feast': 384, 'gala': 430, 'battery': 120, 'soccer': 937, 'futsal': 426, 'performing': 762, 'advanced': 41, 'classes': 215, 'world': 1117, 'science': 893, 'western': 1101, 'americorps': 71, 'economic': 332, 'security': 903, 'initiative': 532, 'esi': 354, 'mill': 662, 'centers': 186, 'midtown': 660, 'zumba': 1135, 'vision': 1077, 'mission': 664, 'analysis': 73, 'lab': 580, 'teaching': 1000, 'housing': 506, 'works': 1114, 'dime': 303, 'assist': 92, 'resume': 862, 'building': 160, 'society': 939, 'coaches': 228, 'vs': 1087, 'committee': 232, 'russian': 880, 'foster': 409, 'celebration': 183, 'may': 645, '21th': 8, 'one': 718, 'pager': 742, 'donation': 314, 'hurricane': 514, 'irene': 547, 'far': 378, 'rockaway': 873, 'working': 1113, 'olympics': 716, 'tournament': 1033, 'reading': 833, 'partners': 750, 'cooper': 249, 'square': 949, 'thrift': 1020, 'spring': 948, 'case': 179, 'management': 628, 'fvcp': 428, 'trail': 1035, 'crew': 270, 'halloween': 471, 'carnival': 177, 'walkathon': 1089, 'feasibility': 383, 'analyst': 74, 'police': 782, 'seminar': 907, 'work': 1111, 'visually': 1082, 'impaired': 521, 'teens': 1006, 'this': 1017, 'energy': 345, 'efficiency': 337, 'season': 898, 'benefits': 132, 'reception': 837, 'drill': 321, 'copywriting': 252, 'coord': 250, 'have': 479, 'penchant': 757, 'all': 62, 'things': 1015, 'vintage': 1075, 'thriftshop': 1021, 'moving': 676, 'storage': 963, 'partner': 749, 'pencil': 758, 'partnership': 751, 'packing': 741, 'sign': 927, 'cuny': 276, '8th': 20, 'sports': 947, 'expo': 370, 'cares': 176, 'cheerleaders': 197, 'wanted': 1092, 'habitat': 469, 'finance': 395, 'coffee': 229, 'english': 347, 'practice': 790, 'learners': 598, 'healthy': 481, 'active': 32, 'time': 1023, 'april': 80, 'fashion': 381, 'strawberry': 970, 'assistants': 95, 'creative': 269, 'thinkers': 1016, 'central': 187, 'zoo': 1134, '125th': 1, 'bideawee': 136, 'greeters': 464, 'looking': 620, 'real': 834, 'impact': 520, 'inform': 529, 'people': 760, 'practices': 791, 'lifebeat': 608, 'streetsquash': 974, 'discovery': 306, 'neighborhood': 693, 'profiles': 803, 'take': 993, 'stand': 955, 'against': 56, 'violence': 1076, 'expert': 369, 'advice': 46, 'june': 565, 'schedule': 888, 'crowdfunding': 275, 'penny': 759, 'harvest': 476, 'green': 458, 'chefs': 198, 'nutritionists': 707, 'foodies': 403, 'mentoring': 654, 'boom': 145, 'newsletter': 699, 'come': 231, 'strides': 976, 'walks': 1090, 'childcare': 200, 'social': 938, 'media': 648, 'giving': 446, 'can': 168, 'ambassador': 68, '2nd': 12, 'thanksgiving': 1010, 'feeding': 387, 'needy': 692, 'publicity': 817, 'patient': 755, 'caregiver': 175, 'visiting': 1079, 'homebound': 494, 'fc': 382, 'nyawc': 709, 'forum': 408, 'about': 25, 'volunteering': 1085, 'refreshments': 844, 'sara': 886, 'roosevelt': 875, 'cleanup': 219, 'beautification': 124, 'animal': 76, 'hudson': 509, 'river': 870, 'mariners': 634, 'response': 860, 'exhibit': 367, 'aboard': 24, 'lilac': 612, 'client': 221, 'welcome': 1099, 'desk': 298, 'older': 715, 'riverbank': 871, 'roller': 874, 'lexington': 602, 'craft': 266, 'poll': 783, 'workers': 1112, 'interperters': 544, 'accounting': 28, 'assistance': 93, 'hosting': 502, 'promotion': 811, 'unicef': 1057, 'tap': 996, '23': 10, 'release': 849, 'dedication': 289, 'programming': 806, 'incarnation': 525, 'donor': 315, 'journalism': 563, 'kieran': 572, 'sponsorship': 946, 'sag': 881, 'gear': 435, 'vehicle': 1070, 'workshops': 1116, 'because': 126, 'every': 362, 'deserves': 295, 'chance': 192, 'prep': 793, 'pin': 773, 'delivered': 292, 'shred': 925, '5th': 18, 'avenue': 107, 'cdsc': 182, 'starving': 957, 'artist': 86, 'show': 923, 'system': 990, 'front': 420, 'share': 919, 'lanch': 581, 'student': 977, 'hemophilia': 488, 'liason': 605, 'methodist': 658, 'hospital': 501, 'bay': 121, 'ridge': 867, 'benonhurst': 133, 'area': 82, 'sought': 940, 'autistic': 105, 'douglaston': 317, 'qns': 823, 'administrator': 37, 'call': 163, 'governor': 450, 'recruiter': 839, 'purim': 821, 'envelope': 350, 'stuffing': 980, 'population': 785, 'estimate': 357, 'jam': 554, 'break': 151, 'down': 319, 'campaigner': 167, 'helpline': 487, 'store': 964, 'first': 398, 'generation': 439, 'van': 1068, 'cortlandt': 256, 'remembrance': 851, 'survey': 987, 'resonations': 858, 'breast': 153, 'engine': 346, 'optimization': 725, 'memorial': 651, 'sloan': 934, 'kettering': 568, 'greenhouse': 459, 'greening': 460, 'concert': 241, 'evacuation': 358, 'carpentry': 178, 'interior': 540, 'resources': 859, 'gift': 441, 'bicycling': 135, 'my': 686, 'friends': 417, 'honor': 498, 'weekend': 1098, 'person': 763, 'mural': 681, 'cooking': 248, 'editor': 334, 'personal': 764, 'shopper': 921, 'pro': 799, 'bono': 143, 'create': 268, 'cards': 172, 'step': 960, 'non': 702, 'provider': 815, 'interns': 542, 'motion': 674, 'graphics': 455, 'best': 134, 'buddies': 157, 'inern': 527, 'back': 111, 'little': 616, 'cosmetologist': 257, 'barber': 115, 'vocational': 1083, 'apartment': 79, 'greeter': 463, 'professional': 801, 'use': 1065, 'skills': 933, 'others': 733, 'figure': 393, 'croton': 274, 'cancercare': 170, '14th': 2, 'eif': 339, 'revlon': 865, 'chinatown': 203, 'therapeutic': 1014, 'aid': 58, 'activities': 34, 'ci': 206, 'corporate': 254, 'wordpress': 1110, 'blog': 141, 'instructer': 535, 'hook': 499, 'divert': 309, 'textiles': 1009, 'from': 419, 'landfill': 582, 'greenmarket': 461, 'textile': 1008, 'calling': 164, 'citizens': 208, 'improve': 522, 'achievement': 30, 'passion': 753, 'inc': 524, 'group': 465, 'drama': 320, '5k': 17, 'laundromats': 589, 'employment': 343, 'strategic': 968, 'never': 697, 'bad': 112, 'friend': 415, 'future': 427, 'class': 214, 'wish': 1106, 'fpcj': 411, 'worship': 1119, 'undergraduate': 1056, 'graduate': 452, 'conference': 242, 'we': 1094, 'promote': 810, 'knowledge': 578, 'parade': 746, 'archivist': 81, 'google': 449, 'adwords': 49, 'imentor': 518, 'more': 671, 'male': 627, 'miles': 661, 'moms': 666, 'charity': 196, 'century': 189, 'tour': 1032, 'civil': 211, 'patrol': 756, 'america': 69, 'kept': 567, 'secret': 901, 'ms131': 678, 'knitter': 577, 'crochet': 272, 'blankets': 140, 'ceo': 190, 'logo': 619, 'unique': 1058, 'will': 1104, 'big': 137, 'adventure': 42, 'accountant': 27, 'session': 915, 'age': 57, 'single': 928, 'mothers': 673, 'choice': 205, 'smc': 935, 'wii': 1102, 'nights': 701, 'market': 636, 'intake': 539, 'monday': 667, 'branding': 150, 'brand': 149, 'identity': 516, 'mt': 679, 'zion': 1133, 'kidz': 571, 'reorganize': 852, 'library': 606, 'athletic': 99, 'league': 596, 'turtle': 1051, 'music': 684, 'decorating': 288, 'party': 752, 'musician': 685, 'alzheimer': 66, 'bash': 117, 'proctor': 800, 'taking': 994, 'exams': 363, 'promotional': 812, 'personnel': 765, 'august': 103, 'operations': 722, 'skill': 931, 'networker': 695, 'ecological': 331, 'puppet': 820, 'income': 526, 'generating': 438, 'organizations': 730, 'lp': 622, 'streetfest': 972, '7th': 19, 'cpr': 265, 'lgbtq': 604, 'el': 340, 'museo': 682, 'del': 290, 'barrio': 116, 'met': 657, 'petition': 766, 'escort': 353, 'sand': 885, 'castle': 180, 'contest': 244, 'schools': 892, 'humanities': 511, 'second': 900, 'language': 584, 'babies': 109, 'teen': 1005, 'al': 61, 'oerter': 712, 'html': 508, 'curriculum': 278, 'photographer': 770, 'secretary': 902, 'pr': 789, 'would': 1120, 'like': 611, 'computers': 239, 'technical': 1003, 'grownyc': 466, 'that': 1011, 'extraordinary': 371, 'foreclosure': 405, 'prevention': 797, 'nylag': 711, 'ny': 708, 'concern': 240, 'inspire': 534, 'academic': 26, 'tutoring': 1053, 'rbi': 829, 'anyone': 78, 'cma': 225, 'cms': 226, 'conversion': 246, 'eating': 330, 'learning': 599, 'chaperones': 194, 'visits': 1081, 'unlimited': 1060, 'lifeguard': 609, 'facilitators': 374, 'troop': 1048, 'route': 877, 'marshall': 638, 'inmotion': 533, 'story': 966, 'stair': 953, 'domestic': 312, 'catskills': 181, 'relief': 850, 'effort': 338, 'audience': 102, 'pharmacy': 767, 'guide': 468, 'overnight': 738, 'immediate': 519, 'dirty': 305, 'hands': 472, 'facilitator': 373, 'specialist': 945, 'chapter': 195, 'stamps': 954, 'iridescent': 548, 'studio': 979, 'advertising': 44, 'filmmakers': 394, 'mayor': 646, 'youcantoo': 1129}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Take a look at the weight of the fourth row.***\n",
        "* Use **`.data`** attribute."
      ],
      "metadata": {
        "id": "DYY5B3pTzdHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(title_text.iloc[3], \"\\n\\n\")\n",
        "print(text_tfidf[3].data) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXZ_q_0DzSx7",
        "outputId": "dbac7295-b44a-4b77-c92c-039b1ed5d326"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fight global hunger and support women farmers - Join the Oxfam Action Corps in NYC! \n",
            "\n",
            "\n",
            "[0.2056446  0.18207404 0.29543299 0.314142   0.314142   0.16847137\n",
            " 0.25633688 0.314142   0.27186242 0.22435362 0.16380658 0.314142\n",
            " 0.314142   0.314142  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Get the indices of the words that have been weighted***\n",
        "* Use **`.indices`** attribute"
      ],
      "metadata": {
        "id": "o9vzabn4zrFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_tfidf[3].indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdMaLLX3zZcb",
        "outputId": "52fd1498-5dbf-44e9-a09e-a4cc8255872d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 710  523  255   31  739 1012  562  380 1108  986   75  512  447  392]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Change the key-values pairs***"
      ],
      "metadata": {
        "id": "6bhETMyt0e-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {v:k for k,v in tfidf_vec.vocabulary_.items()}\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBi0xEZI0Qoi",
        "outputId": "400e4bf6-5fc7-4fba-a674-efa3eb1c1858"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1086: 'volunteers', 690: 'needed', 404: 'for', 869: 'rise', 1061: 'up', 959: 'stay', 822: 'put', 493: 'home', 855: 'rescue', 375: 'fair', 1095: 'web', 297: 'designer', 1063: 'urban', 43: 'adventures', 515: 'ice', 930: 'skating', 98: 'at', 587: 'lasker', 868: 'rink', 392: 'fight', 447: 'global', 512: 'hunger', 75: 'and', 986: 'support', 1108: 'women', 380: 'farmers', 562: 'join', 1012: 'the', 739: 'oxfam', 31: 'action', 255: 'corps', 523: 'in', 710: 'nyc', 962: 'stop', 989: 'swap', 825: 'queens', 951: 'staff', 300: 'development', 1037: 'trainer', 213: 'claro', 155: 'brooklyn', 1084: 'volunteer', 101: 'attorney', 188: 'cents', 23: 'ability', 235: 'community', 480: 'health', 48: 'advocates', 984: 'supervise', 202: 'children', 491: 'highland', 748: 'park', 433: 'garden', 1118: 'worldofmoney', 727: 'org', 1132: 'youth', 67: 'amazing', 826: 'race', 824: 'qualified', 142: 'board', 649: 'member', 899: 'seats', 106: 'available', 1130: 'young', 38: 'adult', 1052: 'tutor', 1062: 'updated', 13: '30', 0: '11', 538: 'insurance', 212: 'claims', 629: 'manager', 1024: 'timebanksnyc', 456: 'great', 364: 'exchange', 218: 'clean', 89: 'asbury', 184: 'cementary', 958: 'staten', 550: 'island', 908: 'senior', 207: 'citizen', 416: 'friendly', 1080: 'visitor', 920: 'shop', 1045: 'tree', 173: 'care', 1115: 'workshop', 5: '20', 675: 'movie', 895: 'screener', 909: 'seniors', 379: 'farm', 454: 'graphic', 437: 'general', 913: 'services', 473: 'handyman', 721: 'open', 505: 'house', 905: 'seeking', 324: 'drupal', 344: 'end', 788: 'poverty', 166: 'campaign', 440: 'get', 1029: 'tools', 1025: 'to', 841: 'recycling', 377: 'family', 360: 'event', 220: 'clerical', 169: 'cancer', 1088: 'walk', 128: 'befitnyc', 772: 'physical', 35: 'activity', 731: 'organizers', 287: 'decision', 284: 'day', 6: '2011', 691: 'needs', 1131: 'your', 484: 'help', 429: 'gain', 1067: 'valuable', 260: 'counseling', 368: 'experience', 717: 'on', 884: 'samaritans', 11: '24', 504: 'hour', 271: 'crisis', 503: 'hotline', 482: 'heart', 431: 'gallery', 734: 'our', 528: 'info', 991: 'table', 397: 'finding', 496: 'homes', 570: 'kids', 1124: 'yiddish', 943: 'speaking', 497: 'homework', 485: 'helper', 932: 'skilled', 835: 'rebuilding', 1027: 'together', 853: 'repairs', 462: 'greenteam', 45: 'advetures', 982: 'summer', 973: 'streets', 1050: 'tuesday', 359: 'evenings', 1107: 'with', 641: 'masa', 623: 'lunch', 805: 'program', 1064: 'us', 737: 'outreach', 647: 'meals', 795: 'preparedness', 236: 'compost', 808: 'project', 642: 'master', 237: 'composter', 191: 'certificate', 264: 'course', 341: 'emblemhealth', 154: 'bronx', 713: 'of', 912: 'service', 557: 'jcc', 630: 'manhattan', 847: 'registration', 442: 'girl', 894: 'scout', 911: 'series', 316: 'dorot', 876: 'rosh', 477: 'hashanah', 740: 'package', 293: 'delivery', 744: 'painting', 536: 'instructor', 556: 'jasa', 489: 'hes', 185: 'center', 14: '3rd', 77: 'annual', 401: 'flyny', 576: 'kite', 390: 'festival', 1028: 'tomorrow', 161: 'business', 594: 'leaders', 997: 'teach', 118: 'basics', 490: 'high', 891: 'schoolers', 559: 'jingle', 130: 'bell', 879: 'run', 85: 'arthritis', 434: 'gardening', 421: 'ft', 1049: 'tryon', 950: 'st', 639: 'martin', 781: 'poetry', 698: 'new', 1126: 'york', 230: 'college', 448: 'goal', 983: 'sunday', 385: 'february', 7: '2012', 280: 'dance', 9: '22nd', 588: 'latino', 633: 'march', 3: '17', 1059: 'university', 887: 'saturday', 1054: 'tutors', 777: 'planet', 510: 'human', 631: 'mapping', 444: 'give', 1097: 'week', 199: 'child', 597: 'learn', 831: 'read', 967: 'storytelling', 258: 'costume', 626: 'making', 952: 'stage', 296: 'design', 342: 'emergency', 21: '9th', 1100: 'west', 926: 'side', 263: 'county', 706: 'nutrition', 336: 'educator', 918: 'shape', 328: 'east', 15: '54st', 836: 'rec', 1093: 'water', 50: 'aerobics', 91: 'asser', 601: 'levy', 743: 'paint', 64: 'alongside', 818: 'publicolor', 978: 'students', 564: 'jumpstart', 832: 'readers', 592: 'lead', 267: 'crafts', 432: 'games', 372: 'face', 784: 'popcorn', 553: 'jackie', 872: 'robinson', 747: 'parent', 399: 'fitness', 956: 'starrett', 210: 'city', 613: 'line', 281: 'dancer', 644: 'math', 615: 'literacy', 122: 'be', 222: 'climb', 1030: 'top', 637: 'marketing', 94: 'assistant', 335: 'education', 703: 'nonprofit', 906: 'seeks', 840: 'recruitment', 655: 'mentors', 845: 'register', 100: 'attend', 152: 'breakfast', 732: 'orientation', 555: 'january', 778: 'planning', 842: 'red', 273: 'cross', 224: 'clubs', 291: 'deliver', 1105: 'winter', 1078: 'visit', 72: 'an', 551: 'isolated', 366: 'exercise', 227: 'coach', 700: 'night', 123: 'beach', 193: 'change', 84: 'art', 807: 'programs', 243: 'consumer', 814: 'protection', 590: 'law', 617: 'liver', 607: 'life', 593: 'leader', 941: 'soup', 575: 'kitchen', 329: 'eastern', 561: 'john', 680: 'muir', 971: 'street', 1071: 'vendor', 670: 'monthly', 1001: 'team', 391: 'fiesta', 1022: 'throgs', 688: 'neck', 238: 'computer', 998: 'teacher', 595: 'leadership', 259: 'council', 724: 'opportunity', 245: 'conversation', 486: 'helpers', 451: 'grades', 745: 'pantry', 308: 'distribution', 327: 'earth', 1002: 'tech', 1096: 'website', 723: 'opportunities', 216: 'classroom', 916: 'set', 156: 'brush', 573: 'kindness', 1044: 'transportation', 65: 'alternatives', 138: 'bike', 1066: 'valet', 1073: 'video', 333: 'editing', 802: 'professionals', 961: 'stipend', 54: 'after', 890: 'school', 653: 'mentor', 696: 'networking', 147: 'bowling', 422: 'fun', 474: 'harlem', 583: 'lanes', 1125: 'yoga', 942: 'spanish', 726: 'or', 413: 'french', 386: 'feed', 513: 'hungry', 1127: 'yorkers', 16: '55', 720: 'only', 768: 'phone', 114: 'bank', 854: 'representative', 830: 'reach', 735: 'out', 672: 'morris', 483: 'heights', 944: 'special', 165: 'camp', 988: 'susan', 579: 'komen', 277: 'cure', 457: 'greater', 52: 'affiliate', 325: 'dumbo', 87: 'arts', 729: 'organizational', 158: 'budget', 668: 'money', 625: 'makes', 910: 'sense', 1039: 'training', 929: 'site', 1074: 'videographer', 400: 'fly', 162: 'by', 1013: 'theater', 453: 'grant', 1121: 'writer', 813: 'proposal', 794: 'preparation', 423: 'fund', 828: 'raising', 475: 'harm', 843: 'reduction', 40: 'adv', 541: 'intern', 914: 'serving', 603: 'lgbt', 39: 'adults', 507: 'how', 866: 'ride', 139: 'bikes', 856: 'research', 425: 'fundraising', 299: 'developement', 247: 'cook', 878: 'row', 55: 'afterschool', 659: 'middle', 924: 'shower', 424: 'fundraisers', 552: 'it', 545: 'interpreters', 591: 'lawyers', 470: 'haitian', 22: 'abe', 792: 'pre', 436: 'ged', 669: 'monitor', 97: 'astoria', 663: 'million', 1046: 'trees', 445: 'giveaway', 310: 'do', 1128: 'you', 1091: 'want', 624: 'make', 302: 'difference', 217: 'classwish', 936: 'snow', 922: 'shoveling', 209: 'citizenship', 796: 'press', 614: 'list', 88: 'as', 816: 'public', 131: 'benefit', 261: 'counselor', 29: 'aces', 848: 'relations', 776: 'plan', 864: 'review', 418: 'friendship', 787: 'positive', 129: 'beginnings', 574: 'kit', 640: 'mary', 838: 'recreation', 311: 'does', 728: 'organization', 689: 'need', 897: 'search', 969: 'strategy', 355: 'esl', 51: 'affected', 965: 'storm', 1040: 'transform', 618: 'lives', 975: 'strengthen', 234: 'communities', 127: 'become', 323: 'driver', 1072: 'veterans', 204: 'chinese', 1042: 'translator', 537: 'instructors', 683: 'museum', 650: 'membership', 294: 'department', 304: 'director', 125: 'beautify', 1041: 'transitional', 857: 'residence', 495: 'homeless', 652: 'men', 995: 'tank', 543: 'internship', 809: 'projects', 1103: 'wild', 148: 'boys', 500: 'hope', 443: 'girls', 233: 'communications', 827: 'raise', 108: 'awareness', 36: 'administrative', 63: 'alliance', 846: 'registrar', 677: 'ms', 1109: 'word', 174: 'career', 754: 'passover', 348: 'enjoy', 736: 'outdoor', 326: 'early', 201: 'childhood', 159: 'build', 780: 'plastic', 146: 'bottle', 896: 'sculpture', 798: 'pride', 549: 'is', 566: 'just', 83: 'around', 253: 'corner', 546: 'involved', 705: 'now', 414: 'fresh', 60: 'air', 999: 'teachers', 396: 'find', 761: 'perfect', 560: 'job', 714: 'office', 1122: 'writing', 282: 'data', 349: 'entry', 33: 'activism', 771: 'photography', 882: 'salesforce', 283: 'database', 279: 'customization', 769: 'photo', 356: 'essay', 600: 'legal', 47: 'advisor', 492: 'hike', 1019: 'thon', 251: 'coordinator', 586: 'laser', 992: 'tag', 318: 'dowling', 4: '175th', 530: 'information', 1004: 'technology', 376: 'fall', 406: 'forest', 861: 'restoration', 569: 'kickoff', 1047: 'trevor', 610: 'lifeline', 262: 'counselors', 1018: 'thomas', 558: 'jefferson', 643: 'materials', 1123: 'year', 410: 'founder', 365: 'executive', 478: 'haunted', 585: 'lantern', 1034: 'tours', 407: 'fort', 1031: 'totten', 687: 'national', 917: 'sexual', 90: 'assault', 719: 'online', 361: 'events', 1038: 'trainers', 53: 'african', 70: 'american', 223: 'clothing', 322: 'drive', 863: 'returning', 904: 'seeds', 981: 'success', 779: 'plant', 1026: 'today', 467: 'growth', 1055: 'udec', 351: 'enviromedia', 665: 'mobile', 635: 'maritime', 110: 'bacchanal', 775: 'pirates', 389: 'fest', 517: 'ikea', 352: 'erie', 119: 'basin', 301: 'diabetes', 96: 'association', 388: 'feria', 285: 'de', 883: 'salud', 694: 'nepali', 113: 'bangla', 819: 'punjabi', 1043: 'translators', 402: 'food', 1007: 'tent', 704: 'not', 804: 'profit', 774: 'pioneer', 171: 'capoeira', 1069: 'various', 786: 'positions', 307: 'dispatcher', 1036: 'trainee', 531: 'ing', 632: 'marathon', 412: 'free', 621: 'love', 144: 'books', 286: 'dear', 104: 'authors', 59: 'aide', 889: 'scheuer', 656: 'merchandise', 313: 'donate', 985: 'supplies', 384: 'feast', 430: 'gala', 120: 'battery', 937: 'soccer', 426: 'futsal', 762: 'performing', 41: 'advanced', 215: 'classes', 1117: 'world', 893: 'science', 1101: 'western', 71: 'americorps', 332: 'economic', 903: 'security', 532: 'initiative', 354: 'esi', 662: 'mill', 186: 'centers', 660: 'midtown', 1135: 'zumba', 1077: 'vision', 664: 'mission', 73: 'analysis', 580: 'lab', 1000: 'teaching', 506: 'housing', 1114: 'works', 303: 'dime', 92: 'assist', 862: 'resume', 160: 'building', 939: 'society', 228: 'coaches', 1087: 'vs', 232: 'committee', 880: 'russian', 409: 'foster', 183: 'celebration', 645: 'may', 8: '21th', 718: 'one', 742: 'pager', 314: 'donation', 514: 'hurricane', 547: 'irene', 378: 'far', 873: 'rockaway', 1113: 'working', 716: 'olympics', 1033: 'tournament', 833: 'reading', 750: 'partners', 249: 'cooper', 949: 'square', 1020: 'thrift', 948: 'spring', 179: 'case', 628: 'management', 428: 'fvcp', 1035: 'trail', 270: 'crew', 471: 'halloween', 177: 'carnival', 1089: 'walkathon', 383: 'feasibility', 74: 'analyst', 782: 'police', 907: 'seminar', 1111: 'work', 1082: 'visually', 521: 'impaired', 1006: 'teens', 1017: 'this', 345: 'energy', 337: 'efficiency', 898: 'season', 132: 'benefits', 837: 'reception', 321: 'drill', 252: 'copywriting', 250: 'coord', 479: 'have', 757: 'penchant', 62: 'all', 1015: 'things', 1075: 'vintage', 1021: 'thriftshop', 676: 'moving', 963: 'storage', 749: 'partner', 758: 'pencil', 751: 'partnership', 741: 'packing', 927: 'sign', 276: 'cuny', 20: '8th', 947: 'sports', 370: 'expo', 176: 'cares', 197: 'cheerleaders', 1092: 'wanted', 469: 'habitat', 395: 'finance', 229: 'coffee', 347: 'english', 790: 'practice', 598: 'learners', 481: 'healthy', 32: 'active', 1023: 'time', 80: 'april', 381: 'fashion', 970: 'strawberry', 95: 'assistants', 269: 'creative', 1016: 'thinkers', 187: 'central', 1134: 'zoo', 1: '125th', 136: 'bideawee', 464: 'greeters', 620: 'looking', 834: 'real', 520: 'impact', 529: 'inform', 760: 'people', 791: 'practices', 608: 'lifebeat', 974: 'streetsquash', 306: 'discovery', 693: 'neighborhood', 803: 'profiles', 993: 'take', 955: 'stand', 56: 'against', 1076: 'violence', 369: 'expert', 46: 'advice', 565: 'june', 888: 'schedule', 275: 'crowdfunding', 759: 'penny', 476: 'harvest', 458: 'green', 198: 'chefs', 707: 'nutritionists', 403: 'foodies', 654: 'mentoring', 145: 'boom', 699: 'newsletter', 231: 'come', 976: 'strides', 1090: 'walks', 200: 'childcare', 938: 'social', 648: 'media', 446: 'giving', 168: 'can', 68: 'ambassador', 12: '2nd', 1010: 'thanksgiving', 387: 'feeding', 692: 'needy', 817: 'publicity', 755: 'patient', 175: 'caregiver', 1079: 'visiting', 494: 'homebound', 382: 'fc', 709: 'nyawc', 408: 'forum', 25: 'about', 1085: 'volunteering', 844: 'refreshments', 886: 'sara', 875: 'roosevelt', 219: 'cleanup', 124: 'beautification', 76: 'animal', 509: 'hudson', 870: 'river', 634: 'mariners', 860: 'response', 367: 'exhibit', 24: 'aboard', 612: 'lilac', 221: 'client', 1099: 'welcome', 298: 'desk', 715: 'older', 871: 'riverbank', 874: 'roller', 602: 'lexington', 266: 'craft', 783: 'poll', 1112: 'workers', 544: 'interperters', 28: 'accounting', 93: 'assistance', 502: 'hosting', 811: 'promotion', 1057: 'unicef', 996: 'tap', 10: '23', 849: 'release', 289: 'dedication', 806: 'programming', 525: 'incarnation', 315: 'donor', 563: 'journalism', 572: 'kieran', 946: 'sponsorship', 881: 'sag', 435: 'gear', 1070: 'vehicle', 1116: 'workshops', 126: 'because', 362: 'every', 295: 'deserves', 192: 'chance', 793: 'prep', 773: 'pin', 292: 'delivered', 925: 'shred', 18: '5th', 107: 'avenue', 182: 'cdsc', 957: 'starving', 86: 'artist', 923: 'show', 990: 'system', 420: 'front', 919: 'share', 581: 'lanch', 977: 'student', 488: 'hemophilia', 605: 'liason', 658: 'methodist', 501: 'hospital', 121: 'bay', 867: 'ridge', 133: 'benonhurst', 82: 'area', 940: 'sought', 105: 'autistic', 317: 'douglaston', 823: 'qns', 37: 'administrator', 163: 'call', 450: 'governor', 839: 'recruiter', 821: 'purim', 350: 'envelope', 980: 'stuffing', 785: 'population', 357: 'estimate', 554: 'jam', 151: 'break', 319: 'down', 167: 'campaigner', 487: 'helpline', 964: 'store', 398: 'first', 439: 'generation', 1068: 'van', 256: 'cortlandt', 851: 'remembrance', 987: 'survey', 858: 'resonations', 153: 'breast', 346: 'engine', 725: 'optimization', 651: 'memorial', 934: 'sloan', 568: 'kettering', 459: 'greenhouse', 460: 'greening', 241: 'concert', 358: 'evacuation', 178: 'carpentry', 540: 'interior', 859: 'resources', 441: 'gift', 135: 'bicycling', 686: 'my', 417: 'friends', 498: 'honor', 1098: 'weekend', 763: 'person', 681: 'mural', 248: 'cooking', 334: 'editor', 764: 'personal', 921: 'shopper', 799: 'pro', 143: 'bono', 268: 'create', 172: 'cards', 960: 'step', 702: 'non', 815: 'provider', 542: 'interns', 674: 'motion', 455: 'graphics', 134: 'best', 157: 'buddies', 527: 'inern', 111: 'back', 616: 'little', 257: 'cosmetologist', 115: 'barber', 1083: 'vocational', 79: 'apartment', 463: 'greeter', 801: 'professional', 1065: 'use', 933: 'skills', 733: 'others', 393: 'figure', 274: 'croton', 170: 'cancercare', 2: '14th', 339: 'eif', 865: 'revlon', 203: 'chinatown', 1014: 'therapeutic', 58: 'aid', 34: 'activities', 206: 'ci', 254: 'corporate', 1110: 'wordpress', 141: 'blog', 535: 'instructer', 499: 'hook', 309: 'divert', 1009: 'textiles', 419: 'from', 582: 'landfill', 461: 'greenmarket', 1008: 'textile', 164: 'calling', 208: 'citizens', 522: 'improve', 30: 'achievement', 753: 'passion', 524: 'inc', 465: 'group', 320: 'drama', 17: '5k', 589: 'laundromats', 343: 'employment', 968: 'strategic', 697: 'never', 112: 'bad', 415: 'friend', 427: 'future', 214: 'class', 1106: 'wish', 411: 'fpcj', 1119: 'worship', 1056: 'undergraduate', 452: 'graduate', 242: 'conference', 1094: 'we', 810: 'promote', 578: 'knowledge', 746: 'parade', 81: 'archivist', 449: 'google', 49: 'adwords', 518: 'imentor', 671: 'more', 627: 'male', 661: 'miles', 666: 'moms', 196: 'charity', 189: 'century', 1032: 'tour', 211: 'civil', 756: 'patrol', 69: 'america', 567: 'kept', 901: 'secret', 678: 'ms131', 577: 'knitter', 272: 'crochet', 140: 'blankets', 190: 'ceo', 619: 'logo', 1058: 'unique', 1104: 'will', 137: 'big', 42: 'adventure', 27: 'accountant', 915: 'session', 57: 'age', 928: 'single', 673: 'mothers', 205: 'choice', 935: 'smc', 1102: 'wii', 701: 'nights', 636: 'market', 539: 'intake', 667: 'monday', 150: 'branding', 149: 'brand', 516: 'identity', 679: 'mt', 1133: 'zion', 571: 'kidz', 852: 'reorganize', 606: 'library', 99: 'athletic', 596: 'league', 1051: 'turtle', 684: 'music', 288: 'decorating', 752: 'party', 685: 'musician', 66: 'alzheimer', 117: 'bash', 800: 'proctor', 994: 'taking', 363: 'exams', 812: 'promotional', 765: 'personnel', 103: 'august', 722: 'operations', 931: 'skill', 695: 'networker', 331: 'ecological', 820: 'puppet', 526: 'income', 438: 'generating', 730: 'organizations', 622: 'lp', 972: 'streetfest', 19: '7th', 265: 'cpr', 604: 'lgbtq', 340: 'el', 682: 'museo', 290: 'del', 116: 'barrio', 657: 'met', 766: 'petition', 353: 'escort', 885: 'sand', 180: 'castle', 244: 'contest', 892: 'schools', 511: 'humanities', 900: 'second', 584: 'language', 109: 'babies', 1005: 'teen', 61: 'al', 712: 'oerter', 508: 'html', 278: 'curriculum', 770: 'photographer', 902: 'secretary', 789: 'pr', 1120: 'would', 611: 'like', 239: 'computers', 1003: 'technical', 466: 'grownyc', 1011: 'that', 371: 'extraordinary', 405: 'foreclosure', 797: 'prevention', 711: 'nylag', 708: 'ny', 240: 'concern', 534: 'inspire', 26: 'academic', 1053: 'tutoring', 829: 'rbi', 78: 'anyone', 225: 'cma', 226: 'cms', 246: 'conversion', 330: 'eating', 599: 'learning', 194: 'chaperones', 1081: 'visits', 1060: 'unlimited', 609: 'lifeguard', 374: 'facilitators', 1048: 'troop', 877: 'route', 638: 'marshall', 533: 'inmotion', 966: 'story', 953: 'stair', 312: 'domestic', 181: 'catskills', 850: 'relief', 338: 'effort', 102: 'audience', 767: 'pharmacy', 468: 'guide', 738: 'overnight', 519: 'immediate', 305: 'dirty', 472: 'hands', 373: 'facilitator', 945: 'specialist', 195: 'chapter', 954: 'stamps', 548: 'iridescent', 979: 'studio', 44: 'advertising', 394: 'filmmakers', 646: 'mayor', 1129: 'youcantoo'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zipped_row = dict(zip(text_tfidf[3].indices, text_tfidf[3].data))\n",
        "print(zipped_row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKXHOYbx0ldv",
        "outputId": "a3bc776e-01f0-4bca-e4c5-c4035bf33bae"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{710: 0.2056446046212042, 523: 0.18207403700074992, 255: 0.295432987618702, 31: 0.31414199966461376, 739: 0.31414199966461376, 1012: 0.16847136678616276, 562: 0.2563368840096324, 380: 0.31414199966461376, 1108: 0.2718624199982478, 986: 0.2243536166671159, 75: 0.16380657953192443, 512: 0.31414199966461376, 447: 0.31414199966461376, 392: 0.31414199966461376}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Define a function to return weights***"
      ],
      "metadata": {
        "id": "MAkrmGxE1Aec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def return_weights(vocab, vector, vector_index):\n",
        "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
        "    return {vocab[i]:zipped[i] for i in vector[vector_index].indices}\n",
        "    \n",
        "print(return_weights(vocab, text_tfidf, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVlGoED405u_",
        "outputId": "9248cb28-0d5a-46e3-e1d7-d216d51e990d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'nyc': 0.2056446046212042, 'in': 0.18207403700074992, 'corps': 0.295432987618702, 'action': 0.31414199966461376, 'oxfam': 0.31414199966461376, 'the': 0.16847136678616276, 'join': 0.2563368840096324, 'farmers': 0.31414199966461376, 'women': 0.2718624199982478, 'support': 0.2243536166671159, 'and': 0.16380657953192443, 'hunger': 0.31414199966461376, 'global': 0.31414199966461376, 'fight': 0.31414199966461376}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in text_tfidf[3].indices:\n",
        "    print(vocab[i], end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRF2bDsA1bsJ",
        "outputId": "c12384a7-57b8-4323-94e8-3199ea05d391"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nyc in corps action oxfam the join farmers women support and hunger global fight "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Exploring text vectors, part 1***\n",
        "\n",
        "Let's expand on the text vector exploration method we just learned about, using the `volunteer` dataset's `title` tf/idf vectors. In this first part of text vector exploration, we're going to add to that function we learned about in the slides. We'll return a list of numbers with the function. In the next exercise, we'll write another function to collect the top words across all documents, extract them, and then use that list to filter down our `text_tfidf` vector.\n",
        "\n",
        "* Add parameters called `original_vocab`, for the **`tfidf_vec.vocabulary_`**, and `top_n`.\n",
        "\n",
        "* Call **`pd.Series`** on the zipped dictionary. This will make it easier to operate on.\n",
        "\n",
        "* Use the **`.sort_values`** function to sort the series and slice the index up to `top_n` words.\n",
        "\n",
        "* Call the function, setting `original_vocab=tfidf_vec.vocabulary_`, setting `vector_index=8` to grab the 9th row, and setting `top_n=3`, to grab the top 3 weighted words."
      ],
      "metadata": {
        "id": "x5Y8m0Sp3QXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "volunteer = pd.read_csv('volunteer_opportunities.csv')\n",
        "title_text = volunteer[\"title\"]\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
        "vocab = {v:k for k,v in tfidf_vec.vocabulary_.items()}\n",
        "\n",
        "# Add in the rest of the parameters\n",
        "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
        "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
        "    print(\"zipped = \\n\", zipped, \"\\n\\n\")\n",
        "    # Let's transform that zipped dict into a series\n",
        "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
        "    print(\"zipped_series = \\n\", zipped_series.head(),\"\\n\\n\")\n",
        "    # Let's sort the series to pull out the top n weighted words\n",
        "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
        "    print(\"zipped_series.sort_values(ascending=False)[:top_n] = \\n\", zipped_series.sort_values(ascending=False)[:top_n], \"\\n\\n\")\n",
        "    print(\"zipped_index = \\n\", zipped_index, \"\\n\\n\")\n",
        "    return [original_vocab[i] for i in zipped_index]\n",
        "\n",
        "# Print out the weighted words\n",
        "print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, vector_index=10, top_n=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTRQvc-71taV",
        "outputId": "d87247e3-0abe-486d-de83-2861e4cb5f56"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zipped = \n",
            " {433: 0.32882242445111365, 748: 0.2729198250303222, 491: 0.37897391016100135, 202: 0.6758931792190142, 984: 0.4029733541886919, 523: 0.23355993620464943} \n",
            "\n",
            "\n",
            "zipped_series = \n",
            " garden       0.328822\n",
            "park         0.272920\n",
            "highland     0.378974\n",
            "children     0.675893\n",
            "supervise    0.402973\n",
            "dtype: float64 \n",
            "\n",
            "\n",
            "zipped_series.sort_values(ascending=False)[:top_n] = \n",
            " children     0.675893\n",
            "supervise    0.402973\n",
            "highland     0.378974\n",
            "dtype: float64 \n",
            "\n",
            "\n",
            "zipped_index = \n",
            " Index(['children', 'supervise', 'highland'], dtype='object') \n",
            "\n",
            "\n",
            "[202, 984, 491]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfidf_vec.vocabulary_['children'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgODNDcx86Jw",
        "outputId": "5f7e0784-b742-48dc-ba72-25d0cdcf39d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Exploring text vectors, part 2***\n",
        "\n",
        "Using the function we wrote in the previous exercise, we're going to extract the top words from each document in the text vector, return a list of the word indices, and use that list to filter the text vector down to those top words.\n",
        "\n",
        "* Call `return_weights` to return the top weighted words for that document.\n",
        "\n",
        "* Call `set` on the returned `filter_list` so we don't get duplicated numbers.\n",
        "\n",
        "* Call `words_to_filter`, passing in the following parameters: \n",
        "  * `vocab` for the `vocab` parameter, \n",
        "  * `tfidf_vec.vocabulary_` for the `original_vocab` parameter, \n",
        "  * `text_tfidf` for the vector parameter, and \n",
        "  * `3` to grab the `top_n` 3 weighted words from each document.\n",
        "\n",
        "Finally, pass that `filtered_words` set into a list to use as a filter for the text vector."
      ],
      "metadata": {
        "id": "3FzhPmXABSjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "volunteer = pd.read_csv('volunteer_opportunities.csv')\n",
        "title_text = volunteer[\"title\"]\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
        "vocab = {v:k for k,v in tfidf_vec.vocabulary_.items()}\n",
        "\n",
        "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
        "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
        "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
        "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
        "    return [original_vocab[i] for i in zipped_index]\n",
        "\n",
        "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
        "    filter_list = []\n",
        "    for i in range(0, vector.shape[0]):\n",
        "    \n",
        "        # Here we'll call the function from the previous exercise, and extend the list we're creating\n",
        "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
        "        filter_list.extend(filtered)\n",
        "    # Return the list in a set, so we don't get duplicate word indices\n",
        "    return set(filter_list)\n",
        "\n",
        "# Call the function to get the list of word indices\n",
        "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
        "\n",
        "# By converting filtered_words back to a list, we can use it to filter the columns in the text vector\n",
        "filtered_text = text_tfidf[:, list(filtered_words)]\n",
        "print(filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3btBXag99jW8",
        "outputId": "fe370eba-f5aa-4696-ca6f-89917ba3b21f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 344)\t0.3163915503784279\n",
            "  (0, 795)\t0.38192461589865456\n",
            "  (0, 455)\t0.3405778550191958\n",
            "  (0, 764)\t0.38192461589865456\n",
            "  (0, 894)\t0.38192461589865456\n",
            "  (0, 989)\t0.25544926998167106\n",
            "  (0, 372)\t0.15529778130809513\n",
            "  (0, 639)\t0.24072387702158726\n",
            "  (0, 1013)\t0.2304728774077965\n",
            "  (1, 271)\t0.6824588570832413\n",
            "  (1, 1021)\t0.7309240099960025\n",
            "  (2, 807)\t0.3903159429478348\n",
            "  (2, 539)\t0.4150336487706769\n",
            "  (2, 84)\t0.2225785986760572\n",
            "  (2, 866)\t0.35917531643639333\n",
            "  (2, 35)\t0.4150336487706769\n",
            "  (2, 991)\t0.4150336487706769\n",
            "  (3, 657)\t0.2056446046212042\n",
            "  (3, 23)\t0.31414199966461376\n",
            "  (3, 686)\t0.31414199966461376\n",
            "  (3, 944)\t0.16847136678616276\n",
            "  (3, 517)\t0.2563368840096324\n",
            "  (3, 349)\t0.31414199966461376\n",
            "  (3, 1034)\t0.2718624199982478\n",
            "  (3, 920)\t0.2243536166671159\n",
            "  (4, 922)\t0.7071067811865475\n",
            "  (4, 897)\t0.7071067811865475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Training Naive Bayes with feature selection***\n",
        "\n",
        "Let's re-run the Naive Bayes text classification model we ran at the end of chapter 3, with our selection choices from the previous exercise, on the `volunteer` dataset's `title` and `category_desc` columns.\n",
        "\n",
        "* Use `train_test_split` on the `filtered_text` text vector, the `y` labels (which is the `category_desc` labels), and pass the `y` set to the `stratify` parameter, since we have an uneven class distribution.\n",
        "\n",
        "* Fit the `nb` Naive Bayes model to `train_X` and `train_y`.\n",
        "\n",
        "* Score the `nb` model on the `test_X` and `test_y` test sets."
      ],
      "metadata": {
        "id": "z7KQTGijDIvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb = GaussianNB(priors=None)\n",
        "volunteer = volunteer[[\"title\", 'category_desc']].dropna()\n",
        "title_text = volunteer[\"title\"]\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
        "vocab = {v:k for k,v in tfidf_vec.vocabulary_.items()}\n",
        "y = volunteer['category_desc']\n",
        "\n",
        "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
        "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
        "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
        "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
        "    return [original_vocab[i] for i in zipped_index]\n",
        "\n",
        "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
        "    filter_list = []\n",
        "    for i in range(0, vector.shape[0]):\n",
        "    \n",
        "        # Here we'll call the function from the previous exercise, and extend the list we're creating\n",
        "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
        "        filter_list.extend(filtered)\n",
        "    # Return the list in a set, so we don't get duplicate word indices\n",
        "    return set(filter_list)\n",
        "\n",
        "# Call the function to get the list of word indices\n",
        "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
        "\n",
        "# By converting filtered_words back to a list, we can use it to filter the columns in the text vector\n",
        "filtered_text = text_tfidf[:, list(filtered_words)]\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "# Split the dataset according to the class distribution of category_desc, using the filtered_text vector\n",
        "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n",
        "\n",
        "# Fit the model to the training data\n",
        "nb.fit(train_X, train_y)\n",
        "\n",
        "# Print out the model's accuracy\n",
        "print(nb.score(test_X, test_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv5XBj6XCu_O",
        "outputId": "945690a8-8c01-4a84-f819-f8f21c6a793e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5161290322580645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that our accuracy score wasn't that different from the score previously. That's okay; the title field is a very small text field, appropriate for demonstrating how filtering vectors works.\n",
        "\n",
        "# ***PCA in scikit-learn***"
      ],
      "metadata": {
        "id": "KcaLq_6VE1Pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "df_pca = pca.fit_transform(df)\n",
        "\n",
        "# ดูว่าตัวแปรใด มีผลต่อความแปรปรวนอย่างไร\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "aPGj1b7eEhw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[0.9981, 0.0017, 0.0001, 0.0001, ...]`"
      ],
      "metadata": {
        "id": "H6T7X5eRHUa3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ความแปรปรวน ส่วนมากถูกอธิบายด้วยตัวแปรที่ 1 ดังนั้น ตัวแปรอื่นๆ อาจจะตัดออกไปได้\n",
        "\n",
        "### ***Using PCA***\n",
        "\n",
        "Let's apply PCA to the `wine` dataset, to see if we can get an increase in our model's accuracy.\n",
        "\n",
        "* Set up the **`PCA`** object. You'll use **`PCA`** on the `wine` dataset minus its label for `Type`, stored in the variable `wine_X`.\n",
        "\n",
        "* Apply **`PCA`** to `wine_X` using **`pca`**'s **`.fit_transform`** method and store the transformed vector in `transformed_X`.\n",
        "\n",
        "* Print out the `explained_variance_ratio_` attribute of `pca` to check how much variance is explained by each component."
      ],
      "metadata": {
        "id": "dkZaVLpvHjSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "wine = pd.read_csv('wine_types.csv')\n",
        "\n",
        "# Set up PCA and the X vector for diminsionality reduction\n",
        "pca = PCA()\n",
        "wine_X = wine.drop(\"Type\", axis=1)\n",
        "\n",
        "# Apply PCA to the wine dataset X vector\n",
        "transformed_X = pca.fit_transform(wine_X)\n",
        "\n",
        "# Look at the percentage of variance explained by the different components\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw8S3N8nHY80",
        "outputId": "bd8db622-d1d7-4609-e94f-7d13d53b1fcf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9.98091230e-01 1.73591562e-03 9.49589576e-05 5.02173562e-05\n",
            " 1.23636847e-05 8.46213034e-06 2.80681456e-06 1.52308053e-06\n",
            " 1.12783044e-06 7.21415811e-07 3.78060267e-07 2.12013755e-07\n",
            " 8.25392788e-08]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Training a model with PCA***\n",
        "\n",
        "Now that we have run **`PCA`** on the `wine` dataset, let's try training a model with it.\n",
        "\n",
        "* Split the `transformed_X` vector and the `y` labels set into training and test sets using **`train_test_split`**.\n",
        "\n",
        "* Fit the **`knn`** model using the **`.fit()`** function on the `X_wine_train` and `y_wine_train` sets.\n",
        "\n",
        "* Print out the score using **`knn`**'s **`.score()`** function on `X_wine_test` and `y_wine_test`."
      ],
      "metadata": {
        "id": "-9S9t5-oJk9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=4)\n",
        "y = wine['Type']\n",
        "\n",
        "# Split the transformed X and the y labels into training and test sets\n",
        "X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(transformed_X, y)\n",
        "\n",
        "# Fit knn to the training data\n",
        "knn.fit(X_wine_train, y_wine_train)\n",
        "\n",
        "# Score knn on the test data and print it out\n",
        "knn.score(X_wine_test, y_wine_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXRJkkIoJdK-",
        "outputId": "1f74e7bb-63ec-4182-a452-337639e7406f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6444444444444445"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Checking column types***\n",
        "\n",
        "Take a look at the UFO dataset's column types using the **`dtypes`** attribute. \n",
        "Two columns jump out for transformation: the `seconds` column, which is a numeric column but is being read in as object, and the `date` column, which can be transformed into the **`datetime`** type. That will make our feature engineering efforts easier later on.\n",
        "\n",
        "* Print out the **`dtypes`** of the `ufo` dataset.\n",
        "\n",
        "* Change the type of the `seconds` column by passing the **`float`** type into the **`.astype()`** method.\n",
        "\n",
        "* Change the type of the `date` column by passing `ufo[\"date\"]` into the **`pd.to_datetime()`** function.\n",
        "\n",
        "* Print out the **`dtypes`** of the seconds and `date` columns, to make sure it worked."
      ],
      "metadata": {
        "id": "8Yi1uGgpLPir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ufo = pd.read_csv('ufo_sightings_large.csv')\n",
        "\n",
        "# Check the column types\n",
        "print(ufo.dtypes)\n",
        "\n",
        "# Change the type of seconds to float\n",
        "ufo[\"seconds\"] = ufo[\"seconds\"].astype(float)\n",
        "\n",
        "# Change the date column to type datetime\n",
        "ufo[\"date\"] = pd.to_datetime(ufo[\"date\"])\n",
        "\n",
        "# Check the column types\n",
        "print(ufo[['seconds', 'date']].dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1NYbMhMKyJ7",
        "outputId": "ebc03c92-4f5c-4976-9e05-b252af0600a7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date               object\n",
            "city               object\n",
            "state              object\n",
            "country            object\n",
            "type               object\n",
            "seconds           float64\n",
            "length_of_time     object\n",
            "desc               object\n",
            "recorded           object\n",
            "lat                object\n",
            "long              float64\n",
            "dtype: object\n",
            "seconds           float64\n",
            "date       datetime64[ns]\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Dropping missing data***\n",
        "\n",
        "Let's remove some of the rows where certain columns have missing values. We're going to look at the `length_of_time` column, the `state` column, and the `type` column. If any of the values in these columns are missing, we're going to drop the rows.\n",
        "\n",
        "* Check how many values are missing in the `length_of_time`, `state`, and `type` columns, using **`.isnull()`** to check for nulls and **`.sum()`** to calculate how many exist.\n",
        "\n",
        "* Use boolean indexing to filter out the rows with those missing values, using **`.notnull()`** to check the column. Here, we can chain together each column we want to check.\n",
        "\n",
        "* Print out the **`shape`** of the new `ufo_no_missing` dataset."
      ],
      "metadata": {
        "id": "vppt5vRoPB_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ufo = pd.read_csv('ufo_sightings_large.csv')[['date', 'state', 'type', 'length_of_time']]\n",
        "\n",
        "# Check how many values are missing in the length_of_time, state, and type columns\n",
        "print(ufo[['length_of_time', 'state', 'type']].isnull().sum())\n",
        "\n",
        "# Keep only rows where length_of_time, state, and type are not null\n",
        "ufo_no_missing = ufo[ufo['length_of_time'].notnull() & ufo['state'].notnull() & ufo['type'].notnull()]\n",
        "\n",
        "# Print out the shape of the new dataset\n",
        "print(ufo_no_missing.shape)\n",
        "\n",
        "# ใช้ ufo.dropna().shape ง่ายกว่า"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKqFYbduOQ5I",
        "outputId": "6e741f94-65f5-4133-c1e4-14fe02782c36"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length_of_time    143\n",
            "state             419\n",
            "type              159\n",
            "dtype: int64\n",
            "(4283, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Extracting numbers from strings***\n",
        "\n",
        "The `length_of_time` field in the UFO dataset is a text field that has the number of minutes within the string. Here, you'll extract that number from that text field using regular expressions.\n",
        "\n",
        "* Pass `\\d+` into **`re.compile()`** in the `pattern` variable to designate that we want to grab as many digits as possible from the string.\n",
        "\n",
        "* Into **`re.match()`**, pass the `pattern` we just created, as well as the `time_string` we want to extract from.\n",
        "\n",
        "* Use **`lambda`** within the **`.apply()`** method to perform the extraction.\n",
        "\n",
        "* Print out the **`.head()`** of both the `length_of_time` and minutes columns to compare."
      ],
      "metadata": {
        "id": "i2epvnsaSwkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ufo = pd.read_csv('ufo_sightings_large.csv').dropna()\n",
        "ufo = ufo[ufo[\"length_of_time\"].str.contains('minutes')]\n",
        "\n",
        "def return_minutes(time_string):\n",
        "\n",
        "    # Use \\d+ to grab digits\n",
        "    pattern = re.compile(r\"\\d+\")\n",
        "    \n",
        "    # Use match on the pattern and column\n",
        "    num = re.match(pattern, time_string)\n",
        "    if num is not None:\n",
        "        return int(num.group(0))\n",
        "        \n",
        "# Apply the extraction to the length_of_time column\n",
        "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)\n",
        "\n",
        "# Take a look at the head of both of the columns\n",
        "print(ufo[[\"length_of_time\", \"minutes\"]].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn9LaS3JQ9-r",
        "outputId": "dae065ce-28f7-45b6-f871-9b8edc9d38de"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     length_of_time  minutes\n",
            "3   about 5 minutes      NaN\n",
            "5        10 minutes     10.0\n",
            "8         2 minutes      2.0\n",
            "9         2 minutes      2.0\n",
            "10        5 minutes      5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, we end up with some `NaN`s in the DataFrame. That's okay for now; we'll take care of those before modeling.\n",
        "\n",
        "### ***Identifying features for standardization***\n",
        "\n",
        "In this section, you'll investigate the variance of columns in the UFO dataset to determine which features should be standardized. After taking a look at the variances of the `seconds` and `minutes` column, you'll see that the variance of the `seconds` column is extremely high. Because `seconds` and `minutes` are related to each other (an issue we'll deal with when we select features for modeling), let's log normlize the `seconds` column.\n",
        "\n",
        "* Use the **`.var()`** method on the `seconds` and `minutes` columns to check the variance. Notice how high the variance is on the `seconds` column.\n",
        "\n",
        "* Using **`np.log()`** to perform log normalization on the `seconds` column, transforming it into a new column named `seconds_log`.\n",
        "\n",
        "* Print out the variance of the `seconds_log` column."
      ],
      "metadata": {
        "id": "O6myIsGjVVhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the variance of the seconds and minutes columns\n",
        "print(ufo[['seconds', 'minutes']].var())\n",
        "\n",
        "# Log normalize the seconds column\n",
        "ufo[\"seconds_log\"] = np.log(ufo['seconds'])\n",
        "\n",
        "# Print out the variance of just the seconds_log column\n",
        "print(ufo[\"seconds_log\"].var())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcKiy3KaUoFf",
        "outputId": "0d3aba4f-6f5b-4b51-d3a9-ecfd6f0bb747"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seconds    433229.169958\n",
            "minutes       119.159917\n",
            "dtype: float64\n",
            "0.8391915489580166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Encoding categorical variables***\n",
        "\n",
        "There are couple of columns in the UFO dataset that need to be encoded before they can be modeled through scikit-learn. You'll do that transformation here, using both binary and one-hot encoding methods.\n",
        "\n",
        "* Using **`.apply()`**, write a **`lambda`** that returns a `1` if the value is `us`, else return `0`. \n",
        "\n",
        "* Next, print out the number of **`unique()`** values of the `type` column.\n",
        "\n",
        "* Using **`pd.get_dummies()`**, create a one-hot encoded set of the `type` column.\n",
        "\n",
        "* Finally, use **`pd.concat()`** to concatenate the `ufo` dataset to the `type_set` encoded variables."
      ],
      "metadata": {
        "id": "lVG1NLxwSlZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ufo = pd.read_csv('ufo_sightings_large.csv')\n",
        "\n",
        "# Use Pandas to encode us values as 1 and others as 0\n",
        "ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda x: 1 if x == 'us' else 0)\n",
        "\n",
        "# Print the number of unique type values\n",
        "print(len(ufo['type'].unique()))\n",
        "\n",
        "# Create a one-hot encoded set of the type values\n",
        "type_set = pd.get_dummies(ufo['type'])\n",
        "\n",
        "# Concatenate this set back to the ufo DataFrame\n",
        "ufo = pd.concat([ufo, type_set], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp_bG3lbWrE8",
        "outputId": "b8708b8f-5644-41f5-ca9e-437233954a8c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Features from dates***\n",
        "\n",
        "Another feature engineering task to perform is month and year extraction. Perform this task on the date column of the `ufo` dataset.\n",
        "\n",
        "* Print out the **`.head()`** of the `date` column.\n",
        "\n",
        "* Using **`.apply()`**, **`lambda`**, and the **`.month`** attribute, extract the month from the `date` column.\n",
        "\n",
        "* Using **`apply()`**, **`lambda`**, and the **`.year`** attribute, extract the year from the `date` column.\n",
        "\n",
        "* Take a look at the **`.head()`** of the `date`, `month`, and `year` columns."
      ],
      "metadata": {
        "id": "jkbITFyLUM18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ufo['date'] = pd.to_datetime(ufo['date'])\n",
        "\n",
        "# Look at the first 5 rows of the date column\n",
        "print(ufo['date'].head())\n",
        "\n",
        "# Extract the month from the date column\n",
        "ufo[\"month\"] = ufo[\"date\"].apply(lambda x: x.month)\n",
        "\n",
        "# Extract the year from the date column\n",
        "ufo[\"year\"] = ufo[\"date\"].apply(lambda x: x.year)\n",
        "\n",
        "# Take a look at the head of all three columns\n",
        "print(ufo[['date', 'month', 'year']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3NRM83CTunU",
        "outputId": "0f7fa976-97aa-4f1e-f56f-4d1b6faf049d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0   2011-11-03 19:21:00\n",
            "1   2004-10-03 19:05:00\n",
            "2   2009-09-25 21:00:00\n",
            "3   2002-11-21 05:45:00\n",
            "4   2010-08-19 12:55:00\n",
            "Name: date, dtype: datetime64[ns]\n",
            "                 date  month  year\n",
            "0 2011-11-03 19:21:00     11  2011\n",
            "1 2004-10-03 19:05:00     10  2004\n",
            "2 2009-09-25 21:00:00      9  2009\n",
            "3 2002-11-21 05:45:00     11  2002\n",
            "4 2010-08-19 12:55:00      8  2010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "` 'apply'` and `'lambda'` are extremely useful for extraction tasks.\n",
        "\n",
        "### ***Text vectorization***\n",
        "\n",
        "Let's transform the `desc` column in the UFO dataset into tf/idf vectors, since there's likely something we can learn from this field.\n",
        "\n",
        "* Print out the **`.head()`** of the `ufo[\"desc\"]` column.\n",
        "\n",
        "* Set `vec` equal to the **`TfidfVectorizer()`** object.\n",
        "\n",
        "* Use `vec`'s **`.fit_transform()`** method on the `ufo[\"desc\"]` column.\n",
        "\n",
        "* Print out the **`shape`** of the `desc_tfidf` vector, to take a look at the number of columns this created. The output is in the shape (rows, columns)."
      ],
      "metadata": {
        "id": "MFTh-XluVSqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = ufo['desc'].drop([2650, 3856, 4538])\n",
        "\n",
        "# Take a look at the head of the desc field\n",
        "print(a.head())\n",
        "\n",
        "# Create the tfidf vectorizer object\n",
        "vec = TfidfVectorizer()\n",
        "\n",
        "# Use vec's fit_transform method on the desc field\n",
        "desc_tfidf = vec.fit_transform(a)\n",
        "\n",
        "# Look at the number of columns this creates\n",
        "print(desc_tfidf.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfL_-Jt3VEr3",
        "outputId": "b5c70ad6-f58a-4c46-b585-8499b53d9229"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    Red blinking objects similar to airplanes or s...\n",
            "1                 Many fighter jets flying towards UFO\n",
            "2    Green&#44 red&#44 and blue pulses of light tha...\n",
            "3    It was a large&#44 triangular shaped flying ob...\n",
            "4       A white spinning disc in the shape of an oval.\n",
            "Name: desc, dtype: object\n",
            "(4932, 6433)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text vector has a large number of columns.\n",
        "\n",
        "### ***Selecting the ideal dataset***\n",
        "\n",
        "Let's get rid of some of the unnecessary features. Because we have an encoded country column, `country_enc`, keep it and drop other columns related to location: `city`, `country`, `lat`, `long`, `state`.\n",
        "\n",
        "We have columns related to month and year, so we don't need the `date` or `recorded` columns.\n",
        "\n",
        "* We vectorized `desc`, so we don't need it anymore. For now we'll keep `type`.\n",
        "\n",
        "* We'll keep `seconds_log` and drop `seconds` and `minutes`.\n",
        "\n",
        "* Let's also get rid of the `length_of_time` column, which is unnecessary after extracting minutes.\n",
        "\n",
        "* Use **`.corr()`** to run the correlation on` seconds`, `seconds_log`, and `minutes` in the `ufo` DataFrame.\n",
        "\n",
        "* Make a list of columns to drop, in alphabetical order.\n",
        "\n",
        "* Use **`.drop()`** to drop the columns.\n",
        "\n",
        "* Use the `words_to_filter()` function we created previously. Pass in `vocab`, `vec.vocabulary_`, `desc_tfidf`, and let's keep the top `4` words as the last parameter."
      ],
      "metadata": {
        "id": "efh10euZaoJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
        "    filter_list = []\n",
        "    \n",
        "    for i in range(0, vector.shape[0]):\n",
        "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
        "        filter_list.extend(filtered)\n",
        "        \n",
        "    return set(filter_list)\n",
        "\n",
        "# Check the correlation between the seconds, seconds_log, and minutes columns\n",
        "print(ufo[['seconds', 'seconds_log', 'minutes']].corr())\n",
        "\n",
        "# Make a list of features to drop\n",
        "to_drop = ['city', 'country', 'date', 'desc','lat', 'length_of_time', 'long', 'minutes', 'recorded', 'seconds', 'state']\n",
        "\n",
        "# Drop those features\n",
        "ufo_dropped = ufo.drop(to_drop, axis=1)\n",
        "\n",
        "# Let's also filter some words out of the text vector we created\n",
        "filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)"
      ],
      "metadata": {
        "id": "a7HleoyTZYJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Modeling the UFO dataset, part 1***\n",
        "\n",
        "In this exercise, we're going to build a k-nearest neighbor model to predict which country the UFO sighting took place in. Our `X` dataset has the log-normalized seconds column, the one-hot encoded type columns, as well as the month and year when the sighting took place. The `y` labels are the encoded country column, where `1` is `us` and `0` is `ca`.\n",
        "\n",
        "* Print out the **`.columns`** of the `X` set.\n",
        "\n",
        "* Split up the `X` and `y` sets using **`train_test_split()`**. Pass the `y` set to the **`stratify=`** parameter, since we have imbalanced classes here.\n",
        "\n",
        "* Use **`.fit()`** to fit `train_X` and `train_y`.\n",
        "\n",
        "* Print out the **`.score()`** of the `knn` model on the `test_X` and `test_y` sets."
      ],
      "metadata": {
        "id": "WIvLti8UdW7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the features in the X set of data\n",
        "print(X.columns)\n",
        "\n",
        "# Split the X and y sets using train_test_split, setting stratify=y\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, stratify=y)\n",
        "\n",
        "# Fit knn to the training sets\n",
        "knn.fit(train_X, train_y)\n",
        "\n",
        "# Print the score of knn on the test sets\n",
        "print(knn.score(test_X, test_y))"
      ],
      "metadata": {
        "id": "B8rnTFjfeA_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Index(['seconds_log', 'changing', 'chevron', 'cigar', 'circle', 'cone',\n",
        "           'cross', 'cylinder', 'diamond', 'disk', 'egg', 'fireball', 'flash',\n",
        "           'formation', 'light', 'other', 'oval', 'rectangle', 'sphere',\n",
        "           'teardrop', 'triangle', 'unknown', 'month', 'year'],\n",
        "          dtype='object')\n",
        "    0.8693790149892934\n",
        "```\n",
        "This model performs pretty well. It seems like we've made pretty good feature selection choices here.\n",
        "\n",
        "### ***Modeling the UFO dataset, part 2***\n",
        "\n",
        "Finally, let's build a model using the text vector we created, `desc_tfidf`, using the `filtered_words` list to create a filtered text vector. Let's see if we can predict the type of the sighting based on the text. We'll use a Naive Bayes model for this.\n",
        "\n",
        "* On the `desc_tfidf` vector, filter by passing a list of `filtered_words` into the index.\n",
        "\n",
        "* Split up the `X` and `y` sets using `train_test_split()`. Remember to convert `filtered_text` using **`toarray()`**. Pass the `y` set to the **`stratify=`** parameter, since we have imbalanced classes here.\n",
        "\n",
        "* Use the `nb` model's `fit()` to fit `train_X` and `train_y`.\n",
        "\n",
        "* Print out the **`.score()`** of the `nb` model on the `test_X` and `test_y` sets.\n"
      ],
      "metadata": {
        "id": "8_saOwkaeoHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the list of filtered words we created to filter the text vector\n",
        "filtered_text = desc_tfidf[:, list(filtered_words)]\n",
        "\n",
        "# Split the X and y sets using train_test_split, setting stratify=y \n",
        "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n",
        "\n",
        "# Fit nb to the training sets\n",
        "nb.fit(train_X, train_y)\n",
        "\n",
        "# Print the score of nb on the test sets\n",
        "print(nb.score(test_X, test_y))"
      ],
      "metadata": {
        "id": "7-ObW5LAfC0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "0.16274089935760172\n",
        "```\n",
        "\n",
        "This model performs very poorly on this text data. This is a clear case where iteration would be necessary to figure out what subset of text improves the model, and if perhaps any of the other features are useful in predicting type.\n",
        "\n"
      ],
      "metadata": {
        "id": "Pyakc23AgBxo"
      }
    }
  ]
}